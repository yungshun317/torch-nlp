{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b76742-6ef1-4a5d-bdcf-8071dd0a9826",
   "metadata": {},
   "source": [
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Example-Based Prompting**.\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\". \n",
    "    - **Role-Based:** Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the steps**\n",
    "    - **Delimiters**: Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format**\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then fill in the blanks in the template. Also called **Template-Based Prompting**. \n",
    "8. **Emotional Prompting:** Asks for tone adjustment.\n",
    "9. **Laddering Prompting:** Break a very complex problem into multiple smaller prompts and problems instead of sending one huge prompt in one step. Alternative names of this approach include **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "10. **LLM-Powered Problem Splitting**:\n",
    "11. **Ask-Before-Answer Prompting:** Asks the LLM which extra information would improve the result & the generated content.\n",
    "12. **Self-Evaluative Prompting**: Asks the LLM to be self-critical. To evaluate & improve the responses it generated.\n",
    "13. **Reverse Prompting:** Reverse Roles. Asks the LLM which prompt should be used to produce an output similar to a provided output.\n",
    "14. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "15. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "16. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "18. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "19. **Least-to-Most Prompting**\n",
    "20. **Self-Consistency**\n",
    "21. **Complexity-Based Prompting**\n",
    "22. **Self-Refine**\n",
    "23. **Generated Knowledge Prompting**\n",
    "24. **Prompt Chaining**\n",
    "25. **Tree of Thoughts**\n",
    "26. **Maieutic Prompting**\n",
    "27. **Retrieval Augmented Generation (RAG)**\n",
    "28. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "29. **Automatic Reasoning and Tool-use (ART)**\n",
    "30. **Automatic Prompt Engineer (APE)**: Uses one LLM to beam search over prompts for another LLM.\n",
    "32. **Active Prompt**\n",
    "33. **Emotion Prompting:**\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "35. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)\n",
    "37. **Program-Aided Language Models**\n",
    "38. **ReAct**\n",
    "39. **Reflexion**\n",
    "40. **Multimodal CoT**\n",
    "41. **Graph Prompting**\n",
    "42. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "43. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "44. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "99. **Prompt Injection:**\n",
    "    - **Prompt Takeovers**\n",
    "    - **Prompt Leaks**\n",
    "100. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
