{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b76742-6ef1-4a5d-bdcf-8071dd0a9826",
   "metadata": {},
   "source": [
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples. Also called **Zero-Shot Learning**.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Few-Shot Learning**, **Example-Based Prompting**.\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\". \n",
    "    - **Role-Based:** Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the steps**\n",
    "    - **Delimiters:** Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format:** **Data-Structured Prompting (DSP)** uses structured data formats, such as tables, lists, or specific schemas.\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then replace the placeholders in the template. Prompt templating also allows for prompts to be stored, reused, shared, and programmed. Also called **Template-Based Prompting**. \n",
    "8. **Emotional Prompting:** Asks for tone adjustment.\n",
    "9. **Laddering Prompting:** Break a very complex problem into multiple smaller prompts and problems instead of sending one huge prompt in one step. Alternative names of this approach include **Prompt Composition**, **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "10. **Prompt Decomposition**\n",
    "11. **Ask-Before-Answer Prompting:** Asks the LLM which extra information would improve the result and the generated content.\n",
    "12. **Self-Evaluative Prompting**: Asks the LLM to be self-critical. To evaluate and improve the responses it generated.\n",
    "13. **Reverse Prompting:** Reverse Roles. Asks the LLM which prompt should be used to produce an output similar to a provided output.\n",
    "14. **Prompt Ensembling:** Uses multiple unanswered prompts for an input.\n",
    "16. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps. Also called **Prompt Augmentation**, **Demonstration Learning**.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "17. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "18. **Few-Shot Chain-of-Thought (CoT) Prompting:**\n",
    "19. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "20. **Self-Consistency**\n",
    "21. **Logical Chain-of-Thought (LogiCoT) Prompting:**\n",
    "22. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "23. **Tree-of-Thoughts (ToT) Prompting:**\n",
    "24. **Graph-of-Thought (GoT) Prompting:**\n",
    "25. **System 2 Attention Prompting:**\n",
    "26. **Thread-of-Thought (ThoT) Prompting:**\n",
    "27. **Contrastive Chain-of-Thought (CCoT):** \n",
    "    - [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277)\n",
    "28. **Chain-of-Table Prompting:**\n",
    "29. **Self-Ask Prompting (SA):**\n",
    "30. **Retrieval Augmented Generation (RAG):**\n",
    "    - **Naive RAG:**\n",
    "    - **Advanced RAG:**\n",
    "    - **Modular RAG:**\n",
    "32. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "33. **ReAct Prompting:**\n",
    "34. **Chain-of-Verification (CoVe) Prompting:**\n",
    "35. **Chain-of-Note (CoN) Prompting:**\n",
    "36. **Chain-of-Knowledge (CoK) Prompting:**\n",
    "37. **Active Prompting**\n",
    "    - [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246)\n",
    "38. **Automatic Prompt Engineer (APE):** Uses one LLM to beam search over prompts for another LLM.\n",
    "39. **Automatic Reasoning and Tool-use (ART)**\n",
    "40. **Emotion Prompting:**\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "41. **Scratchpad Prompting:**\n",
    "42. **Program-of-Thoughts (PoT) Prompting:**\n",
    "43. **Structured Chain-of-Thought (SCoT) Prompting:**\n",
    "44. **Chain-of-Code (CoC) Prompting:**\n",
    "45. **Optimization by Prompting:**\n",
    "46. **Rephrase & Respond (RaR) Prompting:**\n",
    "    - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n",
    "47. **Take a Step Back Prompting**\n",
    "48. **Complexity-Based Prompting:**\n",
    "    - [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)\n",
    "49. **Self-Refine**\n",
    "    - [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n",
    "50. **Generated Knowledge Prompting:**\n",
    "    - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "47. **Prompt Chaining:** \n",
    "48. **Prompt Sharing:** Prompt templates are partially shared.\n",
    "49. **Prompt Pipelining**\n",
    "50. **Maieutic Prompting**\n",
    "    - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)\n",
    "51. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)\n",
    "52. **Program-Aided Language Models (PAL)**\n",
    "    - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "53. **Reflexion**\n",
    "    - [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n",
    "54. **Multimodal Chain-of-Thought (CoT) Prompting**\n",
    "    - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)\n",
    "55. **Graph Prompting:**\n",
    "    - [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://arxiv.org/abs/2302.08043)\n",
    "56. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "57. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "58. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "59. **Least-to-Most Prompting**\n",
    "    - [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n",
    "60. **Autonomous Agents**\n",
    "61. **Prompt Tuning:** **Soft Prompting**\n",
    "62. **Retriever Fine-Tuning**\n",
    "64. **Collaborative Fine-Tuning**\n",
    "65. **Generator Fine-Tunning**\n",
    "99. **Prompt Injection:**\n",
    "    - **Prompt Takeovers**\n",
    "    - **Prompt Leaks**\n",
    "100. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc6bc-8807-4722-8573-8a18bdd917a9",
   "metadata": {},
   "source": [
    "# 2. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "- `langchain-core`: Base abstractions of components and **LangChain Expression Language (LCEL)**.\n",
    "- `langchain-community`: Third party integrations.\n",
    "    - `langchain-openai`\n",
    "    - `langchain-huggingface`\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langgraph`\n",
    "\n",
    "## 2-1. Chat Models & LLMs\n",
    "Chat models use a sequence of messages as inputs and return chat messages as outputs. Traditionally newer models. Inherited from `langchain_core.language_models.chat_models.BaseChatModel`.\n",
    "\n",
    "1. `langchain_openai.chat_models.base.ChatOpenAI`: OpenAI chat model integration.\n",
    "   - `model`\n",
    "   - `n`: Number of responses.\n",
    "   - `temperature`: Adds randomness to responses. Higher values make answers more diverse, while lower values make them more focused and deterministic. \n",
    "   - `timeout`: Request timeout.\n",
    "   - `stop`: Provides a list of stop words to prevent the model from generating responses containing those specific words.\n",
    "   - `max_tokens`: Max tokens to generate.\n",
    "   - `max_retries`: Max number of times to retry requests.\n",
    "   - `api_keys`\n",
    "   - `base_url`: Endpoint to Send Requests to.\n",
    "   - `model_kwargs`: Holds model parameters valid for `openai.OpenAI.chat.completions.create(messages, model, stream, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)` call not explicitly specified.\n",
    "       - `top_p`: **Nucleus Sampling** controls the diversity and quality of the responses. Limits the cumulative probability of the most likely tokens. Higher values allow more tokens, leading to diverse responses, while lower values provide more focused and constrained answers.\n",
    "       - `frequency_panelty`: Controls the model's tendency to generate repetitive words or phrases. Higher values encourage the model to explore more diverse and novel responses; lower values make the model more likely to repeat information.\n",
    "       - `presence_penalty`: Controls avoidance of certain topics. Higher values will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "       - [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "       - [openai-python/src/openai/resources/chat/completions.py](https://github.com/openai/openai-python/blob/main/src/openai/resources/chat/completions.py)  \n",
    "\n",
    "Pure text-in/text-out LLMs tends to be older or lower-level. Inherited from `langchain_core.language_models.llms.BaseLLM`.\n",
    "\n",
    "2. `langchain_openai.llms.base.OpenAI`: OpenAI large language models. Inherited from `langchain_openai.llms.base.BaseOpenAI`.\n",
    "\n",
    "Both `BaseChatModel` and `BaseLLM` classes are inherited from `langchain_core.language_models.base.BaseLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598cecb0-7935-4a89-8ed1-b25e3f54617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-seu77suuc9514oGQFUZbT3BlbkFJ8d49asMUHF9vqMNZxBPU'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9072be63-0000-4fc5-a0d7-b2a6288a5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b58327a5-f9a8-4e48-93f4-b8a3ef1e6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As of 2021, the estimated population of Barcelona is 1.6 million.\n"
     ]
    }
   ],
   "source": [
    "# `OpenAI.predict()`\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, max_tokens=256, timeout=None, max_retries=2)\n",
    "\n",
    "prompt = \"How many citizens does Barcelona have?\"\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f64d3a-386d-4e98-aa31-2de6e52be7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "George Washington was the first president of the United States.\n"
     ]
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "prompt = \"Who was the first president of the United States?\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbe64930-d8fe-4365-b82d-0ec193a19b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Michael Jordan is a retired American professional basketball player who is widely considered one of the greatest basketball players of all time. He played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards, winning six NBA championships and earning numerous individual awards, including five MVP awards. He is known for his incredible athleticism, scoring ability, and competitive drive, and is often credited with popularizing the game of basketball around the world. After retiring from basketball, Jordan became a successful businessman and owner of the Charlotte Hornets NBA team.\n"
     ]
    }
   ],
   "source": [
    "# `OpenAI.invoke()`\n",
    "prompt = \"Who is Michael Jordan?\"\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b18696fa-14be-49a2-9b3d-aa9659343ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7374b4475520> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7374b45cab70> openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    }
   ],
   "source": [
    "# `ChatOpenAI()`\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9429f382-fb9e-425e-97ea-e9494d05de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7374b45caae0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7374b447f530> model_name='gpt-4o' temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2, model_kwargs={\"top_p\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e079e8b-afbc-490b-9abf-39c2388571f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so it's always a good idea to check the latest statistics from official sources such as the Barcelona City Council or the National Institute of Statistics (INE) for the most up-to-date information.\" response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 24, 'total_tokens': 93}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-7e004a7a-5efe-49d9-ae32-e6d335ddc6ad-0' usage_metadata={'input_tokens': 24, 'output_tokens': 69, 'total_tokens': 93}\n"
     ]
    }
   ],
   "source": [
    "# `ChatOpenAI.invoke()`\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"How many citizens does Barcelona have?\")\n",
    "]\n",
    "print(llm.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0338-f12e-4aaa-ba21-8d6cbe0b8df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4d19af-ccb6-435f-860c-461b5fec96f7",
   "metadata": {},
   "source": [
    "## 2-2. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb55ef54-f14f-4386-af08-b931cebf36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I want to open a restaurant for Italian food. Suggest a fency name for this.')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "prompt_template.invoke({\"cuisine\": \"Italian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40bdba48-aa25-4a74-9d8b-413fa551abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Italian food. Suggest a fency name for this.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(cuisine=\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74ca26e1-91bd-4412-8f9c-033b6582caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7405f89b-7828-4b85-a774-bd376f7aa749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cats.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(topic=\"cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2f33-c03e-48c4-adfd-9acaf7c8a578",
   "metadata": {},
   "source": [
    "## 2-3. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e842d4-95b8-4dd8-9ddd-d48129d61904",
   "metadata": {},
   "source": [
    "## 2-4. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347e5e-541f-4a01-8d6b-07695e9753d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
