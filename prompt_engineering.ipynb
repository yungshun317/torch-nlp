{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b76742-6ef1-4a5d-bdcf-8071dd0a9826",
   "metadata": {},
   "source": [
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples. Also called **Zero-Shot Learning**.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Few-Shot Learning**, **Example-Based Prompting**, **Prompt Augmentation**, or **Demonstration Learning**.\n",
    "    - **Sample Selection**\n",
    "    - **Sample Ordering**\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\".\n",
    "    - **Role-Based:** Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the steps**\n",
    "    - **Delimiters:** Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format:** **Data-Structured Prompting (DSP)** uses structured data formats, such as tables, lists, or specific schemas.\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then replace the placeholders in the template. Prompt templating also allows for prompts to be stored, reused, shared, and programmed. Also called **Template-Based Prompting**. \n",
    "8. **Emotional Prompting:** Asks for tone adjustment.\n",
    "9. **Laddering Prompting:** Break a very complex problem into multiple smaller prompts and problems instead of sending one huge prompt in one step. Alternative names of this approach include **Prompt Composition**, **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "10. **Prompt Decomposition**\n",
    "11. **Ask-Before-Answer Prompting:** Asks the LLM which extra information would improve the result and the generated content.\n",
    "12. **Self-Evaluative Prompting**: Asks the LLM to be self-critical. To evaluate and improve the responses it generated.\n",
    "13. **Reverse Prompting:** Reverse Roles. Asks the LLM which prompt should be used to produce an output similar to a provided output.\n",
    "14. **Prompt Ensembling:** Uses multiple unanswered prompts for an input. Multiple predictions result from different prompt variants. Usually combines results of multiple systems to get better performance.\n",
    "15. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "16. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "17. **Few-Shot Chain-of-Thought (CoT) Prompting:**\n",
    "18. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "19. **Self-Consistency**\n",
    "20. **Logical Chain-of-Thought (LogiCoT) Prompting:**\n",
    "21. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "22. **Tree-of-Thoughts (ToT) Prompting:**\n",
    "23. **Graph-of-Thought (GoT) Prompting:**\n",
    "24. **System 2 Attention Prompting:**\n",
    "25. **Thread-of-Thought (ThoT) Prompting:**\n",
    "26. **Contrastive Chain-of-Thought (CCoT):** \n",
    "    - [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277)\n",
    "27. **Chain-of-Table Prompting:**\n",
    "28. **Self-Ask Prompting (SA):**\n",
    "29. **Retrieval Augmented Generation (RAG):**\n",
    "    - **Naive RAG:**\n",
    "    - **Advanced RAG:**\n",
    "    - **Modular RAG:**\n",
    "30. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "31. **ReAct Prompting:**\n",
    "32. **Chain-of-Verification (CoVe) Prompting:**\n",
    "33. **Chain-of-Note (CoN) Prompting:**\n",
    "34. **Chain-of-Knowledge (CoK) Prompting:**\n",
    "35. **Active Prompting**\n",
    "    - [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246)\n",
    "36. **Automatic Prompt Engineer (APE):** Uses one LLM to beam search over prompts for another LLM.\n",
    "37. **Automatic Reasoning and Tool-use (ART)**\n",
    "38. **Emotion Prompting:**\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "39. **Scratchpad Prompting:**\n",
    "40. **Program-of-Thoughts (PoT) Prompting:**\n",
    "41. **Structured Chain-of-Thought (SCoT) Prompting:**\n",
    "42. **Chain-of-Code (CoC) Prompting:**\n",
    "43. **Optimization by Prompting:**\n",
    "44. **Rephrase & Respond (RaR) Prompting:**\n",
    "    - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n",
    "45. **Take a Step Back Prompting**\n",
    "46. **Complexity-Based Prompting:**\n",
    "    - [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)\n",
    "47. **Self-Refine**\n",
    "    - [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n",
    "48. **Generated Knowledge Prompting:**\n",
    "    - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "49. **Prompt Chaining:** Creates a series of LLM calls follow on each other with the output of one prompt in the chain serving as the input of the following.\n",
    "50. **Prompt Sharing:** Prompt templates are partially shared.\n",
    "51. **Prompt Pipelining:** The variables or placeholders in the pre-defined prompt template are populated (prompt injection) with the question from the user and the knowledge to be searched from the knowledge store.\n",
    "52. **Stuff Documents Chain**\n",
    "53. **Reduce Documents Chain**\n",
    "54. **Map Reduce Documents Chain**\n",
    "55. **Refine Documents Chain**\n",
    "56. **Map Rerank Documents Chain**\n",
    "57. **Maieutic Prompting**\n",
    "    - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)\n",
    "58. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)\n",
    "59. **Program-Aided Language Models (PAL)**\n",
    "    - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "60. **Reflexion**\n",
    "    - [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n",
    "61. **Multimodal Chain-of-Thought (CoT) Prompting**\n",
    "    - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)\n",
    "62. **Graph Prompting:**\n",
    "    - [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://arxiv.org/abs/2302.08043)\n",
    "63. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "64. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "65. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "66. **Least-to-Most Prompting**\n",
    "    - [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n",
    "67. **Autonomous Agents:** Prompt Chaining is the execution of a predetermined and set sequence of actions. However, agents can maintain a high level of autonomy.\n",
    "68. **Prompt Tuning:** **Soft Prompting**\n",
    "69. **Retriever Fine-Tuning**\n",
    "70. **Collaborative Fine-Tuning**\n",
    "71. **Generator Fine-Tunning**\n",
    "99. **Prompt Injection Attack**\n",
    "    - **Prompt Takeovers**\n",
    "    - **Prompt Leaks**\n",
    "100. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc6bc-8807-4722-8573-8a18bdd917a9",
   "metadata": {},
   "source": [
    "# 2. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "- `langchain-core`: Base abstractions of components and **LangChain Expression Language (LCEL)**.\n",
    "- `langchain-community`: Third party integrations.\n",
    "    - `langchain-openai`\n",
    "    - `langchain-huggingface`\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langgraph`\n",
    "\n",
    "## 2-1. Chat Models & LLMs\n",
    "Chat models use a sequence of messages as inputs and return chat messages as outputs. Traditionally newer models. Inherited from `langchain_core.language_models.chat_models.BaseChatModel`.\n",
    "\n",
    "1. `langchain_openai.chat_models.base.ChatOpenAI`: OpenAI chat model integration.\n",
    "   - `model`\n",
    "   - `n`: Number of responses.\n",
    "   - `temperature`: Adds randomness to responses. Higher values make answers more diverse, while lower values make them more focused and deterministic. \n",
    "   - `timeout`: Request timeout.\n",
    "   - `stop`: Provides a list of stop words to prevent the model from generating responses containing those specific words.\n",
    "   - `max_tokens`: Max tokens to generate.\n",
    "   - `max_retries`: Max number of times to retry requests.\n",
    "   - `api_keys`\n",
    "   - `base_url`: Endpoint to Send Requests to.\n",
    "   - `model_kwargs`: Holds model parameters valid for `openai.OpenAI.chat.completions.create(messages, model, stream, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)` call not explicitly specified.\n",
    "       - `top_p`: **Nucleus Sampling** controls the diversity and quality of the responses. Limits the cumulative probability of the most likely tokens. Higher values allow more tokens, leading to diverse responses, while lower values provide more focused and constrained answers.\n",
    "       - `frequency_panelty`: Controls the model's tendency to generate repetitive words or phrases. Higher values encourage the model to explore more diverse and novel responses; lower values make the model more likely to repeat information.\n",
    "       - `presence_penalty`: Controls avoidance of certain topics. Higher values will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "       - [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "       - [openai-python/src/openai/resources/chat/completions.py](https://github.com/openai/openai-python/blob/main/src/openai/resources/chat/completions.py)  \n",
    "\n",
    "Pure text-in/text-out LLMs tends to be older or lower-level. Inherited from `langchain_core.language_models.llms.BaseLLM`.\n",
    "\n",
    "2. `langchain_openai.llms.base.OpenAI`: OpenAI large language models. Inherited from `langchain_openai.llms.base.BaseOpenAI`.\n",
    "\n",
    "Both `BaseChatModel` and `BaseLLM` classes are inherited from `langchain_core.language_models.base.BaseLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598cecb0-7935-4a89-8ed1-b25e3f54617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9072be63-0000-4fc5-a0d7-b2a6288a5b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x7374b40dbe30>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7374abf2e5d0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14082095-55d8-4cb2-bb18-abafa7274023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b58327a5-f9a8-4e48-93f4-b8a3ef1e6643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAs of 2021, the estimated population of Barcelona is 1.6 million.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.predict()`\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, max_tokens=256, timeout=None, max_retries=2)\n",
    "\n",
    "prompt = \"How many citizens does Barcelona have?\"\n",
    "response = llm.predict(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6f64d3a-386d-4e98-aa31-2de6e52be7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGeorge Washington was the first president of the United States.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "prompt = \"Who was the first president of the United States?\"\n",
    "response = llm(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbe64930-d8fe-4365-b82d-0ec193a19b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMichael Jordan is a retired American professional basketball player who is widely considered one of the greatest basketball players of all time. He played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards, winning six NBA championships and earning numerous individual awards, including five MVP awards. He is known for his incredible athleticism, scoring ability, and competitive drive, and is often credited with popularizing the game of basketball around the world. After retiring from basketball, Jordan became a successful businessman and owner of the Charlotte Hornets NBA team.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.invoke()`\n",
    "prompt = \"Who is Michael Jordan?\"\n",
    "response = llm.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18696fa-14be-49a2-9b3d-aa9659343ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed87feea0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed87fe270>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI()`\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9429f382-fb9e-425e-97ea-e9494d05de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8600e00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8601b20>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2, model_kwargs={\"top_p\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e079e8b-afbc-490b-9abf-39c2388571f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 24, 'total_tokens': 87}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-62c75271-a1e7-49cc-b4c7-79908fbf8e9c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 63, 'total_tokens': 87})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI.invoke()`\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"How many citizens does Barcelona have?\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7c0dbc6-0151-4bcb-a082-3bef64f519af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d45a0338-f12e-4aaa-ba21-8d6cbe0b8df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9kguKKA9vSKQf2Frv59X5clPddG7T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\", role='assistant', function_call=None, tool_calls=None))], created=1720915052, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_d33f7b429e', usage=CompletionUsage(completion_tokens=79, prompt_tokens=14, total_tokens=93))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `openai.chat.completions.create()`\n",
    "import openai\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How many citizens does Barcelona have?\"\n",
    "}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, temperature=0)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6b7bacc-d874-4410-ba0b-e6a5b3094262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d19af-ccb6-435f-860c-461b5fec96f7",
   "metadata": {},
   "source": [
    "## 2-2. Prompt Templates\n",
    "Prompt templates:\n",
    "\n",
    "1. `langchain_core.prompts.prompt.PromptTemplate(input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags, template, template_format, validate_template)`: `str` prompt templates.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `from_template(template, template_format, partial_variables, **kwargs)`\n",
    "   - `from_file(template_file, input_variables, **kwargs)`\n",
    "   - `from_examples(examples, prefix, suffix, input_variables, example_separator, **kwargs)`: Few-shot prompt templates. Takes examples in list format with prefix and suffix to create a prompt.\n",
    "\n",
    "2. `langchain_core.prompts.chat.ChatPromptTemplate()`: Formats a list of messages. Alternatively, you can also construct the prompt using message role prompt templates described belowed. \n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Returns a list of finalized messages.\n",
    "   - `from_messages(message, template_format)`: Create a chat prompt template from a variety of message formats.\n",
    "\n",
    "Message role prompt templates:\n",
    "\n",
    "3. `langchain_core.prompts.chat.SystemMessagePromptTemplate(additional_kwargs, prompt)`: for optional `system` message role used to set the behavior of the assistant.\n",
    "4. `langchain_core.prompts.chat.HumanMessagePromptTemplate(additional_kwargs, prompt)`: for `user` message role who provides requests.\n",
    "5. `langchain_core.prompts.chat.AIMessagePromptTemplate(additional_kwargs, prompt)`: for `assistant` message role storing previous assistant responses or giving examples of few-shot prompting.\n",
    "\n",
    "- All of the three message role prompt templates aboved have the following methods:\n",
    "    - `format(**kwargs)`\n",
    "    - `format_messages(**kwargs)`\n",
    "    - `from_template(template, template_format, partial_variables, **kwargs)`: Default format of the template is `f-string`.\n",
    "    - `from_template_file(template_file, input_variables, **kwargs)`\n",
    "\n",
    "Few-shot prompt templates:\n",
    "\n",
    "6. `langchain_core.prompts.few_shot.FewShotPromptTemplate(example_prompt, example_selector, example_separator, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, prefix, suffix, tags, template_format, validate_template)`: Prompt templates that contains few shot examples.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "\n",
    "7. `langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate(example_prompt, example_selector, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags)`: Chat prompt templates that supports few-shot examples. Because there is no `suffix` to assign like `FewShotPromptTemplate`, you need one more final prompt to wrap the template.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Formats `**kwargs` into a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb55ef54-f14f-4386-af08-b931cebf36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I want to open a restaurant for Italian food. Suggest a fency name for this.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PromptTemplate()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "messages = prompt_template.invoke({\"cuisine\": \"Italian\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40bdba48-aa25-4a74-9d8b-413fa551abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Italian food. Suggest a fency name for this.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(cuisine=\"Italian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ebd4837-3d99-457d-b267-fb18b25fce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How about \"La Dolce Vita Ristorante\"? This name evokes the charm and elegance of Italian culture, suggesting a delightful and luxurious dining experience.', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 25, 'total_tokens': 53}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-e17ba7b7-3502-4490-a811-6ab506eceabc-0', usage_metadata={'input_tokens': 25, 'output_tokens': 28, 'total_tokens': 53})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74ca26e1-91bd-4412-8f9c-033b6582caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_template()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7405f89b-7828-4b85-a774-bd376f7aa749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cats.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(topic=\"cats\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad297-3124-4826-92b5-1a982c3b4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `from_file()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf46caee-8a03-4b3d-b949-944de31ff683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_examples(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fff62a5-308b-4d93-90b6-6394245fd647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures constructed from basic materials such as wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are compartmentalized into various sections for different activities. These structures lack the advanced climate control and adaptive architecture found in Martian habitats. Instead, they rely on rudimentary heating and cooling systems and are often cluttered with an array of unnecessary objects and decorations.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few-shot prompting without template\n",
    "messages = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\n",
    "Question: What is human cuisine like?\n",
    "Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "\n",
    "Question: What is human entertainment?\n",
    "Response: Crude moving images and loud sounds.\n",
    "\n",
    "Question: What are human homes like?\n",
    "Response: \"\"\"\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6141f4a-795d-40e4-a536-220e36a4662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['userInput'], template=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: {userInput}\\nResponse: \\n\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "examples = [\n",
    "    \"\"\"\n",
    "    Question: What is human cuisine like?\n",
    "    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Question: What is human entertainment?\n",
    "    Response: Crude moving images and loud sounds.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "prefix = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \n",
    "\"\"\"\n",
    "\n",
    "example_template = PromptTemplate.from_examples(examples=examples, prefix=prefix, suffix=suffix, input_variables=['userInput'], example_separator=\"\\n\\n\")\n",
    "example_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52a27edf-9a2d-4134-a577-e251db5d78de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = example_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c87ac348-d28c-416e-928b-0756a33d626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are often segmented into various compartments for different activities such as sleeping, eating, and socializing. These structures lack the advanced adaptive and self-sustaining features of our Martian habitats.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7244116b-8e9b-4a06-a982-9cf6e2bc0811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatPromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eecea368-a6b2-4b65-9f80-b0b2a092c75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9e1ac11-490a-4ab4-a083-efa9be8c0515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "prompt_template = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\")\n",
    "    + HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    ")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce459097-dd4b-4267-a66a-23202e736b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "940e0bb4-b9cd-4451-9bcd-bd055ddfc8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotPromptTemplate()`\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is human cuisine like?\",\n",
    "        \"answer\": \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is human entertainment?\",\n",
    "        \"answer\": \"Crude moving images and loud sounds.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Question: {query}\n",
    "Response: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cc0c496-aee5-4482-bd12-900fc49014f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: ', prefix='You are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31eb9c81-c6cb-4477-a74d-af8f22e8ac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d2be239-7582-40e9-89a9-cc76d552063b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Their homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. These structures are designed to provide shelter from environmental elements and are often divided into multiple rooms for different activities. The layout and design lack the advanced spatial efficiency and adaptive technology found in Martian habitats.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2874b0b3-03da-4d14-a119-a1adea11012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotChatMessagePromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"1+1\",\n",
    "        \"output\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"3+5\",\n",
    "        \"output\": \"8\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"{output}\")\n",
    "])\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "113100d5-dfe1-44c3-8929-820a9c743d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(examples=examples, example_prompt=example_prompt, input_variables=[])\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6855621-ec2c-4dae-b48c-38b4bacb455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt = few_shot_prompt_template.invoke({})\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5a5d74b-18b4-4a92-8e13-f07eb6e31bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        ChatPromptTemplate.from_messages(few_shot_prompt.to_messages()),\n",
    "        (\"user\", \"{userInput}\"),\n",
    "    ]\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages\n",
    "# chain = final_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "# chain.invoke({\"input\":\"What's the square of a triangle?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00a82d26-b889-4246-a1cd-5149c28d245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"square of a triangle\" is not a standard mathematical concept. However, if you are referring to the area of a triangle, you can calculate it using the formula:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, please provide more context so I can assist you better.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5b1d44d-ab07-439b-84cb-439cedfed6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few shot prompting with `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=(\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\") \n",
    "        + AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "    ),\n",
    "    input_variables=[]\n",
    ")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c04cba-3f57-4d99-b595-271d70880e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='1+1'), AIMessage(content='2'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\") \n",
    "    + few_shot_prompt_template\n",
    "    + HumanMessagePromptTemplate.from_template(\"{userInput}\")\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f333b5d7-2816-4830-a028-6883853d31eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like there might be a bit of confusion in your question. The term \"square\" typically refers to a specific geometric shape or the result of multiplying a number by itself. However, a triangle is a different geometric shape and doesn\\'t have a \"square\" in the same sense.\\n\\nIf you are asking about the area of a triangle, the formula to calculate it is:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, could you please clarify?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb132aac-9db1-45c1-8f43-31225683701f",
   "metadata": {},
   "source": [
    "## 2-3. Example Selectors\n",
    "Length:\n",
    "\n",
    "1. `langchain_core.example_selectors.length_based.LengthBasedExampleSelector(example_prompt, example_text_lengths, examples, get_text_length, max_length=2048)`: Selects examples based on length.\n",
    "\n",
    "Similarity:\n",
    "\n",
    "2. `langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector(example_keys, input_keys, k, vectorstore, vectorstore_kwargs)`: Selects examples based on semantic similarity.\n",
    "3. `langchain_community.vectorstores.chroma.Chroma`: ChromaDB vector store.\n",
    "4. `langchain_openai.embeddings.base.OpenAIEmbeddings`: OpenAI embedding models.\n",
    "\n",
    "Maximum marginal relevance (MMR):\n",
    "\n",
    "5. `langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector(example_keys, fetch_k, input_keys, k, vectorstore, vectorstore_kwargs)`: Selects examples based on Max Marginal Relevance.\n",
    "   - [Complementary Explanations for Effective In-Context Learning](https://arxiv.org/abs/2211.13892)\n",
    "6. `langchain_community.vectorstores.faiss.FAISS(embedding_function, index, docstore, index_to_docstore_id, relevance_score_fn, normalize_L2, distance_strategy)`: Meta Faiss vector store.\n",
    "\n",
    "n-gram:\n",
    "\n",
    "7. `langchain_community.example_selectors.ngram_overlap.NGramOverlapExampleSelector(example_prompt, examples, threshold=-1.0)`: Selects and orders examples based on n-gram overlap score (`sentence_bleu` score from NLTK package).\n",
    "   - Currently there is an issue with `bleu_score.py` but not yet built on PyPI. You need to update `nltk/translate/bleu_score.py` manually as on [GitHub](https://github.com/nltk/nltk/commit/28eeb3e83c98d27ad6b67f233576f09883f394fe#diff-644ebc352f93bb9a63ff8edbc7292e137cf1adaf68d5dc4f1aef13f4de96bf20).\n",
    "  \n",
    "- All example selectors have the following methods.\n",
    "    - `add_example(example)`\n",
    "    - `select_examples(input_variables)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45854422-d580-426a-9e3c-08a1ae0afee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `LengthBasedExampleSelector()`\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is human cuisine like?\",\n",
    "        \"answer\": \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human entertainment?\",\n",
    "        \"answer\": \"Crude moving images and loud sounds.\"\n",
    "    }, {\n",
    "        \"query\": \"What do humans use for transportation?\",\n",
    "        \"answer\": \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans communicate with each other?\",\n",
    "        \"answer\": \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans maintain health?\",\n",
    "        \"answer\": \"Consuming organic compounds and performing physical movements.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human education?\",\n",
    "        \"answer\": \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans manage their societies?\",\n",
    "        \"answer\": \"Through chaotic and inefficient systems.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human art?\",\n",
    "        \"answer\": \"Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Question: {query}\n",
    "Response: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c95948-b183-4cc3-a31e-43970358cc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29]), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b157fa-596b-47f7-9692-a177c93b557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9791a25a-39d4-43d8-a5b4-33ae1f2f8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are rudimentary shelters constructed from basic materials such as wood, stone, and synthetic compounds. They are designed to provide minimal protection from environmental elements and are often cluttered with an array of unnecessary objects. These structures lack the advanced climate control and adaptive architecture found in our Martian dwellings.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e517155-6037-44af-9979-5f3f5cee8e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}, {'query': 'What are the birds?', 'answer': 'Mere distractions, I imagine. Noisemakers to fill the void of their silent minds'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29, 21]), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `add_example()`\n",
    "new_example = {\"query\": \"What are the birds?\", \"answer\": \"Mere distractions, I imagine. Noisemakers to fill the void of their silent minds\"}\n",
    "few_shot_prompt_template.example_selector.add_example(new_example)\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f95a70d6-4d81-43ba-9ef7-a54cf30ef742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Crude moving images and loud sounds.',\n",
       "  'query': 'What is human entertainment?'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `SemanticSimilarityExampleSelector()`\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples=examples, embeddings=OpenAIEmbeddings(), vectorstore_cls=Chroma, k=1)\n",
    "query = \"What is human brain made of?\"\n",
    "selected_examples = example_selector.select_examples({\"query\": query})\n",
    "selected_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e287f9ff-5710-4a39-ac79-e9d6d6d9790d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7374a87785c0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bee3b848-7b35-4fb1-88e0-1da6bac13f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae3c2be4-edf8-47b1-88e8-abab6c7ef2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are enclosed structures made from various materials such as wood, brick, and metal. They are designed to provide shelter and comfort, often containing multiple rooms with specific functions like sleeping, cooking, and socializing. These structures are equipped with systems for temperature control, water supply, and waste disposal. Humans decorate their homes with objects and colors that reflect their personal tastes and cultural influences.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc4c8d5f-eec3-4c13-8510-6cb81d961d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxMarginalRelevanceExampleSelector(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cd33fa8890>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None, fetch_k=20)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `MaxMarginalRelevanceExampleSelector()`\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(examples, OpenAIEmbeddings(), FAISS, k=2)\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8118d18-ea9f-4734-950c-abe1ceed416c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=MaxMarginalRelevanceExampleSelector(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cd33fa8890>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None, fetch_k=20), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d73d6bcf-8848-49d3-9088-1615e3157ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabda531-9c57-4f9e-84a7-3b3872b56204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramOverlapExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), threshold=0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `NGramOverlapExampleSelector()`\n",
    "from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(examples=examples, example_prompt=example_prompt, threshold=0.0)\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1ba6fa-0b93-43ac-a2e4-75504907e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=NGramOverlapExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), threshold=0.0), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9736660-1a1f-43e3-85b9-177871b939a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2f33-c03e-48c4-adfd-9acaf7c8a578",
   "metadata": {},
   "source": [
    "## 2-4. Output Parsers\n",
    "\n",
    "Pydantic:\n",
    "\n",
    "1. `langchain_core.output_parsers.pydantic.PydanticOutputParser(diff, pydantic_object)`: Parses and validates an output and returns a Pydantic model instance.\n",
    "\n",
    "JSON:\n",
    "\n",
    "2. `langchain_core.output_parsers.json.JsonOutputParser(diff, pydantic_object)`: Parses the output of an LLM call to a JSON object.\n",
    "\n",
    "XML:\n",
    "\n",
    "3. `langchain_core.output_parsers.xml.XMLOutputParser(encoding_matcher, parser)`: Parses an output using XML format. Outputs `dict`.\n",
    "   - Can be converted to XML using `dict2xml`, but further XML processing with `lxml` could be required.\n",
    "   - Can also be converted to pandas `DataFrame`, further manipulation may be needed.\n",
    "\n",
    "CSV:\n",
    "\n",
    "4. `langchain_core.output_parsers.list.CommaSeparatedListOutputParser`: Parses the output of an LLM call to a comma-separated list.\n",
    "   - Can write the output comma-separated list into a CSV file using `csv`.\n",
    "   \n",
    "Enum:\n",
    "\n",
    "5. `langchain.output_parsers.enum.EnumOutputParser(enum)`: Parses an output that is one of a set of values.\n",
    "    - Clarify the prompt. Make sure the prompt explicitly instructs the LLM to respond with only one of the enum values or you will get an error.\n",
    "    \n",
    "Structured:\n",
    "\n",
    "6. `langchain.output_parsers.structured.StructuredOutputParser(response_schemas)`: Returns structured information.\n",
    "   - Less powerful than `PydanticOutputParser` or `JsonOutputParser` since it only allows for fields to be `str`. Only useful for smaller LLMs.\n",
    "\n",
    "- All of the output parsers aboved have the following methods:\n",
    "    - `get_format_instructions()`\n",
    "    - `invoke(input, config)`: Transforms a single input into an output.\n",
    "    - `parse(text)`\n",
    "    - `parse_from_prompt(completion, prompt)`: Parses the output of an LLM call with the input prompt for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25b2b4fc-9323-4f33-80be-bb99eea5ee79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.Joke'>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PydanticOutputParser()`\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Define your desired data structure\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic\n",
    "    @field_validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "819c8746-b680-4d99-ae7f-bd4324d97965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7113e9f9-3982-4751-9dc6-f026d28e99e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"setup\": \"Why don\\'t scientists trust atoms?\",\\n  \"punchline\": \"Because they make up everything!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 206, 'total_tokens': 236}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-fcc29177-7251-45ae-9bfd-c972484b4977-0', usage_metadata={'input_tokens': 206, 'output_tokens': 30, 'total_tokens': 236})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt_template | llm\n",
    "output = chain.invoke({\"query\": \"Tell me a joke.\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3cad9e4-fa2d-465a-8c99-2c6085033741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cad0db0-558b-43eb-95b4-5da7240824a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bea7a31-6db1-4ee9-b96a-c9b32b080367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_with_prompt(completion=output.content, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b30a402-081e-4dab-a179-ed8663f54b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "553e59c4-a9f7-485c-865c-bb8f1891dd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.Joke'>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `JsonOutputParser()`\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5b9b7b9-93b5-460c-9132-5055adda41e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69ed8f12-38ab-4e7e-a134-46a0205784b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15bf3f90-473a-4f41-a546-da13554e788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XMLOutputParser()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `XMLOutputParser()`\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "parser = XMLOutputParser()\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8aaf2876-4cdf-464d-a5af-ef21b0fa35a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a XML file.\\n1. Output should conform to the tags below. \\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema. \\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "195e0ea2-607f-4396-b3c2-0337f0c93775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filmography': [{'actor': [{'name': 'Tom Hanks'},\n",
       "    {'movies': [{'movie': [{'title': 'Big'},\n",
       "        {'year': '1988'},\n",
       "        {'role': 'Josh Baskin'}]},\n",
       "      {'movie': [{'title': 'Forrest Gump'},\n",
       "        {'year': '1994'},\n",
       "        {'role': 'Forrest Gump'}]},\n",
       "      {'movie': [{'title': 'Sleepless in Seattle'},\n",
       "        {'year': '1993'},\n",
       "        {'role': 'Sam Baldwin'}]},\n",
       "      {'movie': [{'title': 'Saving Private Ryan'},\n",
       "        {'year': '1998'},\n",
       "        {'role': 'Captain John H. Miller'}]},\n",
       "      {'movie': [{'title': 'Cast Away'},\n",
       "        {'year': '2000'},\n",
       "        {'role': 'Chuck Noland'}]},\n",
       "      {'movie': [{'title': 'The Terminal'},\n",
       "        {'year': '2004'},\n",
       "        {'role': 'Viktor Navorski'}]},\n",
       "      {'movie': [{'title': 'Catch Me If You Can'},\n",
       "        {'year': '2002'},\n",
       "        {'role': 'Carl Hanratty'}]},\n",
       "      {'movie': [{'title': 'Bridge of Spies'},\n",
       "        {'year': '2015'},\n",
       "        {'role': 'James B. Donovan'}]},\n",
       "      {'movie': [{'title': 'Sully'},\n",
       "        {'year': '2016'},\n",
       "        {'role': \"Chesley 'Sully' Sullenberger\"}]},\n",
       "      {'movie': [{'title': 'A Beautiful Day in the Neighborhood'},\n",
       "        {'year': '2019'},\n",
       "        {'role': 'Fred Rogers'}]}]}]}]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"{query}\\n{format_instructions}\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "parser_output = chain.invoke({\"query\": \"Generate the shortened filmography for Tom Hanks.\"})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7cb510f2-09e3-48b4-9c47-2116d35de619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to an XML file\n",
    "from dict2xml import dict2xml\n",
    "\n",
    "xml = dict2xml(parser_output)\n",
    "with open('parser_output.xml', 'w') as f:\n",
    "    for line in xml:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "88314afc-cbd0-4185-b6f3-3c9b69e0fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<filmography>\\n  <actor>\\n    <name>Tom Hanks</name>\\n  </actor>\\n  <actor>\\n    <movies>\\n      <movie><title>Big</title><year>1988</year><role>Josh Baskin</role></movie></movies>\\n    <movies>\\n      <movie><title>Forrest Gump</title><year>1994</year><role>Forrest Gump</role></movie></movies>\\n    <movies>\\n      <movie><title>Sleepless in Seattle</title><year>1993</year><role>Sam Baldwin</role></movie></movies>\\n    <movies>\\n      <movie><title>Saving Private Ryan</title><year>1998</year><role>Captain John H. Miller</role></movie></movies>\\n    <movies>\\n      <movie><title>Cast Away</title><year>2000</year><role>Chuck Noland</role></movie></movies>\\n    <movies>\\n      <movie><title>The Terminal</title><year>2004</year><role>Viktor Navorski</role></movie></movies>\\n    <movies>\\n      <movie><title>Catch Me If You Can</title><year>2002</year><role>Carl Hanratty</role></movie></movies>\\n    <movies>\\n      <movie><title>Bridge of Spies</title><year>2015</year><role>James B. Donovan</role></movie></movies>\\n    <movies>\\n      <movie><title>Sully</title><year>2016</year><role>Chesley 'Sully' Sullenberger</role></movie></movies>\\n    <movies>\\n      <movie><title>A Beautiful Day in the Neighborhood</title><year>2019</year><role>Fred Rogers</role></movie></movies>\\n  </actor>\\n</filmography>\""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "tree = etree.fromstring(xml)\n",
    "for movies in tree.findall('.//movies'):\n",
    "    title = movies.find('.//title').text\n",
    "    year = movies.find('.//year').text\n",
    "    role = movies.find('.//role').text\n",
    "    \n",
    "    movie_lst = movies.findall('.//movie')\n",
    "    for movie in movie_lst:\n",
    "        movies.remove(movie)\n",
    "    movies.append(etree.Element('movie'))\n",
    "    movie = movies.find('.//movie')\n",
    "    \n",
    "    title_el = etree.Element('title')\n",
    "    title_el.text = title\n",
    "    year_el = etree.Element('year')\n",
    "    year_el.text = year\n",
    "    role_el = etree.Element('role')\n",
    "    role_el.text = role\n",
    "    \n",
    "    movie.append(title_el)\n",
    "    movie.append(year_el)\n",
    "    movie.append(role_el)\n",
    "xml_parsed = etree.tostring(tree).decode()\n",
    "xml_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d99076a6-4a8a-48f0-a9d9-7abd16fc0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parser_output_parsed.xml', 'w') as f:\n",
    "    for line in xml_parsed:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d939371d-9e70-45c8-b018-b90250308e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'title': 'Big'}, {'year': '1988'}, {'role': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'title': 'Forrest Gump'}, {'year': '1994'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'title': 'Sleepless in Seattle'}, {'year': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'title': 'Saving Private Ryan'}, {'year': '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'title': 'Cast Away'}, {'year': '2000'}, {'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'title': 'The Terminal'}, {'year': '2004'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'title': 'Catch Me If You Can'}, {'year': '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'title': 'Bridge of Spies'}, {'year': '2015'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'title': 'Sully'}, {'year': '2016'}, {'role'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'title': 'A Beautiful Day in the Neighborhoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie\n",
       "0  [{'title': 'Big'}, {'year': '1988'}, {'role': ...\n",
       "1  [{'title': 'Forrest Gump'}, {'year': '1994'}, ...\n",
       "2  [{'title': 'Sleepless in Seattle'}, {'year': '...\n",
       "3  [{'title': 'Saving Private Ryan'}, {'year': '1...\n",
       "4  [{'title': 'Cast Away'}, {'year': '2000'}, {'r...\n",
       "5  [{'title': 'The Terminal'}, {'year': '2004'}, ...\n",
       "6  [{'title': 'Catch Me If You Can'}, {'year': '2...\n",
       "7  [{'title': 'Bridge of Spies'}, {'year': '2015'...\n",
       "8  [{'title': 'Sully'}, {'year': '2016'}, {'role'...\n",
       "9  [{'title': 'A Beautiful Day in the Neighborhoo..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas `DataFrame`\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(parser_output[\"filmography\"][0][\"actor\"][1][\"movies\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "42e85a6e-0bef-407b-9846-f257782d1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big</td>\n",
       "      <td>1988</td>\n",
       "      <td>Josh Baskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994</td>\n",
       "      <td>Forrest Gump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sleepless in Seattle</td>\n",
       "      <td>1993</td>\n",
       "      <td>Sam Baldwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>1998</td>\n",
       "      <td>Captain John H. Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cast Away</td>\n",
       "      <td>2000</td>\n",
       "      <td>Chuck Noland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Terminal</td>\n",
       "      <td>2004</td>\n",
       "      <td>Viktor Navorski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Catch Me If You Can</td>\n",
       "      <td>2002</td>\n",
       "      <td>Carl Hanratty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bridge of Spies</td>\n",
       "      <td>2015</td>\n",
       "      <td>James B. Donovan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sully</td>\n",
       "      <td>2016</td>\n",
       "      <td>Chesley 'Sully' Sullenberger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Beautiful Day in the Neighborhood</td>\n",
       "      <td>2019</td>\n",
       "      <td>Fred Rogers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  year                          role\n",
       "0                                  Big  1988                   Josh Baskin\n",
       "1                         Forrest Gump  1994                  Forrest Gump\n",
       "2                 Sleepless in Seattle  1993                   Sam Baldwin\n",
       "3                  Saving Private Ryan  1998        Captain John H. Miller\n",
       "4                            Cast Away  2000                  Chuck Noland\n",
       "5                         The Terminal  2004               Viktor Navorski\n",
       "6                  Catch Me If You Can  2002                 Carl Hanratty\n",
       "7                      Bridge of Spies  2015              James B. Donovan\n",
       "8                                Sully  2016  Chesley 'Sully' Sullenberger\n",
       "9  A Beautiful Day in the Neighborhood  2019                   Fred Rogers"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "df_parsed = pd.DataFrame(\n",
    "    [ \n",
    "        (title['title'], year['year'], role['role']) for title, year, role in df.movie.apply(json.dumps).apply(json.loads)\n",
    "    ],\n",
    "    columns=['title', 'year', 'role']\n",
    ")\n",
    "df_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4ad025-ccd9-49e7-9d8b-9e49bc02f823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommaSeparatedListOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `CommaSeparatedListOutputParser()`\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aba25f1-cc3b-4a69-8953-e0f43370f80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93fdb94-4187-4929-9cf7-6b261e4e4434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pop', 'Rock', 'Hip-Hop']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"List 3 main-stream {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "parser_output = chain.invoke({\"subject\": \"music styles\"})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2892d080-bd37-46f6-94f4-807d175c644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a CSV file\n",
    "import csv\n",
    "\n",
    "with open('parser_output.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(parser_output)\n",
    "    # for list of list\n",
    "    # writer.writerows(parser_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86ce31fa-70e2-4b1c-94cd-3c30a3dbdade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnumOutputParser(enum=<enum 'Genders'>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `EnumOutputParser()`\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Genders(Enum):\n",
    "    MALE = \"male\"\n",
    "    FEMALE = \"female\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Genders)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c34b7a57-6c32-4dac-af3f-57fc600cdb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select one of the following options: male, female'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2acafef0-bbe2-40ac-aafe-370fdea36235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Genders.MALE: 'male'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the prompt explicitly instructs the LLM to respond with only one of the enum values\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me the gender of the celebrity {name}. Respond only with 'male' or 'female'.\\n{format_instructions}\",\n",
    "    input_variables=[\"name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"name\": \"Michael Jordan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d6858c7-ca23-47a1-b58e-38d5b504346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.', type='string'),\n",
       " ResponseSchema(name='delivery_days', description='How many days did it take for the product to arrive? If this information is not found, output -1.', type='string'),\n",
       " ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and output them as a comma separated Python list.', type='string')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `StructuredOutputParser()`\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
    "response_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d4095d7-26bc-4ace-9ba4-fea1c4df18a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days did it take for the product to arrive? If this information is not found, output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and output them as a comma separated Python list.', type='string')])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1544cf5c-0587-4005-a59e-6a1cf55a9277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32d22266-6990-45cc-8ea4-286c55eea607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': '2',\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_review = \"\"\"\n",
    "This leaf blower is pretty amazing. It has four settings: candle blower, gentle breeze, windy city, and tornado.\n",
    "It arrived in two days, just in time for my wife's anniversary present.\n",
    "I think my wife liked it so much she was speechless.\n",
    "So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn.\n",
    "It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else?\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=review_template, input_variables=[\"text\"], partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"text\": customer_review})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f0302-2635-417a-9ccc-d72de3403102",
   "metadata": {},
   "source": [
    "## 2-5. Chains\n",
    "\n",
    "1. **LCEL**\n",
    "2. `RunnableWithMessageHistory()`\n",
    "   - Offers several benefits over deprecated `ConversationChain`:\n",
    "       - Stream, batch & async support.\n",
    "       - More flexible memory handling, including the ability to manage memory outside the chain.\n",
    "       - Support for multiple threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e842d4-95b8-4dd8-9ddd-d48129d61904",
   "metadata": {},
   "source": [
    "## 2-6. Chat History & Memory\n",
    "\n",
    "Chat History:\n",
    "\n",
    "1. `langchain_core.chat_history.BaseChatMessageHistory`: \n",
    "2. `langchain_core.chat_history.InMemoryChatMessageHistory(messages)`: Stores messages in a memory list.\n",
    "\n",
    "Memory:\n",
    "- These classes are built with deprecated `ConversationChain`. The drawback of them is you cannot distinguish between different users. You have to create user session management in the backend to keep track.\n",
    "- Not difficult to implement for chat history.\n",
    "\n",
    "3. `langchain.memory.buffer.ConversationBufferMemory`\n",
    "4. `langchain.memory.buffer_window.ConversationBufferWindowMemory`\n",
    "5. `langchain.memory.token_buffer.ConversationTokenBufferMemory`\n",
    "6. `langchain.memory.summary.ConversationSummaryMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580e91c6-0a51-4a15-a43b-ed6aa075329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d19e30f-289a-419c-a83b-9be365d68f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x7be116947a60>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7be116069d00>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7be128f2db20>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc50b19-3dae-4722-a391-b02877861c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function representing the adjacent side over the hypotenuse in a right-angled triangle.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 31, 'total_tokens': 55}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-75e249db-acaa-4237-a3fd-8bbce9e5cbbc-0', usage_metadata={'input_tokens': 31, 'output_tokens': 24, 'total_tokens': 55})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aadc215b-fcca-4886-92ca-f2128d4e5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function that measures the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 65, 'total_tokens': 91}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4975a40-00a6-4057-a848-810de98fde55-0', usage_metadata={'input_tokens': 65, 'output_tokens': 26, 'total_tokens': 91})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7ee2789-2936-42ca-a41d-a9de3fa7cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! How can I assist you with math today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 28, 'total_tokens': 39}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-0e9362f8-1bf1-43a1-9d7b-e51dc1e0d690-0', usage_metadata={'input_tokens': 28, 'output_tokens': 11, 'total_tokens': 39})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3636213-dd26-4d0a-aad1-1eaca982e9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x7be115f5d4e0>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7be115f5d800>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7be115f5d760>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='user_id', annotation=<class 'str'>, name='User ID', description='Unique identifier for the user.', default='', is_shared=True, dependencies=None), ConfigurableFieldSpec(id='conversation_id', annotation=<class 'str'>, name='Conversation ID', description='Unique identifier for the conversation.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c050fce-3a3b-4383-888a-960ce9c95f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I assist you with math today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-521b6cf7-caab-457e-bfed-d216142534e9-0', usage_metadata={'input_tokens': 27, 'output_tokens': 12, 'total_tokens': 39})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"Hello\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4af192-a92a-4bc6-a80e-8a71611391e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, I be doin' well, matey! Ready to set sail on the high seas and plunder some treasures. How be ye doin'?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "chain = prompt_template | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
    "\n",
    "wrapped_chain.invoke(\n",
    "    {\"input\": \"how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"42\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b39d5d5-352b-4450-8ea5-df6d5d7af775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='how are you?'), AIMessage(content=\"Arrr, I be doin' well, matey! Ready to set sail on the high seas and plunder some treasures. How be ye doin'?\")])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d35225-9314-402a-aa84-0f00602db874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a pirate. Answer the following questions as best you can.')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8603b60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8649190>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "  | StrOutputParser(), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x79be9ff86b60>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a pirate. Answer the following questions as best you can.')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8603b60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8649190>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "  | StrOutputParser(), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x79be9ff23ce0>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_message_history at 0x79be9ff23420>, input_messages_key='input', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip3 install redis\n",
    "# docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bb5fc25-a051-4a29-ae23-74c5be54e2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, ye be askin' me about cosine again, eh? The cosine be a trigonometric function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be helpin' us pirates calculate angles and distances when sailin' the high seas. Aye, cosine be a valuable tool in our navigational arsenal, me hearty!\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc4dda6-f131-44e0-8964-332df1433ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, the inverse of the cosine function be called arccosine or cos^-1. It be the function that gives ye the angle whose cosine be a given number. It be useful for findin' the original angle from the cosine value, matey. So if ye know the cosine of an angle, ye can use the arccosine to find the angle itself. Aye, it be a handy tool for us pirates to have in our navigational toolkit!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What's its inverse\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c79c2fad-6929-45ab-b624-06734461890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, the inverse of the cosine function be called arccosine or cos^-1. It be the function that gives ye the angle whose cosine be a given number. It be useful for findin\\' the original angle from the cosine value, matey. So if ye know the cosine of an angle, ye can use the arccosine to find the angle itself. Aye, it be a handy tool for us pirates to have in our navigational toolkit!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What\\'s its inverse\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, ye be askin\\' me about cosine again, eh? The cosine be a trigonometric function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be helpin\\' us pirates calculate angles and distances when sailin\\' the high seas. Aye, cosine be a valuable tool in our navigational arsenal, me hearty!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, me hearty! The cosine be a mathematical function that relates to the angle between two sides of a right triangle. It be calculated by dividing the length of the adjacent side by the length of the hypotenuse. It be helpin\\' us pirates navigate the seas with our trusty maps and charts. Aye, it be a useful tool for us buccaneers!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr matey, ye be talkin\\' about the inverse of a number, eh? The inverse of a number be its reciprocal. So if ye be talkin\\' about the number 5, its inverse be 1/5, savvy? Aye, the inverse be like the opposite of the number, ye see? If ye have any more questions, just ask away!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What\\'s its inverse\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, cosine be a mathematical function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be used to calculate angles and distances in navigation, me heartie.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~$ docker exec -it <CONTAINER ID> redis-cli\n",
    "# 127.0.0.1:6379> KEYS *\n",
    "# 1) \"message_store:foobar\"\n",
    "# 127.0.0.1:6379> LRANGE message_store:foobar 0 -1\n",
    "\n",
    "import redis\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "r.lrange(\"message_store:foobar\", 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4562fb-9186-40d2-bc7a-42528cd022c8",
   "metadata": {},
   "source": [
    "## 2-7. Document Loaders\n",
    "\n",
    "LangChain has hundreds of integrations with various data sources to load data from, such as Google Drive, Amazon Simple Storage Service (Amazon S3), Blockchain, Microsoft PowerPoint & more.\n",
    "- From `langchain_community`. Additional setups may be required for specific loaders.\n",
    "- [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "\n",
    "Text:\n",
    "\n",
    "1. `TextLoader`\n",
    "\n",
    "File Directory:\n",
    "\n",
    "2. `DirectoryLoader`\n",
    "\n",
    "CSV:\n",
    "\n",
    "3. `CSVLoader`\n",
    "\n",
    "JSON:\n",
    "\n",
    "4. `JSONLoader`\n",
    "\n",
    "HTML:\n",
    "\n",
    "5. `UnstructuredHTMLLoader`\n",
    "\n",
    "PDF:\n",
    "\n",
    "6. `PyPDFLoader` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548f4b8d-f1ea-4be2-8278-e12aca7d403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './README.md'}, page_content='# PyTorch Natural Language Processing\\n')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./README.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a5b5591-7c47-4e7e-bdf0-81c153ca9dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading https://www.nytimes.com/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_community/document_loaders/text.py:42\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://www.nytimes.com/'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredHTMLLoader\n\u001b[1;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.nytimes.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_community/document_loaders/text.py:58\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     60\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)}\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading https://www.nytimes.com/"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = TextLoader(\"https://www.nytimes.com/\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a82ba5d2-1768-4b58-aff1-fbff8510877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/1706.03762v7.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pypdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/1706.03762v7.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba2f4c-c2e3-45a3-b120-34084b59d317",
   "metadata": {},
   "source": [
    "## 2-8. Text Splitters\n",
    "\n",
    "Base interface:\n",
    "\n",
    "1. `langchain_text_splitters.base.TextSplitter(chunk_size=4000, chunk_overlap=200, length_function=<built-in function len>, keep_separator=False, add_start_index=False, strip_whitespace=True)`: All LangChain text splitters inherit from. To maintain context between chunks, setting `chunk_overlap` can ensure information isn't lost at chunk boundaries. `add_start_index` includes chunk's start index in metadata if `True`.\n",
    "    - `split_text(text)`\n",
    "    - `from_tiktoken_encoder(encoding_name='gpt2', model_name=None, allowed_special={}, disallowed_special='all', **kwargs)`\n",
    "    - `from_huggingface_tokenizer(tokenizer, **kwargs)`\n",
    "\n",
    "Semantic chunking:\n",
    "\n",
    "Split by tokens:\n",
    "\n",
    "1. `langchain_text_splitters.base.TokenTextSplitter(encoding_name='gpt2', model_name=None, allowed_special={}, disallowed_special='all', **kwargs)`: Splits text to tokens using model tokenizer.\n",
    "2. `langchain_text_splitters.character.CharacterTextSplitter(separator='\\n\\n', is_separator_regex=False, **kwargs)`: Splits text that looks at characters.\n",
    "3. `langchain_text_splitters.character.RecursiveCharacterTextSplitter(separators=None, keep_separator=True, is_separator_regex=False, **kwargs)`: Splits text by recursively look at characters.\n",
    "   - `from_language(language, **kwargs)`: splits code.\n",
    "\n",
    "Split HTML:\n",
    "\n",
    "Split Markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f3f6e2b-4e94-4cec-b471-61edc2a2301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1992, which is longer than the specified 1000\n",
      "Created a chunk of size 1317, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1849, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Created a chunk of size 1370, which is longer than the specified 1000\n",
      "Created a chunk of size 1265, which is longer than the specified 1000\n",
      "Created a chunk of size 1465, which is longer than the specified 1000\n",
      "Created a chunk of size 1624, which is longer than the specified 1000\n",
      "Created a chunk of size 1227, which is longer than the specified 1000\n",
      "Created a chunk of size 1144, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 1439, which is longer than the specified 1000\n",
      "Created a chunk of size 1970, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                        Enter Ghost.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `CharacterTextSplitter()`\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"./datasets/hamlet.txt\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\", chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57c8768e-29d8-4524-a513-688bf5df4c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db526a085e34466899680815ef484b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a5cb3554264c26990a1b356f7382f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1340475a8e4a8388a4cb1ae343000c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220a07645aca4434b25c7c2d0e00611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8849a11670484664a03f33b95bbd0e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Created a chunk of size 1323, which is longer than the specified 1000\n",
      "Created a chunk of size 2474, which is longer than the specified 1000\n",
      "Created a chunk of size 1575, which is longer than the specified 1000\n",
      "Created a chunk of size 1368, which is longer than the specified 1000\n",
      "Created a chunk of size 1023, which is longer than the specified 1000\n",
      "Created a chunk of size 1676, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 2120, which is longer than the specified 1000\n",
      "Created a chunk of size 1403, which is longer than the specified 1000\n",
      "Created a chunk of size 1622, which is longer than the specified 1000\n",
      "Created a chunk of size 1537, which is longer than the specified 1000\n",
      "Created a chunk of size 1880, which is longer than the specified 1000\n",
      "Created a chunk of size 2063, which is longer than the specified 1000\n",
      "Created a chunk of size 1411, which is longer than the specified 1000\n",
      "Created a chunk of size 1718, which is longer than the specified 1000\n",
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1628, which is longer than the specified 1000\n",
      "Created a chunk of size 2732, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hugging Face tokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de72f406-9e10-4b10-bf60-92e3e07134d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                        Enter Ghost.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `RecursiveCharacterTextSplitter()`\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(model_name=\"gpt-4\", chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf241555-61f4-4afc-9313-c88c283a6ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split code\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "python_code = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=50, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents([python_code])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4b7b0d9-21ac-4c79-9dd6-4305e0ca4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                  \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `TokenTextSplitter()`\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7a63263-f368-4b65-abab-1ff6ba1b094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  \\n\\nMarcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  \\n\\nPolonius, Lord Chamberlain.\\n  \\n\\nHoratio, friend to Hamlet.\\n  \\n\\nLaertes, son to Polonius.\\n  \\n\\nVoltemand, courtier.\\n  \\n\\nCornelius, courtier.\\n  \\n\\nRosencrantz, courtier.\\n  \\n\\nGuildenstern, courtier.\\n  \\n\\nOsric, courtier.\\n  \\n\\nA Gentleman, courtier.\\n  \\n\\nA Priest.\\n  \\n\\nMarcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  \\n\\nTwo Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  \\n\\nA Norwegian Captain.\\n  \\n\\nEnglish Ambassadors.\\n\\n  \\n\\nGetrude, Queen of Denmark, mother to Hamlet.\\n  \\n\\nOphelia, daughter to Polonius.\\n\\n  \\n\\nGhost of Hamlet's Father.\\n\\n  \\n\\nLords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\n\\n\\nACT I. Scene I.\\nElsinore.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `SpacyTextSplitter()`\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867e640-0bc7-4e78-94da-05113c58d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `SentenceTransformersTokenTextSplitter()`\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3f55948-590b-4ec5-9b7d-45b9f4186365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n\\nMarcellus, Officer.\\n\\nHamlet, son to the former, and nephew to the present king.\\n\\nPolonius, Lord Chamberlain.\\n\\nHoratio, friend to Hamlet.\\n\\nLaertes, son to Polonius.\\n\\nVoltemand, courtier.\\n\\nCornelius, courtier.\\n\\nRosencrantz, courtier.\\n\\nGuildenstern, courtier.\\n\\nOsric, courtier.\\n\\nA Gentleman, courtier.\\n\\nA Priest.\\n\\nMarcellus, officer.\\n\\nBernardo, officer.\\n\\nFrancisco, a soldier\\n  Reynaldo, servant to Polonius.\\n\\nPlayers.\\n\\nTwo Clowns, gravediggers.\\n\\nFortinbras, Prince of Norway.\\n\\nA Norwegian Captain.\\n\\nEnglish Ambassadors.\\n\\nGetrude, Queen of Denmark, mother to Hamlet.\\n\\nOphelia, daughter to Polonius.\\n\\nGhost of Hamlet's Father.\\n\\nLords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\nACT I.\\n\\nScene I.\\nElsinore.\\n\\nA platform before the Castle.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `NLTKTextSplitter()`\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda38b0c-dffd-433a-ba2b-b5ba03179066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e27546-f614-42f1-91f7-de0da6485c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28910bda-ac14-432f-9cea-34adb43b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `MarkdownHeaderTextSplitter()`\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64dcc0-6071-4fcd-8768-1ec8f7b3d264",
   "metadata": {},
   "source": [
    "## 2-9. Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc89ce-17b6-4577-aa6c-4a3f10506499",
   "metadata": {},
   "source": [
    "## 2-10. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ee879-b96d-401b-987a-a1c7bae7c6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
