{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b76742-6ef1-4a5d-bdcf-8071dd0a9826",
   "metadata": {},
   "source": [
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples. Also called **Zero-Shot Learning**.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Few-Shot Learning**, **Example-Based Prompting**, **Prompt Augmentation**, or **Demonstration Learning**.\n",
    "    - **Sample Selection**\n",
    "    - **Sample Ordering**\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\". \n",
    "    - **Role-Based:** Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the steps**\n",
    "    - **Delimiters:** Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format:** **Data-Structured Prompting (DSP)** uses structured data formats, such as tables, lists, or specific schemas.\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then replace the placeholders in the template. Prompt templating also allows for prompts to be stored, reused, shared, and programmed. Also called **Template-Based Prompting**. \n",
    "8. **Emotional Prompting:** Asks for tone adjustment.\n",
    "9. **Laddering Prompting:** Break a very complex problem into multiple smaller prompts and problems instead of sending one huge prompt in one step. Alternative names of this approach include **Prompt Composition**, **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "10. **Prompt Decomposition**\n",
    "11. **Ask-Before-Answer Prompting:** Asks the LLM which extra information would improve the result and the generated content.\n",
    "12. **Self-Evaluative Prompting**: Asks the LLM to be self-critical. To evaluate and improve the responses it generated.\n",
    "13. **Reverse Prompting:** Reverse Roles. Asks the LLM which prompt should be used to produce an output similar to a provided output.\n",
    "14. **Prompt Ensembling:** Uses multiple unanswered prompts for an input. Multiple predictions result from different prompt variants. Usually combines results of multiple systems to get better performance.\n",
    "15. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "16. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "17. **Few-Shot Chain-of-Thought (CoT) Prompting:**\n",
    "18. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "19. **Self-Consistency**\n",
    "20. **Logical Chain-of-Thought (LogiCoT) Prompting:**\n",
    "21. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "22. **Tree-of-Thoughts (ToT) Prompting:**\n",
    "23. **Graph-of-Thought (GoT) Prompting:**\n",
    "24. **System 2 Attention Prompting:**\n",
    "25. **Thread-of-Thought (ThoT) Prompting:**\n",
    "26. **Contrastive Chain-of-Thought (CCoT):** \n",
    "    - [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277)\n",
    "27. **Chain-of-Table Prompting:**\n",
    "28. **Self-Ask Prompting (SA):**\n",
    "29. **Retrieval Augmented Generation (RAG):**\n",
    "    - **Naive RAG:**\n",
    "    - **Advanced RAG:**\n",
    "    - **Modular RAG:**\n",
    "30. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "31. **ReAct Prompting:**\n",
    "32. **Chain-of-Verification (CoVe) Prompting:**\n",
    "33. **Chain-of-Note (CoN) Prompting:**\n",
    "34. **Chain-of-Knowledge (CoK) Prompting:**\n",
    "35. **Active Prompting**\n",
    "    - [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246)\n",
    "36. **Automatic Prompt Engineer (APE):** Uses one LLM to beam search over prompts for another LLM.\n",
    "37. **Automatic Reasoning and Tool-use (ART)**\n",
    "38. **Emotion Prompting:**\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "39. **Scratchpad Prompting:**\n",
    "40. **Program-of-Thoughts (PoT) Prompting:**\n",
    "41. **Structured Chain-of-Thought (SCoT) Prompting:**\n",
    "42. **Chain-of-Code (CoC) Prompting:**\n",
    "43. **Optimization by Prompting:**\n",
    "44. **Rephrase & Respond (RaR) Prompting:**\n",
    "    - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n",
    "45. **Take a Step Back Prompting**\n",
    "46. **Complexity-Based Prompting:**\n",
    "    - [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)\n",
    "47. **Self-Refine**\n",
    "    - [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n",
    "48. **Generated Knowledge Prompting:**\n",
    "    - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "49. **Prompt Chaining:** Creates a series of LLM calls follow on each other with the output of one prompt in the chain serving as the input of the following.\n",
    "50. **Prompt Sharing:** Prompt templates are partially shared.\n",
    "51. **Prompt Pipelining:** The variables or placeholders in the pre-defined prompt template are populated (prompt injection) with the question from the user and the knowledge to be searched from the knowledge store.\n",
    "52. **Stuff Documents Chain**\n",
    "53. **Reduce Documents Chain**\n",
    "54. **Map Reduce Documents Chain**\n",
    "55. **Refine Documents Chain**\n",
    "56. **Map Rerank Documents Chain**\n",
    "57. **Maieutic Prompting**\n",
    "    - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)\n",
    "58. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)\n",
    "59. **Program-Aided Language Models (PAL)**\n",
    "    - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "60. **Reflexion**\n",
    "    - [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n",
    "61. **Multimodal Chain-of-Thought (CoT) Prompting**\n",
    "    - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)\n",
    "62. **Graph Prompting:**\n",
    "    - [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://arxiv.org/abs/2302.08043)\n",
    "63. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "64. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "65. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "66. **Least-to-Most Prompting**\n",
    "    - [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n",
    "67. **Autonomous Agents:** Prompt Chaining is the execution of a predetermined and set sequence of actions. However, agents can maintain a high level of autonomy.\n",
    "68. **Prompt Tuning:** **Soft Prompting**\n",
    "69. **Retriever Fine-Tuning**\n",
    "70. **Collaborative Fine-Tuning**\n",
    "71. **Generator Fine-Tunning**\n",
    "99. **Prompt Injection Attack**\n",
    "    - **Prompt Takeovers**\n",
    "    - **Prompt Leaks**\n",
    "100. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc6bc-8807-4722-8573-8a18bdd917a9",
   "metadata": {},
   "source": [
    "# 2. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "- `langchain-core`: Base abstractions of components and **LangChain Expression Language (LCEL)**.\n",
    "- `langchain-community`: Third party integrations.\n",
    "    - `langchain-openai`\n",
    "    - `langchain-huggingface`\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langgraph`\n",
    "\n",
    "## 2-1. Chat Models & LLMs\n",
    "Chat models use a sequence of messages as inputs and return chat messages as outputs. Traditionally newer models. Inherited from `langchain_core.language_models.chat_models.BaseChatModel`.\n",
    "\n",
    "1. `langchain_openai.chat_models.base.ChatOpenAI`: OpenAI chat model integration.\n",
    "   - `model`\n",
    "   - `n`: Number of responses.\n",
    "   - `temperature`: Adds randomness to responses. Higher values make answers more diverse, while lower values make them more focused and deterministic. \n",
    "   - `timeout`: Request timeout.\n",
    "   - `stop`: Provides a list of stop words to prevent the model from generating responses containing those specific words.\n",
    "   - `max_tokens`: Max tokens to generate.\n",
    "   - `max_retries`: Max number of times to retry requests.\n",
    "   - `api_keys`\n",
    "   - `base_url`: Endpoint to Send Requests to.\n",
    "   - `model_kwargs`: Holds model parameters valid for `openai.OpenAI.chat.completions.create(messages, model, stream, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)` call not explicitly specified.\n",
    "       - `top_p`: **Nucleus Sampling** controls the diversity and quality of the responses. Limits the cumulative probability of the most likely tokens. Higher values allow more tokens, leading to diverse responses, while lower values provide more focused and constrained answers.\n",
    "       - `frequency_panelty`: Controls the model's tendency to generate repetitive words or phrases. Higher values encourage the model to explore more diverse and novel responses; lower values make the model more likely to repeat information.\n",
    "       - `presence_penalty`: Controls avoidance of certain topics. Higher values will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "       - [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "       - [openai-python/src/openai/resources/chat/completions.py](https://github.com/openai/openai-python/blob/main/src/openai/resources/chat/completions.py)  \n",
    "\n",
    "Pure text-in/text-out LLMs tends to be older or lower-level. Inherited from `langchain_core.language_models.llms.BaseLLM`.\n",
    "\n",
    "2. `langchain_openai.llms.base.OpenAI`: OpenAI large language models. Inherited from `langchain_openai.llms.base.BaseOpenAI`.\n",
    "\n",
    "Both `BaseChatModel` and `BaseLLM` classes are inherited from `langchain_core.language_models.base.BaseLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598cecb0-7935-4a89-8ed1-b25e3f54617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9072be63-0000-4fc5-a0d7-b2a6288a5b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x7374b40dbe30>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7374abf2e5d0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14082095-55d8-4cb2-bb18-abafa7274023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b58327a5-f9a8-4e48-93f4-b8a3ef1e6643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAs of 2021, the estimated population of Barcelona is 1.6 million.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.predict()`\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, max_tokens=256, timeout=None, max_retries=2)\n",
    "\n",
    "prompt = \"How many citizens does Barcelona have?\"\n",
    "response = llm.predict(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6f64d3a-386d-4e98-aa31-2de6e52be7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGeorge Washington was the first president of the United States.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "prompt = \"Who was the first president of the United States?\"\n",
    "response = llm(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbe64930-d8fe-4365-b82d-0ec193a19b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMichael Jordan is a retired American professional basketball player who is widely considered one of the greatest basketball players of all time. He played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards, winning six NBA championships and earning numerous individual awards, including five MVP awards. He is known for his incredible athleticism, scoring ability, and competitive drive, and is often credited with popularizing the game of basketball around the world. After retiring from basketball, Jordan became a successful businessman and owner of the Charlotte Hornets NBA team.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.invoke()`\n",
    "prompt = \"Who is Michael Jordan?\"\n",
    "response = llm.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b18696fa-14be-49a2-9b3d-aa9659343ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x76cd3d540b60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x76cd3d541ee0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI()`\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9429f382-fb9e-425e-97ea-e9494d05de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x76cd3d5436b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x76cd3d564a40>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2, model_kwargs={\"top_p\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e079e8b-afbc-490b-9abf-39c2388571f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 24, 'total_tokens': 87}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-62c75271-a1e7-49cc-b4c7-79908fbf8e9c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 63, 'total_tokens': 87})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI.invoke()`\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"How many citizens does Barcelona have?\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7c0dbc6-0151-4bcb-a082-3bef64f519af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d45a0338-f12e-4aaa-ba21-8d6cbe0b8df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9kguKKA9vSKQf2Frv59X5clPddG7T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\", role='assistant', function_call=None, tool_calls=None))], created=1720915052, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_d33f7b429e', usage=CompletionUsage(completion_tokens=79, prompt_tokens=14, total_tokens=93))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `openai.chat.completions.create()`\n",
    "import openai\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How many citizens does Barcelona have?\"\n",
    "}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, temperature=0)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6b7bacc-d874-4410-ba0b-e6a5b3094262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d19af-ccb6-435f-860c-461b5fec96f7",
   "metadata": {},
   "source": [
    "## 2-2. Prompt Templates\n",
    "Prompt templates:\n",
    "\n",
    "1. `langchain_core.prompts.prompt.PromptTemplate(input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags, template, template_format, validate_template)`: `str` prompt templates.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `from_template(template, template_format, partial_variables, **kwargs)`\n",
    "   - `from_file(template_file, input_variables, **kwargs)`\n",
    "   - `from_examples(examples, prefix, suffix, input_variables, example_separator, **kwargs)`: Few-shot prompt templates. Takes examples in list format with prefix and suffix to create a prompt.\n",
    "\n",
    "2. `langchain_core.prompts.chat.ChatPromptTemplate()`: Formats a list of messages. Alternatively, you can also construct the prompt using message role prompt templates described belowed. \n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Returns a list of finalized messages.\n",
    "   - `from_messages(message, template_format)`: Create a chat prompt template from a variety of message formats.\n",
    "\n",
    "Message role prompt templates:\n",
    "\n",
    "3. `langchain_core.prompts.chat.SystemMessagePromptTemplate(additional_kwargs, prompt)`: for optional `system` message role used to set the behavior of the assistant.\n",
    "4. `langchain_core.prompts.chat.HumanMessagePromptTemplate(additional_kwargs, prompt)`: for `user` message role who provides requests.\n",
    "5. `langchain_core.prompts.chat.AIMessagePromptTemplate(additional_kwargs, prompt)`: for `assistant` message role storing previous assistant responses or giving examples of few-shot prompting.\n",
    "\n",
    "- All of the three message role prompt templates aboved have the following methods:\n",
    "    - `format(**kwargs)`\n",
    "    - `format_messages(**kwargs)`\n",
    "    - `from_template(template, template_format, partial_variables, **kwargs)`: Default format of the template is `f-string`.\n",
    "    - `from_template_file(template_file, input_variables, **kwargs)`\n",
    "\n",
    "Few-shot prompt templates:\n",
    "\n",
    "6. `langchain_core.prompts.few_shot.FewShotPromptTemplate(example_prompt, example_selector, example_separator, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, prefix, suffix, tags, template_format, validate_template)`: Prompt templates that contains few shot examples.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "\n",
    "7. `langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate(example_prompt, example_selector, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags)`: Chat prompt templates that supports few-shot examples. Because there is no `suffix` to assign like `FewShotPromptTemplate`, you need one more final prompt to wrap the template.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Formats `**kwargs` into a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb55ef54-f14f-4386-af08-b931cebf36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I want to open a restaurant for Italian food. Suggest a fency name for this.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PromptTemplate()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "messages = prompt_template.invoke({\"cuisine\": \"Italian\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40bdba48-aa25-4a74-9d8b-413fa551abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Italian food. Suggest a fency name for this.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(cuisine=\"Italian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ebd4837-3d99-457d-b267-fb18b25fce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How about \"La Dolce Vita Ristorante\"? This name evokes the charm and elegance of Italian culture, suggesting a delightful and luxurious dining experience.', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 25, 'total_tokens': 53}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-e17ba7b7-3502-4490-a811-6ab506eceabc-0', usage_metadata={'input_tokens': 25, 'output_tokens': 28, 'total_tokens': 53})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74ca26e1-91bd-4412-8f9c-033b6582caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_template()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7405f89b-7828-4b85-a774-bd376f7aa749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cats.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(topic=\"cats\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad297-3124-4826-92b5-1a982c3b4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `from_file()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf46caee-8a03-4b3d-b949-944de31ff683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_examples(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fff62a5-308b-4d93-90b6-6394245fd647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures constructed from basic materials such as wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are compartmentalized into various sections for different activities. These structures lack the advanced climate control and adaptive architecture found in Martian habitats. Instead, they rely on rudimentary heating and cooling systems and are often cluttered with an array of unnecessary objects and decorations.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few-shot prompting without template\n",
    "messages = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\n",
    "Question: What is human cuisine like?\n",
    "Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "\n",
    "Question: What is human entertainment?\n",
    "Response: Crude moving images and loud sounds.\n",
    "\n",
    "Question: What are human homes like?\n",
    "Response: \"\"\"\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b6141f4a-795d-40e4-a536-220e36a4662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['userInput'], template=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: {userInput}\\nResponse: \\n\")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "examples = [\n",
    "    \"\"\"\n",
    "    Question: What is human cuisine like?\n",
    "    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Question: What is human entertainment?\n",
    "    Response: Crude moving images and loud sounds.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "prefix = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \n",
    "\"\"\"\n",
    "\n",
    "example_template = PromptTemplate.from_examples(examples=examples, prefix=prefix, suffix=suffix, input_variables=['userInput'], example_separator=\"\\n\\n\")\n",
    "example_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52a27edf-9a2d-4134-a577-e251db5d78de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = example_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c87ac348-d28c-416e-928b-0756a33d626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are often segmented into various compartments for different activities such as sleeping, eating, and socializing. These structures lack the advanced adaptive and self-sustaining features of our Martian habitats.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7244116b-8e9b-4a06-a982-9cf6e2bc0811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatPromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eecea368-a6b2-4b65-9f80-b0b2a092c75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9e1ac11-490a-4ab4-a083-efa9be8c0515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "prompt_template = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\")\n",
    "    + HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    ")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce459097-dd4b-4267-a66a-23202e736b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "940e0bb4-b9cd-4451-9bcd-bd055ddfc8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotPromptTemplate()`\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is human cuisine like?\",\n",
    "        \"answer\": \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is human entertainment?\",\n",
    "        \"answer\": \"Crude moving images and loud sounds.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Question: {query}\n",
    "Response: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cc0c496-aee5-4482-bd12-900fc49014f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: ', prefix='You are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31eb9c81-c6cb-4477-a74d-af8f22e8ac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d2be239-7582-40e9-89a9-cc76d552063b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Their homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. These structures are designed to provide shelter from environmental elements and are often divided into multiple rooms for different activities. The layout and design lack the advanced spatial efficiency and adaptive technology found in Martian habitats.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2874b0b3-03da-4d14-a119-a1adea11012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotChatMessagePromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"1+1\",\n",
    "        \"output\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"3+5\",\n",
    "        \"output\": \"8\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"{output}\")\n",
    "])\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "113100d5-dfe1-44c3-8929-820a9c743d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(examples=examples, example_prompt=example_prompt, input_variables=[])\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6855621-ec2c-4dae-b48c-38b4bacb455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt = few_shot_prompt_template.invoke({})\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5a5d74b-18b4-4a92-8e13-f07eb6e31bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        ChatPromptTemplate.from_messages(few_shot_prompt.to_messages()),\n",
    "        (\"user\", \"{userInput}\"),\n",
    "    ]\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages\n",
    "# chain = final_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "# chain.invoke({\"input\":\"What's the square of a triangle?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00a82d26-b889-4246-a1cd-5149c28d245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"square of a triangle\" is not a standard mathematical concept. However, if you are referring to the area of a triangle, you can calculate it using the formula:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, please provide more context so I can assist you better.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5b1d44d-ab07-439b-84cb-439cedfed6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few shot prompting with `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=(\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\") \n",
    "        + AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "    ),\n",
    "    input_variables=[]\n",
    ")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c04cba-3f57-4d99-b595-271d70880e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='1+1'), AIMessage(content='2'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\") \n",
    "    + few_shot_prompt_template\n",
    "    + HumanMessagePromptTemplate.from_template(\"{userInput}\")\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f333b5d7-2816-4830-a028-6883853d31eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like there might be a bit of confusion in your question. The term \"square\" typically refers to a specific geometric shape or the result of multiplying a number by itself. However, a triangle is a different geometric shape and doesn\\'t have a \"square\" in the same sense.\\n\\nIf you are asking about the area of a triangle, the formula to calculate it is:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, could you please clarify?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb132aac-9db1-45c1-8f43-31225683701f",
   "metadata": {},
   "source": [
    "## 2-3. Example Selectors\n",
    "1. `LengthBasedExampleSelector`\n",
    "2. `SemanticSimilarityExampleSelector`\n",
    "3. `MaxMarginalRelevanceExampleSelector`\n",
    "4. `NGramOverlapExampleSelector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f95a70d6-4d81-43ba-9ef7-a54cf30ef742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Crude moving images and loud sounds.',\n",
       "  'query': 'What is human entertainment?'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples=examples, embeddings=OpenAIEmbeddings(), vectorstore_cls=Chroma, k=1)\n",
    "query = \"What is human brain made of?\"\n",
    "selected_examples = example_selector.select_examples({\"query\": query})\n",
    "selected_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e287f9ff-5710-4a39-ac79-e9d6d6d9790d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7374a87785c0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bee3b848-7b35-4fb1-88e0-1da6bac13f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae3c2be4-edf8-47b1-88e8-abab6c7ef2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are enclosed structures made from various materials such as wood, brick, and metal. They are designed to provide shelter and comfort, often containing multiple rooms with specific functions like sleeping, cooking, and socializing. These structures are equipped with systems for temperature control, water supply, and waste disposal. Humans decorate their homes with objects and colors that reflect their personal tastes and cultural influences.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2f33-c03e-48c4-adfd-9acaf7c8a578",
   "metadata": {},
   "source": [
    "## 2-4. Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee0922-c381-484f-8c4b-3cbb79f144d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33e842d4-95b8-4dd8-9ddd-d48129d61904",
   "metadata": {},
   "source": [
    "## 2-5. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347e5e-541f-4a01-8d6b-07695e9753d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
