{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b76742-6ef1-4a5d-bdcf-8071dd0a9826",
   "metadata": {},
   "source": [
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples. Also called **Zero-Shot Learning**.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Few-Shot Learning**, **Example-Based Prompting**.\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\". \n",
    "    - **Role-Based:** Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the steps**\n",
    "    - **Delimiters:** Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format:** **Data-Structured Prompting (DSP)** uses structured data formats, such as tables, lists, or specific schemas.\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then replace the placeholders in the template. Prompt templating also allows for prompts to be stored, reused, shared, and programmed. Also called **Template-Based Prompting**. \n",
    "8. **Emotional Prompting:** Asks for tone adjustment.\n",
    "9. **Laddering Prompting:** Break a very complex problem into multiple smaller prompts and problems instead of sending one huge prompt in one step. Alternative names of this approach include **Prompt Composition**, **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "10. **Prompt Decomposition**\n",
    "11. **Ask-Before-Answer Prompting:** Asks the LLM which extra information would improve the result and the generated content.\n",
    "12. **Self-Evaluative Prompting**: Asks the LLM to be self-critical. To evaluate and improve the responses it generated.\n",
    "13. **Reverse Prompting:** Reverse Roles. Asks the LLM which prompt should be used to produce an output similar to a provided output.\n",
    "14. **Prompt Ensembling:** Uses multiple unanswered prompts for an input.\n",
    "16. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps. Also called **Prompt Augmentation**, **Demonstration Learning**.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "17. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "18. **Few-Shot Chain-of-Thought (CoT) Prompting:**\n",
    "19. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "20. **Self-Consistency**\n",
    "21. **Logical Chain-of-Thought (LogiCoT) Prompting:**\n",
    "22. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "23. **Tree-of-Thoughts (ToT) Prompting:**\n",
    "24. **Graph-of-Thought (GoT) Prompting:**\n",
    "25. **System 2 Attention Prompting:**\n",
    "26. **Thread-of-Thought (ThoT) Prompting:**\n",
    "27. **Contrastive Chain-of-Thought (CCoT):** \n",
    "    - [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277)\n",
    "28. **Chain-of-Table Prompting:**\n",
    "29. **Self-Ask Prompting (SA):**\n",
    "30. **Retrieval Augmented Generation (RAG):**\n",
    "    - **Naive RAG:**\n",
    "    - **Advanced RAG:**\n",
    "    - **Modular RAG:**\n",
    "32. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "33. **ReAct Prompting:**\n",
    "34. **Chain-of-Verification (CoVe) Prompting:**\n",
    "35. **Chain-of-Note (CoN) Prompting:**\n",
    "36. **Chain-of-Knowledge (CoK) Prompting:**\n",
    "37. **Active Prompting**\n",
    "    - [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246)\n",
    "38. **Automatic Prompt Engineer (APE):** Uses one LLM to beam search over prompts for another LLM.\n",
    "39. **Automatic Reasoning and Tool-use (ART)**\n",
    "40. **Emotion Prompting:**\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "41. **Scratchpad Prompting:**\n",
    "42. **Program-of-Thoughts (PoT) Prompting:**\n",
    "43. **Structured Chain-of-Thought (SCoT) Prompting:**\n",
    "44. **Chain-of-Code (CoC) Prompting:**\n",
    "45. **Optimization by Prompting:**\n",
    "46. **Rephrase & Respond (RaR) Prompting:**\n",
    "    - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n",
    "47. **Take a Step Back Prompting**\n",
    "48. **Complexity-Based Prompting:**\n",
    "    - [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)\n",
    "49. **Self-Refine**\n",
    "    - [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n",
    "50. **Generated Knowledge Prompting:**\n",
    "    - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "47. **Prompt Chaining:** \n",
    "48. **Prompt Sharing:** Prompt templates are partially shared.\n",
    "49. **Prompt Pipelining**\n",
    "50. **Maieutic Prompting**\n",
    "    - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)\n",
    "51. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)\n",
    "52. **Program-Aided Language Models (PAL)**\n",
    "    - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "53. **Reflexion**\n",
    "    - [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n",
    "54. **Multimodal Chain-of-Thought (CoT) Prompting**\n",
    "    - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)\n",
    "55. **Graph Prompting:**\n",
    "    - [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://arxiv.org/abs/2302.08043)\n",
    "56. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "57. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "58. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "59. **Least-to-Most Prompting**\n",
    "    - [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n",
    "60. **Autonomous Agents**\n",
    "61. **Prompt Tuning:** **Soft Prompting**\n",
    "62. **Retriever Fine-Tuning**\n",
    "64. **Collaborative Fine-Tuning**\n",
    "65. **Generator Fine-Tunning**\n",
    "99. **Prompt Injection:**\n",
    "    - **Prompt Takeovers**\n",
    "    - **Prompt Leaks**\n",
    "100. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc6bc-8807-4722-8573-8a18bdd917a9",
   "metadata": {},
   "source": [
    "# 2. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "- `langchain-core`: Base abstractions of components and **LangChain Expression Language (LCEL)**.\n",
    "- `langchain-community`: Third party integrations.\n",
    "    - `langchain-openai`\n",
    "    - `langchain-huggingface`\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langgraph`\n",
    "\n",
    "## 2-1. Chat Models & LLMs\n",
    "Chat models use a sequence of messages as inputs and return chat messages as outputs. Traditionally newer models. Inherited from `langchain_core.language_models.chat_models.BaseChatModel`.\n",
    "\n",
    "1. `langchain_openai.chat_models.base.ChatOpenAI`: OoenAI chat model integration.\n",
    "   - `model`\n",
    "   - `temperature`\n",
    "   - `timeout`\n",
    "   - `max_tokens`\n",
    "   - `stop`\n",
    "   - `max_retries`\n",
    "   - `api_keys`\n",
    "   - `base_url`\n",
    "\n",
    "Pure text-in/text-out LLMs tends to be older or lower-level. Inherited from `langchain_core.language_models.llms.BaseLLM`.\n",
    "\n",
    "2. `langchain_openai.llms.base.OpenAI`: OpenAI large language models. Inherited from `langchain_openai.llms.base.BaseOpenAI`.\n",
    "\n",
    "Both `BaseChatModel` and `BaseLLM` classes are inherited from `langchain_core.language_models.base.BaseLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598cecb0-7935-4a89-8ed1-b25e3f54617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9072be63-0000-4fc5-a0d7-b2a6288a5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As of 2021, Barcelona has a population of approximately 1.6 million citizens.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = \"How many citizens does Barcelona have?\"\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f64d3a-386d-4e98-aa31-2de6e52be7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "George Washington was the first president of the United States. He served from 1789 to 1797.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who was the first president of the United States?\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe64930-d8fe-4365-b82d-0ec193a19b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Michael Jordan is a retired professional basketball player who is widely considered one of the greatest basketball players of all time. He played for the Chicago Bulls for most of his career, winning six NBA championships and earning numerous awards and accolades, including five NBA MVP awards. He is known for his incredible athleticism, scoring ability, and competitiveness on the court. Jordan is also a successful businessman and owner of the Charlotte Hornets NBA team.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is Michael Jordan?\"\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7e6665-dcc3-46c4-8e4a-b26c38cd6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.9, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18696fa-14be-49a2-9b3d-aa9659343ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4d19af-ccb6-435f-860c-461b5fec96f7",
   "metadata": {},
   "source": [
    "## 2-2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
