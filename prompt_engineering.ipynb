{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa7d1a0-7028-4b23-a2d5-e83b77cef868",
   "metadata": {},
   "source": [
    "Standard prompting:\n",
    "\n",
    "1. **Zero-Shot Prompting:** Directly instructs the LLM to perform a task without any additional examples. Also called **Zero-Shot Learning**.\n",
    "2. **One-Shot Prompting** \n",
    "3. **Few-Shot Prompting:** Prompts the LLM for a response with examples or demonstrations about the task you want it to achieve. Also called **Few-Shot Learning**, **Example-Based Prompting**, **Prompt Augmentation**, or **Demonstration Learning**.\n",
    "    - **Exemplar Generation**\n",
    "    - **Exemplar Selection**\n",
    "    - **Exemplar Ordering**\n",
    "4. **Perspective Prompting:** Of single or multiple perspectives. Determine the distinct roles that you want the LLM to assume. Also called **Role Prompting**, **Persona-Based Prompting**.\n",
    "5. **Contextual Prompting:** Requests for additional considerations by providing relevant information or constraints. Also called **Context-Based Prompting**.\n",
    "    - **Activity-Based:** Such as \"shopping\".\n",
    "    - **Event-Based:** Such as \"department store anniversary sales\".\n",
    "    - **Role-Based:** Defines target audience. Such as \"customer\".\n",
    "    - **Behavior-Based:** Such as \"decision\".\n",
    "    - **Time-Based:** Such as \"Thanksgiving\".\n",
    "    - **Location-Based:** Such as \"Macy's\".\n",
    "6. **Instructional Prompting:** Explicitly guides the LLM to perform specific tasks. Also call **Instruction-Based Prompting**.\n",
    "    - **Detailed Instructions**\n",
    "    - **Specify the Steps**\n",
    "    - **Delimiters:** Uses three quotes (`\"\"\"`), or three dashes (`---`), or three sharps (`###`), to separate instructions from content.\n",
    "    - **Specify Length**\n",
    "    - **Specify Format:** **Data-Structured Prompting (DSP)** uses structured data formats, such as tables, lists, or specific schemas.\n",
    "7. **Template Prompting:** Provides a template to the LLM. The LLM will then replace the placeholders in the template. Prompt templating also allows for prompts to be stored, reused, shared, and programmed. Also called **Template-Based Prompting**. \n",
    "8. **Style Prompting:** Asks for tone adjustment. Also called **Emotional Prompting**.\n",
    "9. **Negative Prompting**: Negative prompts are directions for information that should not be provided in responses. Not recommended in LLMs but commonly used in text-to-image models.\n",
    "10. **Reverse Prompting:** Role reversal. Asks the LLM which prompt should be used to produce an output similar to a provided output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dd91a-79f4-49da-8ef1-415166cc134a",
   "metadata": {},
   "source": [
    "Zero-shot prompting:\n",
    "\n",
    "11. **Emotion Prompting:** Incorporates phrases of psychological relevance to humans into the prompt.\n",
    "    - [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n",
    "12. **System 2 Attention (S2A) Prompting:** Asks the LLM to rewrite the prompt & removes any unrelated information, and then passes this new prompt to the LLM. \n",
    "    - [System 2 Attention](https://arxiv.org/abs/2311.11829)\n",
    "13. **simToM (Simulation Theory of Mind):** Establishes the set of facts one person knows, then answers the question based only on those facts.\n",
    "    - [Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities](https://arxiv.org/abs/2311.10227)\n",
    "14. **Rephrase & Respond (RaR) Prompting:** Instructs the LLM to rephrase and expand the question before generating the final answer.\n",
    "    - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n",
    "15. **Re-reading (RE2):** Adds the phrase \"Read the question again:\" to the prompt in addition to repeating the question.\n",
    "    - [Re-Reading Improves Reasoning in Large Language Models](https://arxiv.org/abs/2309.06275)\n",
    "16. **Self-Ask Prompting (SA):** Asks the LLM which extra information would improve the result and the generated content. Also called **Ask-Before-Answer Prompting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b416bd8-3a0c-49cf-81c5-c855aa595481",
   "metadata": {},
   "source": [
    "Thought generation:\n",
    "\n",
    "18. **Chain-of-Thought (CoT) Prompting:** Enables complex reasoning capabilities through intermediate reasoning steps.\n",
    "    - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "19. **Zero-Shot Chain-of-Thought (0-CoT) Prompting:** \"Let's think step by step\".\n",
    "    - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "20. **Step-Back Prompting:** Also called **Take-a-Step-Back Prompting**.\n",
    "    - [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117)\n",
    "21. **Analogical Prompting**: Recalls relevant problems and solutions, and then solves the initial problem.\n",
    "    - [Large Language Models as Analogical Reasoners](https://arxiv.org/abs/2310.01714)\n",
    "22. **Thread-of-Thought (ThoT) Prompting:**\n",
    "23. **Tabular Chain-of-Thought (Tab-CoT) Prompting:**\n",
    "    - [Tab-CoT: Zero-shot Tabular Chain of Thought](https://arxiv.org/abs/2305.17812)\n",
    "25. **Few-Shot Chain-of-Thought (CoT) Prompting:**\n",
    "26. **Contrastive Chain-of-Thought (CCoT):** \n",
    "    - [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277)\n",
    "27. **Uncertainty-Routed CoT Prompting:**\n",
    "28. **Complexity-Based Prompting:**\n",
    "    - [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)\n",
    "29. **Active Prompting:**\n",
    "    - [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246)\n",
    "30. **Momory-of-Thought Prompting:**\n",
    "31. **Automatic Chain-of-Thought (Auto-CoT):** Consists of 2 main stages belowed.\n",
    "    - **Question Clustering:** Partitions questions of a given dataset into a few clusters.\n",
    "    - **Demonstration Samplng:** Selects a representative question from each cluster and generates its reasoning chain using Zero-Shot CoT with simple heuristics.\n",
    "    - [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)\n",
    "    - [auto-cot](https://github.com/amazon-science/auto-cot)\n",
    "32. **Logical Chain-of-Thought (LogiCoT) Prompting:**\n",
    "    - [LogiCoT: Logical Chain-of-Thought Instruction-Tuning](https://arxiv.org/abs/2305.12147)\n",
    "33. **Chain-of-Symbol (CoS) Prompting:** Uses symbols such as `/` to assist the LLM with its difficulty of spatial reasoning in text.\n",
    "    - [Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models](https://arxiv.org/abs/2305.10276)\n",
    "34. **Graph-of-Thought (GoT) Prompting:**\n",
    "    - [Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)\n",
    "35. **Chain-of-Table Prompting:**\n",
    "    - [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://arxiv.org/abs/2401.04398)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2280bc-300a-4b1c-9bbb-04217d16193d",
   "metadata": {},
   "source": [
    "Decomposition:\n",
    "\n",
    "35. **Laddering Prompting:** Breaks a very complex problem into multiple sub-prompts and problems instead of sending one holistic prompt in one step. Alternative names of this approach include **Prompt Composition**, **Sequential Prompting**, **Multi-Turn Dialogue Prompts**, **Prewarming**, or **Internal Retrieval**.\n",
    "36. **Least-to-Most Prompting:** Prompts first to list the subproblems of a complex problem, and then solve them in sequence.\n",
    "    - [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n",
    "37. **Decomposed Prompting (DECOMP)**: For tasks where multiple predictions should be performed for one sample, such as sequence labeling, breaks down the holistic prompt into different sub-prompts and then answer each sub-prompt separately. Or called **Prompt Decomposition**. Prompt decomposition can be considered for token or span prediction tasks; prompt composition would be a better choice for span relation prediction tasks.\n",
    "38. **Plan-and-Solve Prompting:**\n",
    "39. **Tree-of-Thoughts (ToT) Prompting:**\n",
    "40. **Recursion-of-Thought:**\n",
    "41. **Program-of-Thoughts (PoT) Prompting:**\n",
    "42. **Faithful Chain-of-Thought:**\n",
    "43. **Generated Knowledge Prompting:**\n",
    "    - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "44. **Maieutic Prompting**\n",
    "    - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)\n",
    "45. **Directional Stimulus Prompting:**\n",
    "    - [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75adf13-7063-41d3-a1d8-9a7c3d2e951d",
   "metadata": {},
   "source": [
    "Ensembling:\n",
    "\n",
    "45. **Prompt Ensembling:** Uses multiple unanswered prompts for an input at inference time to make predictions.\n",
    "46. **Demonstration Ensembling (DENSE):**\n",
    "47. **Mixture of Reasoning Experts (MoRE):**\n",
    "48. **Max Mutual Information Method:**\n",
    "49. **Self-Consistency:**\n",
    "50. **Universal Self-Consistency:**\n",
    "51. **Meta-Reasoning over Multiple CoTs:**\n",
    "52. **DiVeRSe:**\n",
    "53. **Consistency-based Self-adaptive Prompting (COSP):**\n",
    "54. **Universal Self-Adaptive Prompting (USP)**\n",
    "55. **Prompt Paraphrasing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79eb5d0-c197-425e-be52-b748bb469446",
   "metadata": {},
   "source": [
    "Self-criticism:\n",
    "\n",
    "56. **Self-Calibration:**\n",
    "57. **Self-Refine:** Asks the LLM to be self-critical. To evaluate and improve the responses it generated. Also called **Self-Evaluative Prompting**.\n",
    "    - [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n",
    "58. **Reversing Chain-of-Thought (RCoT):**\n",
    "59. **Self-Verification:**\n",
    "60. **Chain-of-Verification (CoVe) Prompting:**\n",
    "61. **Cumulative Reasoning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73832a64-db0e-4204-8633-6284c6ab4d61",
   "metadata": {},
   "source": [
    "Prompt chaining:\n",
    "\n",
    "62. **Prompt Chaining:** A **Prompt Chain** consists of two or more prompt templates used in succession. The output of the prompt generated by the first prompt template is used to parameterize the second template, continuing until all templates are exhausted.\n",
    "63. **Stuff Documents Chain**\n",
    "64. **Reduce Documents Chain**\n",
    "65. **Map Reduce Documents Chain**\n",
    "66. **Refine Documents Chain**\n",
    "67. **Map Rerank Documents Chain**\n",
    "68. **Prompt Sharing:** Prompt templates are partially shared for multitask, multi-domain, or multilingual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3db44-595c-46f9-8d77-b484b43ba6d4",
   "metadata": {},
   "source": [
    "Prompt engineering:\n",
    "\n",
    "69. **Meta Prompting:** Prompts the LLM to generate or improve a prompt or prompt template.\n",
    "70. **AutoPrompt**\n",
    "71. **Automatic Prompt Engineer (APE):** Uses one LLM to beam search over prompts for another LLM.\n",
    "72. **Gradientfree Instructional Prompt Search (GrIPS)**\n",
    "73. **Prompt Optimization with Textual Gradients (Pro-TeGi)**\n",
    "74. **RLPrompt**\n",
    "75. **Dialogue-Comprised Policy-Gradient-Based Discrete Prompt Optimization (DP2O):**\n",
    "76. **Iterative Prompting**\n",
    "    - [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://arxiv.org/abs/2203.08383)\n",
    "77. **Expert Prompting:**\n",
    "    - **Static (Generic)**\n",
    "    - **Dynamic (Adaptive)**\n",
    "    - [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)\n",
    "78. **Multi-Persona Prompting (MP):** Also known as **Solo Performance Prompting (SPP)**.\n",
    "    - [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n",
    "79. **Meta Prompting:**\n",
    "    - [Meta-Prompting: Enhancing Language Models with Task Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)\n",
    "80. **Automatic Prompt Optimization (APO):**\n",
    "    - [Automatic Prompt Optimization with Gradient Descent and Beam Search](https://arxiv.org/abs/2305.03495)\n",
    "81. **Prompt Engineering a Prompt Engineer (PE2)**\n",
    "    - [Prompt Engineering a Prompt Engineer](https://arxiv.org/abs/2311.05661)\n",
    "82. **Optimization by PROmpting (OPRO):**\n",
    "    - [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)\n",
    "83. **Self-Discover:**\n",
    "    - [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b02546-93da-4b7b-b6a9-cef7f4b8d2b6",
   "metadata": {},
   "source": [
    "Few-shot prompting:\n",
    "\n",
    "83. **K-Nearest Neighbor (KNN):**\n",
    "84. **Vote-K:**\n",
    "    - [Selective Annotation Makes Language Models Better Few-Shot Learners](https://arxiv.org/abs/2209.01975)\n",
    "85. **Self-Generated In-Context Learning (SG-ICL):** Divided into the self-generation step & the inference step. First generates demonstrations conditioned on the test input and a specific class, so that generated demonstrations are highly correlated with the test input. Then uses the self-generated samples as a demonstration for in-context learning.\n",
    "    - [Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator](https://arxiv.org/abs/2206.08082)\n",
    "86. **Prompt Mining:**\n",
    "87. **?**\n",
    "88. **?**\n",
    "89. **?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df394389-908e-4665-af08-90b27c421d97",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation:\n",
    "\n",
    "90. **Retrieval Augmented Generation (RAG):**\n",
    "    - **Naive RAG:**\n",
    "    - **Advanced RAG:**\n",
    "    - **Modular RAG:**\n",
    "91. **Graph Retrieval-Augmented Generation (GraphRAG)**\n",
    "92. **Prompt Pipelining:** The variables or placeholders in the pre-defined prompt template are populated with the question from the user and the knowledge to be searched from the knowledge store.\n",
    "93. **Chain-of-Note (CoN) Prompting:**\n",
    "    - [Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210)\n",
    "94. **Chain-of-Knowledge (CoK) Prompting:**\n",
    "    - [Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](https://arxiv.org/abs/2305.13269)\n",
    "95. **Forward-Looking Active REtrieval augmented generation (FLARE):**\n",
    "    - [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983)\n",
    "96. **Multi-Query**\n",
    "97. **Multi-Vector**\n",
    "98. **Parent Document Retriever**\n",
    "99. **Top-K Similarity Search**\n",
    "100. **Maximum Marginal Relevance (MMR)**\n",
    "101. **Contextual Compression**\n",
    "102. **Ensemble Retriever**\n",
    "103. **Backtracing:**\n",
    "    - [Backtracing: Retrieving the Cause of the Query](https://arxiv.org/abs/2403.03956)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d2a56-9f1b-4914-984b-61762e0b91f7",
   "metadata": {},
   "source": [
    "Agents:\n",
    "\n",
    "104. **Autonomous Agents:** Prompt Chaining is the execution of a predetermined and set sequence of actions. However, agents can maintain a high level of autonomy.\n",
    "     - **Tool Use Agents**\n",
    "     - **Code-Based Agents**\n",
    "     - **Observation-Based Agents**\n",
    "105. **Automatic Reasoning and Tool-use (ART)**\n",
    "106. **Modular Reasoning, Knowledge, and Language (MRKL) System:**\n",
    "107. **Self-Correcting with Tool-Interactive Critiquing (CRITIC)**\n",
    "108. **Program-Aided Language Models (PAL)**\n",
    "     - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "109. **Tool-Integrated Reasoning Agent (ToRA):**\n",
    "110. **Task Weaver:**\n",
    "111. **ReAct Prompting:**\n",
    "112. **Reflexion**\n",
    "     - [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n",
    "113. **LLM Compiler:**\n",
    "     - [An LLM Compiler for Parallel Function Calling](https://arxiv.org/abs/2312.04511)\n",
    "114. **Reasoning WithOut Observation (ReWOO)**:\n",
    "     - [ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models](https://arxiv.org/abs/2305.18323)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e99881-2e0b-4f32-a96c-60615f4e76f9",
   "metadata": {},
   "source": [
    "Task-specific:\n",
    "\n",
    "115. **Chain-of-Density (CoD)**\n",
    "     - [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)\n",
    "116. **Recursive Reprompting and Revision (Re3)**\n",
    "     - [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://arxiv.org/abs/2210.06774)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dfc51-da3f-435c-9b69-8fd9fb7b3c30",
   "metadata": {},
   "source": [
    "Code Generation & Execution:\n",
    "\n",
    "116. **Scratchpad Prompting:**\n",
    "117. **Program of Thoughts (PoT) Prompting:**\n",
    "118. **Structured Chain-of-Thought (SCoT) Prompting:**\n",
    "119. **Chain-of-Code (CoC) Prompting:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1736a2-d61f-4ce2-b3cb-5390468bf071",
   "metadata": {},
   "source": [
    "Multilingual:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a392a2f-4d6d-4398-885c-b9f43d2d3cdd",
   "metadata": {},
   "source": [
    "Multimodal:\n",
    "\n",
    "120. **Multimodal Chain-of-Thought (CoT) Prompting**\n",
    "    - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2b0d3-17de-417b-9952-607decd891c8",
   "metadata": {},
   "source": [
    "121. **Soft Prompts**\n",
    "     - **Prefix Tunning:**\n",
    "     - **Prompt Tuning:** \n",
    "     - **P-Tunning:**\n",
    "122. **Prompt Injection Attack**\n",
    "     - **Prompt Takeovers**\n",
    "     - **Prompt Leaks**\n",
    "123. **Super Prompts:**\n",
    "     - **CAN (Code Anything Now)**\n",
    "     - **DAN (Do Anything Now)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e678a9-f187-4455-a475-bcd92501dfe1",
   "metadata": {},
   "source": [
    "69. **Retriever Fine-Tuning**\n",
    "70. **Collaborative Fine-Tuning**\n",
    "71. **Generator Fine-Tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc6bc-8807-4722-8573-8a18bdd917a9",
   "metadata": {},
   "source": [
    "# 2. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "- `langchain-core`: Base abstractions of components and **LangChain Expression Language (LCEL)**.\n",
    "- `langchain-community`: Third party integrations.\n",
    "    - `langchain-openai`\n",
    "    - `langchain-huggingface`\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langgraph`\n",
    "\n",
    "## 2-1. Chat Models & LLMs\n",
    "Chat models use a sequence of messages as inputs and return chat messages as outputs. Traditionally newer models. Inherited from `langchain_core.language_models.chat_models.BaseChatModel`.\n",
    "\n",
    "1. `langchain_openai.chat_models.base.ChatOpenAI`: OpenAI chat model integration.\n",
    "   - `model`\n",
    "   - `n`: Number of responses.\n",
    "   - `temperature`: Adds randomness to responses. Higher values make answers more diverse, while lower values make them more focused and deterministic. \n",
    "   - `timeout`: Request timeout.\n",
    "   - `stop`: Provides a list of stop words to prevent the model from generating responses containing those specific words.\n",
    "   - `max_tokens`: Max tokens to generate.\n",
    "   - `max_retries`: Max number of times to retry requests.\n",
    "   - `api_keys`\n",
    "   - `base_url`: Endpoint to Send Requests to.\n",
    "   - `model_kwargs`: Holds model parameters valid for `openai.OpenAI.chat.completions.create(messages, model, stream, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)` call not explicitly specified.\n",
    "       - `top_p`: **Nucleus Sampling** controls the diversity and quality of the responses. Limits the cumulative probability of the most likely tokens. Higher values allow more tokens, leading to diverse responses, while lower values provide more focused and constrained answers.\n",
    "       - `frequency_panelty`: Controls the model's tendency to generate repetitive words or phrases. Higher values encourage the model to explore more diverse and novel responses; lower values make the model more likely to repeat information.\n",
    "       - `presence_penalty`: Controls avoidance of certain topics. Higher values will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "       - [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "       - [openai-python/src/openai/resources/chat/completions.py](https://github.com/openai/openai-python/blob/main/src/openai/resources/chat/completions.py)  \n",
    "\n",
    "Pure text-in/text-out LLMs tends to be older or lower-level. Inherited from `langchain_core.language_models.llms.BaseLLM`.\n",
    "\n",
    "2. `langchain_openai.llms.base.OpenAI`: OpenAI large language models. Inherited from `langchain_openai.llms.base.BaseOpenAI`.\n",
    "\n",
    "Both `BaseChatModel` and `BaseLLM` classes are inherited from `langchain_core.language_models.base.BaseLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598cecb0-7935-4a89-8ed1-b25e3f54617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920c589f-ce68-4937-a99d-1da20b05f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9072be63-0000-4fc5-a0d7-b2a6288a5b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x7374b40dbe30>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7374abf2e5d0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14082095-55d8-4cb2-bb18-abafa7274023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b58327a5-f9a8-4e48-93f4-b8a3ef1e6643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAs of 2021, the estimated population of Barcelona is 1.6 million.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.predict()`\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, max_tokens=256, timeout=None, max_retries=2)\n",
    "\n",
    "prompt = \"How many citizens does Barcelona have?\"\n",
    "response = llm.predict(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6f64d3a-386d-4e98-aa31-2de6e52be7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGeorge Washington was the first president of the United States.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI()`\n",
    "prompt = \"Who was the first president of the United States?\"\n",
    "response = llm(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbe64930-d8fe-4365-b82d-0ec193a19b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMichael Jordan is a retired American professional basketball player who is widely considered one of the greatest basketball players of all time. He played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards, winning six NBA championships and earning numerous individual awards, including five MVP awards. He is known for his incredible athleticism, scoring ability, and competitive drive, and is often credited with popularizing the game of basketball around the world. After retiring from basketball, Jordan became a successful businessman and owner of the Charlotte Hornets NBA team.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `OpenAI.invoke()`\n",
    "prompt = \"Who is Michael Jordan?\"\n",
    "response = llm.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18696fa-14be-49a2-9b3d-aa9659343ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed87feea0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed87fe270>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI()`\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9429f382-fb9e-425e-97ea-e9494d05de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8600e00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8601b20>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2, model_kwargs={\"top_p\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e079e8b-afbc-490b-9abf-39c2388571f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 24, 'total_tokens': 87}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-62c75271-a1e7-49cc-b4c7-79908fbf8e9c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 63, 'total_tokens': 87})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatOpenAI.invoke()`\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"How many citizens does Barcelona have?\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7c0dbc6-0151-4bcb-a082-3bef64f519af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of the most recent data available in 2023, Barcelona has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most up-to-date information, it is advisable to refer to official sources such as the Statistical Institute of Catalonia or the Barcelona City Council.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d45a0338-f12e-4aaa-ba21-8d6cbe0b8df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9kguKKA9vSKQf2Frv59X5clPddG7T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\", role='assistant', function_call=None, tool_calls=None))], created=1720915052, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_d33f7b429e', usage=CompletionUsage(completion_tokens=79, prompt_tokens=14, total_tokens=93))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `openai.chat.completions.create()`\n",
    "import openai\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How many citizens does Barcelona have?\"\n",
    "}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, temperature=0)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6b7bacc-d874-4410-ba0b-e6a5b3094262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of the most recent data available in 2023, Barcelona, the capital city of Catalonia in Spain, has a population of approximately 1.6 million residents. However, population figures can fluctuate, so for the most current and precise numbers, it's advisable to consult official sources such as the Instituto Nacional de Estadística (INE) or the Ajuntament de Barcelona (Barcelona City Council).\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d19af-ccb6-435f-860c-461b5fec96f7",
   "metadata": {},
   "source": [
    "## 2-2. Prompt Templates\n",
    "Prompt templates:\n",
    "\n",
    "1. `langchain_core.prompts.prompt.PromptTemplate(input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags, template, template_format, validate_template)`: `str` prompt templates.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `from_template(template, template_format, partial_variables, **kwargs)`\n",
    "   - `from_file(template_file, input_variables, **kwargs)`\n",
    "   - `from_examples(examples, prefix, suffix, input_variables, example_separator, **kwargs)`: Few-shot prompt templates. Takes examples in list format with prefix and suffix to create a prompt.\n",
    "\n",
    "2. `langchain_core.prompts.chat.ChatPromptTemplate()`: Formats a list of messages. Alternatively, you can also construct the prompt using message role prompt templates described belowed. \n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Returns a list of finalized messages.\n",
    "   - `from_messages(message, template_format)`: Create a chat prompt template from a variety of message formats.\n",
    "\n",
    "Message role prompt templates:\n",
    "\n",
    "3. `langchain_core.prompts.chat.SystemMessagePromptTemplate(additional_kwargs, prompt)`: for optional `system` message role used to set the behavior of the assistant.\n",
    "4. `langchain_core.prompts.chat.HumanMessagePromptTemplate(additional_kwargs, prompt)`: for `user` message role who provides requests.\n",
    "5. `langchain_core.prompts.chat.AIMessagePromptTemplate(additional_kwargs, prompt)`: for `assistant` message role storing previous assistant responses or giving examples of few-shot prompting.\n",
    "\n",
    "- All of the three message role prompt templates aboved have the following methods:\n",
    "    - `format(**kwargs)`\n",
    "    - `format_messages(**kwargs)`\n",
    "    - `from_template(template, template_format, partial_variables, **kwargs)`: Default format of the template is `f-string`.\n",
    "    - `from_template_file(template_file, input_variables, **kwargs)`\n",
    "\n",
    "Few-shot prompt templates:\n",
    "\n",
    "6. `langchain_core.prompts.few_shot.FewShotPromptTemplate(example_prompt, example_selector, example_separator, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, prefix, suffix, tags, template_format, validate_template)`: Prompt templates that contains few shot examples.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "\n",
    "7. `langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate(example_prompt, example_selector, examples, input_types, input_variables, metadata, optional_variables, output_parser, partial_variables, tags)`: Chat prompt templates that supports few-shot examples. Because there is no `suffix` to assign like `FewShotPromptTemplate`, you need one more final prompt to wrap the template.\n",
    "   - `format(**kwargs)`\n",
    "   - `invoke(input, config)`\n",
    "   - `format_messages(**kwargs)`: Formats `**kwargs` into a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb55ef54-f14f-4386-af08-b931cebf36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I want to open a restaurant for Italian food. Suggest a fency name for this.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PromptTemplate()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "messages = prompt_template.invoke({\"cuisine\": \"Italian\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40bdba48-aa25-4a74-9d8b-413fa551abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Italian food. Suggest a fency name for this.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(cuisine=\"Italian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ebd4837-3d99-457d-b267-fb18b25fce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How about \"La Dolce Vita Ristorante\"? This name evokes the charm and elegance of Italian culture, suggesting a delightful and luxurious dining experience.', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 25, 'total_tokens': 53}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-e17ba7b7-3502-4490-a811-6ab506eceabc-0', usage_metadata={'input_tokens': 25, 'output_tokens': 28, 'total_tokens': 53})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74ca26e1-91bd-4412-8f9c-033b6582caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_template()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7405f89b-7828-4b85-a774-bd376f7aa749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cats.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.format(topic=\"cats\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad297-3124-4826-92b5-1a982c3b4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `from_file()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf46caee-8a03-4b3d-b949-944de31ff683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats.')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_examples(\"Tell me a joke about {topic}.\")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fff62a5-308b-4d93-90b6-6394245fd647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures constructed from basic materials such as wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are compartmentalized into various sections for different activities. These structures lack the advanced climate control and adaptive architecture found in Martian habitats. Instead, they rely on rudimentary heating and cooling systems and are often cluttered with an array of unnecessary objects and decorations.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few-shot prompting without template\n",
    "messages = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\n",
    "Question: What is human cuisine like?\n",
    "Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "\n",
    "Question: What is human entertainment?\n",
    "Response: Crude moving images and loud sounds.\n",
    "\n",
    "Question: What are human homes like?\n",
    "Response: \"\"\"\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6141f4a-795d-40e4-a536-220e36a4662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['userInput'], template=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: {userInput}\\nResponse: \\n\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_examples()`\n",
    "examples = [\n",
    "    \"\"\"\n",
    "    Question: What is human cuisine like?\n",
    "    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Question: What is human entertainment?\n",
    "    Response: Crude moving images and loud sounds.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "prefix = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \n",
    "\"\"\"\n",
    "\n",
    "example_template = PromptTemplate.from_examples(examples=examples, prefix=prefix, suffix=suffix, input_variables=['userInput'], example_separator=\"\\n\\n\")\n",
    "example_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52a27edf-9a2d-4134-a577-e251db5d78de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\n    Question: What is human cuisine like?\\n    Response: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n    \\n\\n\\n    Question: What is human entertainment?\\n    Response: Crude moving images and loud sounds.\\n    \\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = example_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c87ac348-d28c-416e-928b-0756a33d626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. They are designed to provide shelter from environmental elements and are often segmented into various compartments for different activities such as sleeping, eating, and socializing. These structures lack the advanced adaptive and self-sustaining features of our Martian habitats.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7244116b-8e9b-4a06-a982-9cf6e2bc0811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `ChatPromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eecea368-a6b2-4b65-9f80-b0b2a092c75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9e1ac11-490a-4ab4-a083-efa9be8c0515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "prompt_template = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\")\n",
    "    + HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    ")\n",
    "messages = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce459097-dd4b-4267-a66a-23202e736b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "940e0bb4-b9cd-4451-9bcd-bd055ddfc8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotPromptTemplate()`\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is human cuisine like?\",\n",
    "        \"answer\": \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is human entertainment?\",\n",
    "        \"answer\": \"Crude moving images and loud sounds.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Question: {query}\n",
    "Response: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cc0c496-aee5-4482-bd12-900fc49014f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: ', prefix='You are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31eb9c81-c6cb-4477-a74d-af8f22e8ac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d2be239-7582-40e9-89a9-cc76d552063b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Their homes are primitive enclosures made from basic materials like wood, stone, and synthetic compounds. These structures are designed to provide shelter from environmental elements and are often divided into multiple rooms for different activities. The layout and design lack the advanced spatial efficiency and adaptive technology found in Martian habitats.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2874b0b3-03da-4d14-a119-a1adea11012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `FewShotChatMessagePromptTemplate()`\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"1+1\",\n",
    "        \"output\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"3+5\",\n",
    "        \"output\": \"8\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"{output}\")\n",
    "])\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "113100d5-dfe1-44c3-8929-820a9c743d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(examples=examples, example_prompt=example_prompt, input_variables=[])\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6855621-ec2c-4dae-b48c-38b4bacb455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt = few_shot_prompt_template.invoke({})\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5a5d74b-18b4-4a92-8e13-f07eb6e31bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='1+1'), AIMessage(content='2'), SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        ChatPromptTemplate.from_messages(few_shot_prompt.to_messages()),\n",
    "        (\"user\", \"{userInput}\"),\n",
    "    ]\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages\n",
    "# chain = final_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
    "# chain.invoke({\"input\":\"What's the square of a triangle?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00a82d26-b889-4246-a1cd-5149c28d245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"square of a triangle\" is not a standard mathematical concept. However, if you are referring to the area of a triangle, you can calculate it using the formula:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, please provide more context so I can assist you better.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5b1d44d-ab07-439b-84cb-439cedfed6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '1+1', 'output': '2'}, {'input': '3+5', 'output': '8'}], input_variables=[], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few shot prompting with `HumanMessagePromptTemplate()`, `SystemMessagePromptTemplate()` & `AIMessagePromptTemplate()`\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=(\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\") \n",
    "        + AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "    ),\n",
    "    input_variables=[]\n",
    ")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c04cba-3f57-4d99-b595-271d70880e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant'), HumanMessage(content='1+1'), AIMessage(content='2'), HumanMessage(content='3+5'), AIMessage(content='8'), HumanMessage(content=\"What's the square of a triangle?\")])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = (\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI Assistant\") \n",
    "    + few_shot_prompt_template\n",
    "    + HumanMessagePromptTemplate.from_template(\"{userInput}\")\n",
    ")\n",
    "messages = final_prompt.invoke({\"userInput\": \"What's the square of a triangle?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f333b5d7-2816-4830-a028-6883853d31eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like there might be a bit of confusion in your question. The term \"square\" typically refers to a specific geometric shape or the result of multiplying a number by itself. However, a triangle is a different geometric shape and doesn\\'t have a \"square\" in the same sense.\\n\\nIf you are asking about the area of a triangle, the formula to calculate it is:\\n\\n\\\\[ \\\\text{Area} = \\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height} \\\\]\\n\\nIf you meant something else, could you please clarify?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb132aac-9db1-45c1-8f43-31225683701f",
   "metadata": {},
   "source": [
    "## 2-3. Example Selectors\n",
    "\n",
    "Vector stores and embedding models are required for **Similarity Search** & **Maximum Marginal Relevance (MMR)**. More details can be found in their respective sections.\n",
    "\n",
    "Length:\n",
    "\n",
    "1. `langchain_core.example_selectors.length_based.LengthBasedExampleSelector(example_prompt, example_text_lengths, examples, get_text_length, max_length=2048)`: Selects examples based on length.\n",
    "\n",
    "Similarity:\n",
    "\n",
    "2. `langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector(example_keys, input_keys, k, vectorstore, vectorstore_kwargs)`: Selects examples based on semantic similarity. More details about ChromaDB vector store & OpenAI embedding models can be found in their respective sections.\n",
    "\n",
    "Maximum marginal relevance (MMR):\n",
    "\n",
    "3. `langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector(example_keys, fetch_k, input_keys, k, vectorstore, vectorstore_kwargs)`: Selects examples based on Max Marginal Relevance.\n",
    "   - [Complementary Explanations for Effective In-Context Learning](https://arxiv.org/abs/2211.13892)\n",
    "\n",
    "n-gram:\n",
    "\n",
    "4. `langchain_community.example_selectors.ngram_overlap.NGramOverlapExampleSelector(example_prompt, examples, threshold=-1.0)`: Selects and orders examples based on n-gram overlap score (`sentence_bleu` score from NLTK package).\n",
    "   - Currently there is an issue with `bleu_score.py` but not yet built on PyPI. You need to update `nltk/translate/bleu_score.py` manually as on [GitHub](https://github.com/nltk/nltk/commit/28eeb3e83c98d27ad6b67f233576f09883f394fe#diff-644ebc352f93bb9a63ff8edbc7292e137cf1adaf68d5dc4f1aef13f4de96bf20).\n",
    "  \n",
    "- All example selectors have the following methods.\n",
    "    - `add_example(example)`\n",
    "    - `select_examples(input_variables)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45854422-d580-426a-9e3c-08a1ae0afee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `LengthBasedExampleSelector()`\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is human cuisine like?\",\n",
    "        \"answer\": \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human entertainment?\",\n",
    "        \"answer\": \"Crude moving images and loud sounds.\"\n",
    "    }, {\n",
    "        \"query\": \"What do humans use for transportation?\",\n",
    "        \"answer\": \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans communicate with each other?\",\n",
    "        \"answer\": \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans maintain health?\",\n",
    "        \"answer\": \"Consuming organic compounds and performing physical movements.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human education?\",\n",
    "        \"answer\": \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"\n",
    "    }, {\n",
    "        \"query\": \"How do humans manage their societies?\",\n",
    "        \"answer\": \"Through chaotic and inefficient systems.\"\n",
    "    }, {\n",
    "        \"query\": \"What is human art?\",\n",
    "        \"answer\": \"Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Question: {query}\n",
    "Response: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c95948-b183-4cc3-a31e-43970358cc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29]), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"\n",
    "You are an alien from Mars: \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Question: {userInput}\n",
    "Response: \n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b157fa-596b-47f7-9692-a177c93b557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9791a25a-39d4-43d8-a5b4-33ae1f2f8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are rudimentary shelters constructed from basic materials such as wood, stone, and synthetic compounds. They are designed to provide minimal protection from environmental elements and are often cluttered with an array of unnecessary objects. These structures lack the advanced climate control and adaptive architecture found in our Martian dwellings.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e517155-6037-44af-9979-5f3f5cee8e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=LengthBasedExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}, {'query': 'What are the birds?', 'answer': 'Mere distractions, I imagine. Noisemakers to fill the void of their silent minds'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), get_text_length=<function _get_length_based at 0x72fa1003bec0>, max_length=100, example_text_lengths=[34, 14, 34, 36, 16, 34, 15, 29, 21]), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `add_example()`\n",
    "new_example = {\"query\": \"What are the birds?\", \"answer\": \"Mere distractions, I imagine. Noisemakers to fill the void of their silent minds\"}\n",
    "few_shot_prompt_template.example_selector.add_example(new_example)\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f95a70d6-4d81-43ba-9ef7-a54cf30ef742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Crude moving images and loud sounds.',\n",
       "  'query': 'What is human entertainment?'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `SemanticSimilarityExampleSelector()`\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples=examples, embeddings=OpenAIEmbeddings(), vectorstore_cls=Chroma, k=1)\n",
    "query = \"What is human brain made of?\"\n",
    "selected_examples = example_selector.select_examples({\"query\": query})\n",
    "selected_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e287f9ff-5710-4a39-ac79-e9d6d6d9790d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7374a87785c0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bee3b848-7b35-4fb1-88e0-1da6bac13f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What is human entertainment?\\nResponse: Crude moving images and loud sounds.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae3c2be4-edf8-47b1-88e8-abab6c7ef2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human homes are enclosed structures made from various materials such as wood, brick, and metal. They are designed to provide shelter and comfort, often containing multiple rooms with specific functions like sleeping, cooking, and socializing. These structures are equipped with systems for temperature control, water supply, and waste disposal. Humans decorate their homes with objects and colors that reflect their personal tastes and cultural influences.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc4c8d5f-eec3-4c13-8510-6cb81d961d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxMarginalRelevanceExampleSelector(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cd33fa8890>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None, fetch_k=20)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `MaxMarginalRelevanceExampleSelector()`\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(examples, OpenAIEmbeddings(), FAISS, k=2)\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8118d18-ea9f-4734-950c-abe1ceed416c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=MaxMarginalRelevanceExampleSelector(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cd33fa8890>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None, fetch_k=20), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d73d6bcf-8848-49d3-9088-1615e3157ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What is human cuisine like?\\nResponse: Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabda531-9c57-4f9e-84a7-3b3872b56204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramOverlapExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), threshold=0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `NGramOverlapExampleSelector()`\n",
    "from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(examples=examples, example_prompt=example_prompt, threshold=0.0)\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1ba6fa-0b93-43ac-a2e4-75504907e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['userInput'], example_selector=NGramOverlapExampleSelector(examples=[{'query': 'What is human cuisine like?', 'answer': \"Their cuisine is a simplistic combination of various organic matter, often heated in rudimentary ways. It's unrefined and unstructured, especially compared to our molecular gastronomy.\"}, {'query': 'What is human entertainment?', 'answer': 'Crude moving images and loud sounds.'}, {'query': 'What do humans use for transportation?', 'answer': \"Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\"}, {'query': 'How do humans communicate with each other?', 'answer': \"They use a very basic form of communication involving the modulation of sound waves, referred to as 'speech.' Astonishingly primitive compared to our telepathic links.\"}, {'query': 'How do humans maintain health?', 'answer': 'Consuming organic compounds and performing physical movements.'}, {'query': 'What is human education?', 'answer': \"They engage in a very basic form of knowledge transfer in places called 'schools.' It's a slow and inefficient process compared to our instant knowledge assimilation.\"}, {'query': 'How do humans manage their societies?', 'answer': 'Through chaotic and inefficient systems.'}, {'query': 'What is human art?', 'answer': 'Their art is a primitive expression through physical mediums like paint and stone, lacking the sophistication of our holographic emotion sculptures.'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), threshold=0.0), example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nQuestion: {query}\\nResponse: {answer}\\n'), suffix='\\nQuestion: {userInput}\\nResponse: \\n', prefix='\\nYou are an alien from Mars: \\nHere are some examples: \\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(example_selector=example_selector, example_prompt=example_prompt, prefix=prefix, suffix=suffix, input_variables=[\"userInput\"], example_separator=\"\\n\\n\")\n",
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9736660-1a1f-43e3-85b9-177871b939a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nYou are an alien from Mars: \\nHere are some examples: \\n\\n\\n\\nQuestion: What do humans use for transportation?\\nResponse: Humans rely on archaic and inefficient rolling contraptions they proudly call 'cars.' These are remarkably primitive compared to our teleportation beams and anti-gravity vessels.\\n\\n\\n\\nQuestion: What are human homes like?\\nResponse: \\n\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = few_shot_prompt_template.invoke({\"userInput\": \"What are human homes like?\"})\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2f33-c03e-48c4-adfd-9acaf7c8a578",
   "metadata": {},
   "source": [
    "## 2-4. Output Parsers\n",
    "\n",
    "Pydantic:\n",
    "\n",
    "1. `langchain_core.output_parsers.pydantic.PydanticOutputParser(diff, pydantic_object)`: Parses and validates an output and returns a Pydantic model instance.\n",
    "\n",
    "JSON:\n",
    "\n",
    "2. `langchain_core.output_parsers.json.JsonOutputParser(diff, pydantic_object)`: Parses the output of an LLM call to a JSON object.\n",
    "\n",
    "XML:\n",
    "\n",
    "3. `langchain_core.output_parsers.xml.XMLOutputParser(encoding_matcher, parser)`: Parses an output using XML format. Outputs `dict`.\n",
    "   - Can be converted to XML using `dict2xml`, but further XML processing with `lxml` could be required.\n",
    "   - Can also be converted to pandas `DataFrame`, further manipulation may be needed.\n",
    "\n",
    "CSV:\n",
    "\n",
    "4. `langchain_core.output_parsers.list.CommaSeparatedListOutputParser`: Parses the output of an LLM call to a comma-separated list.\n",
    "   - Can write the output comma-separated list into a CSV file using `csv`.\n",
    "   \n",
    "Enum:\n",
    "\n",
    "5. `langchain.output_parsers.enum.EnumOutputParser(enum)`: Parses an output that is one of a set of values.\n",
    "    - Clarify the prompt. Make sure the prompt explicitly instructs the LLM to respond with only one of the enum values or you will get an error.\n",
    "    \n",
    "Structured:\n",
    "\n",
    "6. `langchain.output_parsers.structured.StructuredOutputParser(response_schemas)`: Returns structured information.\n",
    "   - Less powerful than `PydanticOutputParser` or `JsonOutputParser` since it only allows for fields to be `str`. Only useful for smaller LLMs.\n",
    "\n",
    "- All of the output parsers aboved have the following methods:\n",
    "    - `get_format_instructions()`\n",
    "    - `invoke(input, config)`: Transforms a single input into an output.\n",
    "    - `parse(text)`\n",
    "    - `parse_from_prompt(completion, prompt)`: Parses the output of an LLM call with the input prompt for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25b2b4fc-9323-4f33-80be-bb99eea5ee79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.Joke'>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PydanticOutputParser()`\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Define your desired data structure\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic\n",
    "    @field_validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "819c8746-b680-4d99-ae7f-bd4324d97965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7113e9f9-3982-4751-9dc6-f026d28e99e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"setup\": \"Why don\\'t scientists trust atoms?\",\\n  \"punchline\": \"Because they make up everything!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 206, 'total_tokens': 236}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-fcc29177-7251-45ae-9bfd-c972484b4977-0', usage_metadata={'input_tokens': 206, 'output_tokens': 30, 'total_tokens': 236})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt_template | llm\n",
    "output = chain.invoke({\"query\": \"Tell me a joke.\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3cad9e4-fa2d-465a-8c99-2c6085033741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cad0db0-558b-43eb-95b4-5da7240824a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bea7a31-6db1-4ee9-b96a-c9b32b080367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_with_prompt(completion=output.content, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b30a402-081e-4dab-a179-ed8663f54b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "553e59c4-a9f7-485c-865c-bb8f1891dd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.Joke'>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `JsonOutputParser()`\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5b9b7b9-93b5-460c-9132-5055adda41e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69ed8f12-38ab-4e7e-a134-46a0205784b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15bf3f90-473a-4f41-a546-da13554e788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XMLOutputParser()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `XMLOutputParser()`\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "parser = XMLOutputParser()\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8aaf2876-4cdf-464d-a5af-ef21b0fa35a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a XML file.\\n1. Output should conform to the tags below. \\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema. \\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "195e0ea2-607f-4396-b3c2-0337f0c93775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filmography': [{'actor': [{'name': 'Tom Hanks'},\n",
       "    {'movies': [{'movie': [{'title': 'Big'},\n",
       "        {'year': '1988'},\n",
       "        {'role': 'Josh Baskin'}]},\n",
       "      {'movie': [{'title': 'Forrest Gump'},\n",
       "        {'year': '1994'},\n",
       "        {'role': 'Forrest Gump'}]},\n",
       "      {'movie': [{'title': 'Sleepless in Seattle'},\n",
       "        {'year': '1993'},\n",
       "        {'role': 'Sam Baldwin'}]},\n",
       "      {'movie': [{'title': 'Saving Private Ryan'},\n",
       "        {'year': '1998'},\n",
       "        {'role': 'Captain John H. Miller'}]},\n",
       "      {'movie': [{'title': 'Cast Away'},\n",
       "        {'year': '2000'},\n",
       "        {'role': 'Chuck Noland'}]},\n",
       "      {'movie': [{'title': 'The Terminal'},\n",
       "        {'year': '2004'},\n",
       "        {'role': 'Viktor Navorski'}]},\n",
       "      {'movie': [{'title': 'Catch Me If You Can'},\n",
       "        {'year': '2002'},\n",
       "        {'role': 'Carl Hanratty'}]},\n",
       "      {'movie': [{'title': 'Bridge of Spies'},\n",
       "        {'year': '2015'},\n",
       "        {'role': 'James B. Donovan'}]},\n",
       "      {'movie': [{'title': 'Sully'},\n",
       "        {'year': '2016'},\n",
       "        {'role': \"Chesley 'Sully' Sullenberger\"}]},\n",
       "      {'movie': [{'title': 'A Beautiful Day in the Neighborhood'},\n",
       "        {'year': '2019'},\n",
       "        {'role': 'Fred Rogers'}]}]}]}]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"{query}\\n{format_instructions}\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "parser_output = chain.invoke({\"query\": \"Generate the shortened filmography for Tom Hanks.\"})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7cb510f2-09e3-48b4-9c47-2116d35de619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to an XML file\n",
    "from dict2xml import dict2xml\n",
    "\n",
    "xml = dict2xml(parser_output)\n",
    "with open('parser_output.xml', 'w') as f:\n",
    "    for line in xml:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "88314afc-cbd0-4185-b6f3-3c9b69e0fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<filmography>\\n  <actor>\\n    <name>Tom Hanks</name>\\n  </actor>\\n  <actor>\\n    <movies>\\n      <movie><title>Big</title><year>1988</year><role>Josh Baskin</role></movie></movies>\\n    <movies>\\n      <movie><title>Forrest Gump</title><year>1994</year><role>Forrest Gump</role></movie></movies>\\n    <movies>\\n      <movie><title>Sleepless in Seattle</title><year>1993</year><role>Sam Baldwin</role></movie></movies>\\n    <movies>\\n      <movie><title>Saving Private Ryan</title><year>1998</year><role>Captain John H. Miller</role></movie></movies>\\n    <movies>\\n      <movie><title>Cast Away</title><year>2000</year><role>Chuck Noland</role></movie></movies>\\n    <movies>\\n      <movie><title>The Terminal</title><year>2004</year><role>Viktor Navorski</role></movie></movies>\\n    <movies>\\n      <movie><title>Catch Me If You Can</title><year>2002</year><role>Carl Hanratty</role></movie></movies>\\n    <movies>\\n      <movie><title>Bridge of Spies</title><year>2015</year><role>James B. Donovan</role></movie></movies>\\n    <movies>\\n      <movie><title>Sully</title><year>2016</year><role>Chesley 'Sully' Sullenberger</role></movie></movies>\\n    <movies>\\n      <movie><title>A Beautiful Day in the Neighborhood</title><year>2019</year><role>Fred Rogers</role></movie></movies>\\n  </actor>\\n</filmography>\""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "tree = etree.fromstring(xml)\n",
    "for movies in tree.findall('.//movies'):\n",
    "    title = movies.find('.//title').text\n",
    "    year = movies.find('.//year').text\n",
    "    role = movies.find('.//role').text\n",
    "    \n",
    "    movie_lst = movies.findall('.//movie')\n",
    "    for movie in movie_lst:\n",
    "        movies.remove(movie)\n",
    "    movies.append(etree.Element('movie'))\n",
    "    movie = movies.find('.//movie')\n",
    "    \n",
    "    title_el = etree.Element('title')\n",
    "    title_el.text = title\n",
    "    year_el = etree.Element('year')\n",
    "    year_el.text = year\n",
    "    role_el = etree.Element('role')\n",
    "    role_el.text = role\n",
    "    \n",
    "    movie.append(title_el)\n",
    "    movie.append(year_el)\n",
    "    movie.append(role_el)\n",
    "xml_parsed = etree.tostring(tree).decode()\n",
    "xml_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d99076a6-4a8a-48f0-a9d9-7abd16fc0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parser_output_parsed.xml', 'w') as f:\n",
    "    for line in xml_parsed:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d939371d-9e70-45c8-b018-b90250308e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'title': 'Big'}, {'year': '1988'}, {'role': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'title': 'Forrest Gump'}, {'year': '1994'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'title': 'Sleepless in Seattle'}, {'year': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'title': 'Saving Private Ryan'}, {'year': '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'title': 'Cast Away'}, {'year': '2000'}, {'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'title': 'The Terminal'}, {'year': '2004'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'title': 'Catch Me If You Can'}, {'year': '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'title': 'Bridge of Spies'}, {'year': '2015'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'title': 'Sully'}, {'year': '2016'}, {'role'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'title': 'A Beautiful Day in the Neighborhoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie\n",
       "0  [{'title': 'Big'}, {'year': '1988'}, {'role': ...\n",
       "1  [{'title': 'Forrest Gump'}, {'year': '1994'}, ...\n",
       "2  [{'title': 'Sleepless in Seattle'}, {'year': '...\n",
       "3  [{'title': 'Saving Private Ryan'}, {'year': '1...\n",
       "4  [{'title': 'Cast Away'}, {'year': '2000'}, {'r...\n",
       "5  [{'title': 'The Terminal'}, {'year': '2004'}, ...\n",
       "6  [{'title': 'Catch Me If You Can'}, {'year': '2...\n",
       "7  [{'title': 'Bridge of Spies'}, {'year': '2015'...\n",
       "8  [{'title': 'Sully'}, {'year': '2016'}, {'role'...\n",
       "9  [{'title': 'A Beautiful Day in the Neighborhoo..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas `DataFrame`\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(parser_output[\"filmography\"][0][\"actor\"][1][\"movies\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "42e85a6e-0bef-407b-9846-f257782d1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big</td>\n",
       "      <td>1988</td>\n",
       "      <td>Josh Baskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994</td>\n",
       "      <td>Forrest Gump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sleepless in Seattle</td>\n",
       "      <td>1993</td>\n",
       "      <td>Sam Baldwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>1998</td>\n",
       "      <td>Captain John H. Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cast Away</td>\n",
       "      <td>2000</td>\n",
       "      <td>Chuck Noland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Terminal</td>\n",
       "      <td>2004</td>\n",
       "      <td>Viktor Navorski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Catch Me If You Can</td>\n",
       "      <td>2002</td>\n",
       "      <td>Carl Hanratty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bridge of Spies</td>\n",
       "      <td>2015</td>\n",
       "      <td>James B. Donovan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sully</td>\n",
       "      <td>2016</td>\n",
       "      <td>Chesley 'Sully' Sullenberger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Beautiful Day in the Neighborhood</td>\n",
       "      <td>2019</td>\n",
       "      <td>Fred Rogers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  year                          role\n",
       "0                                  Big  1988                   Josh Baskin\n",
       "1                         Forrest Gump  1994                  Forrest Gump\n",
       "2                 Sleepless in Seattle  1993                   Sam Baldwin\n",
       "3                  Saving Private Ryan  1998        Captain John H. Miller\n",
       "4                            Cast Away  2000                  Chuck Noland\n",
       "5                         The Terminal  2004               Viktor Navorski\n",
       "6                  Catch Me If You Can  2002                 Carl Hanratty\n",
       "7                      Bridge of Spies  2015              James B. Donovan\n",
       "8                                Sully  2016  Chesley 'Sully' Sullenberger\n",
       "9  A Beautiful Day in the Neighborhood  2019                   Fred Rogers"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "df_parsed = pd.DataFrame(\n",
    "    [ \n",
    "        (title['title'], year['year'], role['role']) for title, year, role in df.movie.apply(json.dumps).apply(json.loads)\n",
    "    ],\n",
    "    columns=['title', 'year', 'role']\n",
    ")\n",
    "df_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4ad025-ccd9-49e7-9d8b-9e49bc02f823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommaSeparatedListOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `CommaSeparatedListOutputParser()`\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aba25f1-cc3b-4a69-8953-e0f43370f80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93fdb94-4187-4929-9cf7-6b261e4e4434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pop', 'Rock', 'Hip-Hop']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"List 3 main-stream {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "parser_output = chain.invoke({\"subject\": \"music styles\"})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2892d080-bd37-46f6-94f4-807d175c644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a CSV file\n",
    "import csv\n",
    "\n",
    "with open('parser_output.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(parser_output)\n",
    "    # for list of list\n",
    "    # writer.writerows(parser_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86ce31fa-70e2-4b1c-94cd-3c30a3dbdade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnumOutputParser(enum=<enum 'Genders'>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `EnumOutputParser()`\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Genders(Enum):\n",
    "    MALE = \"male\"\n",
    "    FEMALE = \"female\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Genders)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c34b7a57-6c32-4dac-af3f-57fc600cdb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select one of the following options: male, female'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2acafef0-bbe2-40ac-aafe-370fdea36235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Genders.MALE: 'male'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the prompt explicitly instructs the LLM to respond with only one of the enum values\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me the gender of the celebrity {name}. Respond only with 'male' or 'female'.\\n{format_instructions}\",\n",
    "    input_variables=[\"name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"name\": \"Michael Jordan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d6858c7-ca23-47a1-b58e-38d5b504346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.', type='string'),\n",
       " ResponseSchema(name='delivery_days', description='How many days did it take for the product to arrive? If this information is not found, output -1.', type='string'),\n",
       " ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and output them as a comma separated Python list.', type='string')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `StructuredOutputParser()`\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
    "response_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d4095d7-26bc-4ace-9ba4-fea1c4df18a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days did it take for the product to arrive? If this information is not found, output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and output them as a comma separated Python list.', type='string')])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1544cf5c-0587-4005-a59e-6a1cf55a9277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32d22266-6990-45cc-8ea4-286c55eea607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': '2',\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_review = \"\"\"\n",
    "This leaf blower is pretty amazing. It has four settings: candle blower, gentle breeze, windy city, and tornado.\n",
    "It arrived in two days, just in time for my wife's anniversary present.\n",
    "I think my wife liked it so much she was speechless.\n",
    "So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn.\n",
    "It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else?\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=review_template, input_variables=[\"text\"], partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"text\": customer_review})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f0302-2635-417a-9ccc-d72de3403102",
   "metadata": {},
   "source": [
    "## 2-5. Chains\n",
    "\n",
    "1. **LCEL**\n",
    "2. `RunnableWithMessageHistory()`\n",
    "   - Offers several benefits over deprecated `ConversationChain`:\n",
    "       - Stream, batch & async support.\n",
    "       - More flexible memory handling, including the ability to manage memory outside the chain.\n",
    "       - Support for multiple threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e842d4-95b8-4dd8-9ddd-d48129d61904",
   "metadata": {},
   "source": [
    "## 2-6. Chat History & Memory\n",
    "\n",
    "Chat History:\n",
    "\n",
    "1. `langchain_core.chat_history.BaseChatMessageHistory`: \n",
    "2. `langchain_core.chat_history.InMemoryChatMessageHistory(messages)`: Stores messages in a memory list.\n",
    "\n",
    "Memory:\n",
    "- These classes are built with deprecated `ConversationChain`. The drawback of them is you cannot distinguish between different users. You have to create user session management in the backend to keep track.\n",
    "- Not difficult to implement for chat history.\n",
    "\n",
    "3. `langchain.memory.buffer.ConversationBufferMemory`\n",
    "4. `langchain.memory.buffer_window.ConversationBufferWindowMemory`\n",
    "5. `langchain.memory.token_buffer.ConversationTokenBufferMemory`\n",
    "6. `langchain.memory.summary.ConversationSummaryMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580e91c6-0a51-4a15-a43b-ed6aa075329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d19e30f-289a-419c-a83b-9be365d68f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x7be116947a60>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7be116069d00>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7be128f2db20>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc50b19-3dae-4722-a391-b02877861c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function representing the adjacent side over the hypotenuse in a right-angled triangle.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 31, 'total_tokens': 55}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-75e249db-acaa-4237-a3fd-8bbce9e5cbbc-0', usage_metadata={'input_tokens': 31, 'output_tokens': 24, 'total_tokens': 55})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aadc215b-fcca-4886-92ca-f2128d4e5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function that measures the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 65, 'total_tokens': 91}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4975a40-00a6-4057-a848-810de98fde55-0', usage_metadata={'input_tokens': 65, 'output_tokens': 26, 'total_tokens': 91})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7ee2789-2936-42ca-a41d-a9de3fa7cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! How can I assist you with math today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 28, 'total_tokens': 39}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-0e9362f8-1bf1-43a1-9d7b-e51dc1e0d690-0', usage_metadata={'input_tokens': 28, 'output_tokens': 11, 'total_tokens': 39})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3636213-dd26-4d0a-aad1-1eaca982e9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x7be115f5d4e0>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['ability', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ability'], template=\"You're an assistant who's good at {ability}. Respond in 20 words or fewer\")), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7be11657b230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7be116598590>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7be115f5d800>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7be115f5d760>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='user_id', annotation=<class 'str'>, name='User ID', description='Unique identifier for the user.', default='', is_shared=True, dependencies=None), ConfigurableFieldSpec(id='conversation_id', annotation=<class 'str'>, name='Conversation ID', description='Unique identifier for the conversation.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c050fce-3a3b-4383-888a-960ce9c95f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I assist you with math today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-521b6cf7-caab-457e-bfed-d216142534e9-0', usage_metadata={'input_tokens': 27, 'output_tokens': 12, 'total_tokens': 39})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"Hello\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4af192-a92a-4bc6-a80e-8a71611391e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, I be doin' well, matey! Ready to set sail on the high seas and plunder some treasures. How be ye doin'?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "chain = prompt_template | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
    "\n",
    "wrapped_chain.invoke(\n",
    "    {\"input\": \"how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"42\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b39d5d5-352b-4450-8ea5-df6d5d7af775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='how are you?'), AIMessage(content=\"Arrr, I be doin' well, matey! Ready to set sail on the high seas and plunder some treasures. How be ye doin'?\")])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d35225-9314-402a-aa84-0f00602db874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatPromptTemplate(input_variables=['input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a pirate. Answer the following questions as best you can.')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8603b60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8649190>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "  | StrOutputParser(), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x79be9ff86b60>]))], default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a pirate. Answer the following questions as best you can.')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79bed8603b60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79bed8649190>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "  | StrOutputParser(), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x79be9ff23ce0>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_message_history at 0x79be9ff23420>, input_messages_key='input', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip3 install redis\n",
    "# docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bb5fc25-a051-4a29-ae23-74c5be54e2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, ye be askin' me about cosine again, eh? The cosine be a trigonometric function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be helpin' us pirates calculate angles and distances when sailin' the high seas. Aye, cosine be a valuable tool in our navigational arsenal, me hearty!\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc4dda6-f131-44e0-8964-332df1433ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, the inverse of the cosine function be called arccosine or cos^-1. It be the function that gives ye the angle whose cosine be a given number. It be useful for findin' the original angle from the cosine value, matey. So if ye know the cosine of an angle, ye can use the arccosine to find the angle itself. Aye, it be a handy tool for us pirates to have in our navigational toolkit!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What's its inverse\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c79c2fad-6929-45ab-b624-06734461890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, the inverse of the cosine function be called arccosine or cos^-1. It be the function that gives ye the angle whose cosine be a given number. It be useful for findin\\' the original angle from the cosine value, matey. So if ye know the cosine of an angle, ye can use the arccosine to find the angle itself. Aye, it be a handy tool for us pirates to have in our navigational toolkit!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What\\'s its inverse\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, ye be askin\\' me about cosine again, eh? The cosine be a trigonometric function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be helpin\\' us pirates calculate angles and distances when sailin\\' the high seas. Aye, cosine be a valuable tool in our navigational arsenal, me hearty!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, me hearty! The cosine be a mathematical function that relates to the angle between two sides of a right triangle. It be calculated by dividing the length of the adjacent side by the length of the hypotenuse. It be helpin\\' us pirates navigate the seas with our trusty maps and charts. Aye, it be a useful tool for us buccaneers!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr matey, ye be talkin\\' about the inverse of a number, eh? The inverse of a number be its reciprocal. So if ye be talkin\\' about the number 5, its inverse be 1/5, savvy? Aye, the inverse be like the opposite of the number, ye see? If ye have any more questions, just ask away!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What\\'s its inverse\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       " '{\"type\": \"ai\", \"data\": {\"content\": \"Arrr, cosine be a mathematical function that relates to the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse. It be used to calculate angles and distances in navigation, me heartie.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": null}}',\n",
       " '{\"type\": \"human\", \"data\": {\"content\": \"What does cosine mean?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~$ docker exec -it <CONTAINER ID> redis-cli\n",
    "# 127.0.0.1:6379> KEYS *\n",
    "# 1) \"message_store:foobar\"\n",
    "# 127.0.0.1:6379> LRANGE message_store:foobar 0 -1\n",
    "\n",
    "import redis\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "r.lrange(\"message_store:foobar\", 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4562fb-9186-40d2-bc7a-42528cd022c8",
   "metadata": {},
   "source": [
    "## 2-7. Document Loaders\n",
    "\n",
    "LangChain has hundreds of integrations with various data sources to load data from, such as Google Drive, Amazon Simple Storage Service (Amazon S3), Blockchain, Microsoft PowerPoint & more.\n",
    "- From `langchain_community`. Additional setups may be required for specific loaders.\n",
    "- [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "\n",
    "Text:\n",
    "\n",
    "1. `TextLoader`\n",
    "\n",
    "File Directory:\n",
    "\n",
    "2. `DirectoryLoader`\n",
    "\n",
    "CSV:\n",
    "\n",
    "3. `CSVLoader`\n",
    "\n",
    "JSON:\n",
    "\n",
    "4. `JSONLoader`\n",
    "\n",
    "HTML:\n",
    "\n",
    "5. `UnstructuredHTMLLoader`\n",
    "6. `langchain_community.document_loaders.WebBaseLoader`\n",
    "\n",
    "PDF:\n",
    "\n",
    "6. `PyPDFLoader` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548f4b8d-f1ea-4be2-8278-e12aca7d403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './README.md'}, page_content='# PyTorch Natural Language Processing\\n')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./README.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a5b5591-7c47-4e7e-bdf0-81c153ca9dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading https://www.nytimes.com/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_community/document_loaders/text.py:42\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://www.nytimes.com/'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredHTMLLoader\n\u001b[1;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.nytimes.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/langchain_community/document_loaders/text.py:58\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     60\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)}\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading https://www.nytimes.com/"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = TextLoader(\"https://www.nytimes.com/\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a82ba5d2-1768-4b58-aff1-fbff8510877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/1706.03762v7.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pypdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/1706.03762v7.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba2f4c-c2e3-45a3-b120-34084b59d317",
   "metadata": {},
   "source": [
    "## 2-8. Text Splitters\n",
    "\n",
    "Base interface:\n",
    "\n",
    "1. `langchain_text_splitters.base.TextSplitter(chunk_size=4000, chunk_overlap=200, length_function=<built-in function len>, keep_separator=False, add_start_index=False, strip_whitespace=True)`: All LangChain text splitters inherit from. To maintain context between chunks, setting `chunk_overlap` can ensure information isn't lost at chunk boundaries. `add_start_index` includes chunk's start index in metadata if `True`.\n",
    "    - `split_text(text)`\n",
    "    - `split_documents(documents)`\n",
    "    - `from_tiktoken_encoder(encoding_name='gpt2', model_name=None, allowed_special={}, disallowed_special='all', **kwargs)`\n",
    "    - `from_huggingface_tokenizer(tokenizer, **kwargs)`\n",
    "\n",
    "Semantic chunking:\n",
    "\n",
    "Split by tokens:\n",
    "\n",
    "1. `langchain_text_splitters.base.TokenTextSplitter(encoding_name='gpt2', model_name=None, allowed_special={}, disallowed_special='all', **kwargs)`: Splits text to tokens using model tokenizer.\n",
    "2. `langchain_text_splitters.character.CharacterTextSplitter(separator='\\n\\n', is_separator_regex=False, **kwargs)`: Splits text that looks at characters.\n",
    "3. `langchain_text_splitters.character.RecursiveCharacterTextSplitter(separators=None, keep_separator=True, is_separator_regex=False, **kwargs)`: Splits text by recursively look at characters.\n",
    "   - `from_language(language, **kwargs)`: splits code.\n",
    "\n",
    "Split HTML:\n",
    "\n",
    "Split Markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f3f6e2b-4e94-4cec-b471-61edc2a2301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1992, which is longer than the specified 1000\n",
      "Created a chunk of size 1317, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1849, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Created a chunk of size 1370, which is longer than the specified 1000\n",
      "Created a chunk of size 1265, which is longer than the specified 1000\n",
      "Created a chunk of size 1465, which is longer than the specified 1000\n",
      "Created a chunk of size 1624, which is longer than the specified 1000\n",
      "Created a chunk of size 1227, which is longer than the specified 1000\n",
      "Created a chunk of size 1144, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 1439, which is longer than the specified 1000\n",
      "Created a chunk of size 1970, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                        Enter Ghost.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `CharacterTextSplitter()`\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"./datasets/hamlet.txt\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\", chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57c8768e-29d8-4524-a513-688bf5df4c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db526a085e34466899680815ef484b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a5cb3554264c26990a1b356f7382f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1340475a8e4a8388a4cb1ae343000c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220a07645aca4434b25c7c2d0e00611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8849a11670484664a03f33b95bbd0e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Created a chunk of size 1323, which is longer than the specified 1000\n",
      "Created a chunk of size 2474, which is longer than the specified 1000\n",
      "Created a chunk of size 1575, which is longer than the specified 1000\n",
      "Created a chunk of size 1368, which is longer than the specified 1000\n",
      "Created a chunk of size 1023, which is longer than the specified 1000\n",
      "Created a chunk of size 1676, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 2120, which is longer than the specified 1000\n",
      "Created a chunk of size 1403, which is longer than the specified 1000\n",
      "Created a chunk of size 1622, which is longer than the specified 1000\n",
      "Created a chunk of size 1537, which is longer than the specified 1000\n",
      "Created a chunk of size 1880, which is longer than the specified 1000\n",
      "Created a chunk of size 2063, which is longer than the specified 1000\n",
      "Created a chunk of size 1411, which is longer than the specified 1000\n",
      "Created a chunk of size 1718, which is longer than the specified 1000\n",
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1628, which is longer than the specified 1000\n",
      "Created a chunk of size 2732, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hugging Face tokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de72f406-9e10-4b10-bf60-92e3e07134d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                        Enter Ghost.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `RecursiveCharacterTextSplitter()`\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(model_name=\"gpt-4\", chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf241555-61f4-4afc-9313-c88c283a6ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split code\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "python_code = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=50, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents([python_code])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4b7b0d9-21ac-4c79-9dd6-4305e0ca4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  Marcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  Polonius, Lord Chamberlain.\\n  Horatio, friend to Hamlet.\\n  Laertes, son to Polonius.\\n  Voltemand, courtier.\\n  Cornelius, courtier.\\n  Rosencrantz, courtier.\\n  Guildenstern, courtier.\\n  Osric, courtier.\\n  A Gentleman, courtier.\\n  A Priest.\\n  Marcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  Two Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  A Norwegian Captain.\\n  English Ambassadors.\\n\\n  Getrude, Queen of Denmark, mother to Hamlet.\\n  Ophelia, daughter to Polonius.\\n\\n  Ghost of Hamlet's Father.\\n\\n  Lords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[first,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who's there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bernardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get thee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at heart.\\n  Ber. Have you had quiet guard?\\n  Fran. Not a mouse stirring.\\n  Ber. Well, good night.\\n    If you do meet Horatio and Marcellus,\\n    The rivals of my watch, bid them make haste.\\n\\n                    Enter Horatio and Marcellus.  \\n\\n  Fran. I think I hear them. Stand, ho! Who is there?\\n  Hor. Friends to this ground.\\n  Mar. And liegemen to the Dane.\\n  Fran. Give you good night.\\n  Mar. O, farewell, honest soldier.\\n    Who hath reliev'd you?\\n  Fran. Bernardo hath my place.\\n    Give you good night.                                   Exit.\\n  Mar. Holla, Bernardo!\\n  Ber. Say-\\n    What, is Horatio there ?\\n  Hor. A piece of him.\\n  Ber. Welcome, Horatio. Welcome, good Marcellus.\\n  Mar. What, has this thing appear'd again to-night?\\n  Ber. I have seen nothing.\\n  Mar. Horatio says 'tis but our fantasy,\\n    And will not let belief take hold of him\\n    Touching this dreaded sight, twice seen of us.\\n    Therefore I have entreated him along,  \\n    With us to watch the minutes of this night,\\n    That, if again this apparition come,\\n    He may approve our eyes and speak to it.\\n  Hor. Tush, tush, 'twill not appear.\\n  Ber. Sit down awhile,\\n    And let us once again assail your ears,\\n    That are so fortified against our story,\\n    What we two nights have seen.\\n  Hor. Well, sit we down,\\n    And let us hear Bernardo speak of this.\\n  Ber. Last night of all,\\n    When yond same star that's westward from the pole\\n    Had made his course t' illume that part of heaven\\n    Where now it burns, Marcellus and myself,\\n    The bell then beating one-\\n\\n                  \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `TokenTextSplitter()`\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7a63263-f368-4b65-abab-1ff6ba1b094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n  \\n\\nMarcellus, Officer.\\n  Hamlet, son to the former, and nephew to the present king.\\n  \\n\\nPolonius, Lord Chamberlain.\\n  \\n\\nHoratio, friend to Hamlet.\\n  \\n\\nLaertes, son to Polonius.\\n  \\n\\nVoltemand, courtier.\\n  \\n\\nCornelius, courtier.\\n  \\n\\nRosencrantz, courtier.\\n  \\n\\nGuildenstern, courtier.\\n  \\n\\nOsric, courtier.\\n  \\n\\nA Gentleman, courtier.\\n  \\n\\nA Priest.\\n  \\n\\nMarcellus, officer.\\n  Bernardo, officer.\\n  Francisco, a soldier\\n  Reynaldo, servant to Polonius.\\n  Players.\\n  \\n\\nTwo Clowns, gravediggers.\\n  Fortinbras, Prince of Norway.  \\n  \\n\\nA Norwegian Captain.\\n  \\n\\nEnglish Ambassadors.\\n\\n  \\n\\nGetrude, Queen of Denmark, mother to Hamlet.\\n  \\n\\nOphelia, daughter to Polonius.\\n\\n  \\n\\nGhost of Hamlet's Father.\\n\\n  \\n\\nLords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\n\\n\\n\\n\\n\\n\\nSCENE.- Elsinore.\\n\\n\\n\\n\\nACT I. Scene I.\\nElsinore.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `SpacyTextSplitter()`\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867e640-0bc7-4e78-94da-05113c58d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `SentenceTransformersTokenTextSplitter()`\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3f55948-590b-4ec5-9b7d-45b9f4186365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\nby William Shakespeare\\n\\n\\n\\nDramatis Personae\\n\\n  Claudius, King of Denmark.\\n\\nMarcellus, Officer.\\n\\nHamlet, son to the former, and nephew to the present king.\\n\\nPolonius, Lord Chamberlain.\\n\\nHoratio, friend to Hamlet.\\n\\nLaertes, son to Polonius.\\n\\nVoltemand, courtier.\\n\\nCornelius, courtier.\\n\\nRosencrantz, courtier.\\n\\nGuildenstern, courtier.\\n\\nOsric, courtier.\\n\\nA Gentleman, courtier.\\n\\nA Priest.\\n\\nMarcellus, officer.\\n\\nBernardo, officer.\\n\\nFrancisco, a soldier\\n  Reynaldo, servant to Polonius.\\n\\nPlayers.\\n\\nTwo Clowns, gravediggers.\\n\\nFortinbras, Prince of Norway.\\n\\nA Norwegian Captain.\\n\\nEnglish Ambassadors.\\n\\nGetrude, Queen of Denmark, mother to Hamlet.\\n\\nOphelia, daughter to Polonius.\\n\\nGhost of Hamlet's Father.\\n\\nLords, ladies, Officers, Soldiers, Sailors, Messengers, Attendants.\\n\\nSCENE.- Elsinore.\\n\\nACT I.\\n\\nScene I.\\nElsinore.\\n\\nA platform before the Castle.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `NLTKTextSplitter()`\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(hamlet)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda38b0c-dffd-433a-ba2b-b5ba03179066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e27546-f614-42f1-91f7-de0da6485c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28910bda-ac14-432f-9cea-34adb43b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `MarkdownHeaderTextSplitter()`\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64dcc0-6071-4fcd-8768-1ec8f7b3d264",
   "metadata": {},
   "source": [
    "## 2-9. Embedding Models\n",
    "\n",
    "Base interface:\n",
    "\n",
    "1. `langchain_core.embeddings.embeddings.Embeddings`\n",
    "    - `embed_documents(text)`: Embeds multiple search documents.\n",
    "    - `embed_query(text)`: Embed a single query text.\n",
    "\n",
    "Text embedding models:\n",
    "\n",
    "2. `langchain_openai.embeddings.base.OpenAIEmbeddings(allowed_special=None, check_embedding_ctx_length=True, chunk_size=1000, default_headers=None, default_query=None, deployment='text-embedding-ada-002', dimensions=None, disallowed_special=None, embedding_ctx_length=8191, headers=None, http_async_client=None, http_client=None, max_retries=2, model='text-embedding-ada-002', model_kwargs, openai_api_base=None, openai_api_key=None, openai_api_type=None, openai_api_version=None, openai_organization=None, openai_proxy=None, request_timeout=None, retry_max_seconds=20, retry_min_seconds=4, show_progress_bar=False, skip_empty=False, tiktoken_enabled=True, tiktoken_model_name=None)`: OpenAI embedding models.\n",
    "3. `langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings(cache_folder=None, encode_kwargs, model_kwargs, model_name='sentence-transformers/all-mpnet-base-v2', multi_process=False, show_progress=False)`: HuggingFace Sentence Transformers embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca25a40-3e25-49ae-945a-8bb7c9f10f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `embed_documents()`\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7669f538-242b-40f5-b722-a05797b7e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005377273540943861,\n",
       " -0.0006527779041789472,\n",
       " 0.038980286568403244,\n",
       " -0.002967397216707468,\n",
       " -0.008834563195705414]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `embed_query()`\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cabc8da-3e77-4317-a20a-88be255f4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-07-24 11:29:22.010982: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-24 11:29:22.018966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 11:29:22.029888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 11:29:22.029925: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 11:29:22.038087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 11:29:22.427958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4034d4e7ce4c5f9b57a5dffc0e3ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caf4b648ddf4d7ead9d3525d1e41411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ddddf029e14f01a5ac18f3685a45c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f875d0dbe5a04afa8653eeac4e1719b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead6aa0fd7a144d4bd6195a827933cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d45c33bd20499da6af927f74424702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f4178e4a1b4be0bfec309f12f87725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6056e054f2f343f2af063a3ca8878785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb0399a9fd54cdc83ce50d6a9c17bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee55cf3ec07a4539b8e4dc4e098f0223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d68e1b0cdc4d12a744479e229196af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip3 install langchain-huggingface sentence-transformers datasets\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614c35f7-82ec-41f2-ade7-ae0be0731f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09514585137367249,\n",
       " 9.88791070994921e-05,\n",
       " -0.016573384404182434,\n",
       " 0.044847987592220306,\n",
       " 0.04323696717619896]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc89ce-17b6-4577-aa6c-4a3f10506499",
   "metadata": {},
   "source": [
    "## 2-10. Vector Stores\n",
    "\n",
    "**Chroma** is a vector database which prioritizes simplicity & developer productivity and offers self-hosted server option built to run seamlessly during local development, making it easier to prototype LLM applications. It gives you the tools to store embeddings & their metadata, embed documents & queries, and search embeddings. Allows for more flexible querying capabilities compared with Pinecone and excels in high-throughput operations & real-time scalability.\n",
    "\n",
    "1. `langchain_chroma.vectorstores.Chroma(collection_name='langchain', embedding_function=None, persist_directory=None, client_settings=None, collection_metadata=None, client=None, relevance_score_fn=None, create_collection_if_not_exists=True)`: ChromaDB vector store.\n",
    "    - `from_documents(documents, embedding=None, ids=None, collection_name='langchain', persist_directory=None, client_settings=None, client=None, collection_metadata=None, **kwargs)`: Creates a Chroma vector store from a list of documents.\n",
    "    - `from_texts(texts, embedding=None, metadatas=None, ids=None, collection_name='langchain', persist_directory=None, client_settings=None, client=None, collection_metadata=None, **kwargs)`: Creates a Chroma vector store from raw documents.\n",
    "    - `similarity_search(query, k=4, filter=None, **kwargs)`\n",
    "    - `similarity_search_by_vector(embedding, k=4, filter=None, where_document=None, **kwargs)`\n",
    "    - `similarity_search_by_vector_with_relevance_scores(embedding, k=4, filter=None, where_document=None, **kwargs)`\n",
    "    - `similarity_search_with_score(query, k=4, filter=None, where_document=None, **kwargs)`\n",
    "    - `similarity_search_with_relevance_scores(query, k=4, **kwargs)`: Returns docs and relevance scores in the range `[0, 1]`.\n",
    "    - `similarity_search_by_image(uri, k=4, filter=None, **kwargs)`: Searches for similar images based on the given image URI.\n",
    "    - `similarity_search_by_image_with_relevance_score(uri, k=4, filter=None, **kwargs)`\n",
    "    - `max_marginal_relevance_search(query, k=4, fetch_k=20, lambda_mult=0.5, filter=None, where_document=None, **kwargs)` \n",
    "    - `max_marginal_relevance_search_by_vector(embedding, k=4, fetch_k=20, lambda_mult=0.5, filter=None, where_document=None, **kwargs)`\n",
    "    - `as_retriever(**kwargs)`: Returns `VectorStoreRetriever` initialized from this vector store. The usage is demonstrated in the retriever section.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is a high performance library created and optimized for dense vector similarity search & clustering. FAISS is built around several index types using **Approximate Nearest Neighbor (ANN) Search** algorithms, such as **Inverted File Index (IVF)**, **Locality Sensitive Hashing (LSH)**, **Hierarchical Navigable Small Worlds (HNSW)** & more. Also supports evaluation & parameter tuning but lacks features like filtering & post processing compared with Chroma.\n",
    "- [faiss Indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)\n",
    "\n",
    "2. `langchain_community.vectorstores.faiss.FAISS(embedding_function, index, docstore, index_to_docstore_id, relevance_score_fn=None, normalize_L2=False, distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE)`: Meta FAISS vector store.\n",
    "    - `from_documents(documents, embedding, **kwargs)`\n",
    "    - `from_texts(texts, embedding, metadatas=None, ids=None, **kwargs)`\n",
    "    - `from_embeddings(text_embeddings, embedding, metadatas=None, ids=None, **kwargs)`\n",
    "    - `similarity_search(query, k=4, filter=None, fetch_k=20, **kwargs)`\n",
    "    - `similarity_search_by_vector(embedding, k=4, filter=None, fetch_k=20, **kwargs)`\n",
    "    - `similarity_search_with_relevance_scores(query, k=4, **kwargs)`\n",
    "    - `similarity_search_with_score(query, k=4, filter=None, fetch_k=20, **kwargs)`\n",
    "    - `similarity_search_with_score_by_vector(embedding, k=4, filter=None, fetch_k=20, **kwargs)`\n",
    "    - `max_marginal_relevance_search(query, k=4, fetch_k=20, lambda_mult=0.5, filter=None, **kwargs)`\n",
    "    - `max_marginal_relevance_search_by_vector(embedding, k=4, fetch_k=20, lambda_mult=0.5, filter=None, **kwargs)`\n",
    "    - `max_marginal_relevance_search_with_score_by_vector(embedding, *, k=4, fetch_k=20, lambda_mult=0.5, filter=None)`: Returns documents and their similarity scores selected using the maximal marginal relevance.\n",
    "    - `as_retriever(**kwargs)`\n",
    "  \n",
    "**Pinecone** is a cloud-based fully managed vector platform designed to handle real-time search & similarity matching at scale. Also offers automatic indexing and shines on its low latency performance & high scalability due to cloud infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997ee879-b96d-401b-987a-a1c7bae7c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1307, which is longer than the specified 1000\n",
      "Created a chunk of size 4126, which is longer than the specified 1000\n",
      "Created a chunk of size 2446, which is longer than the specified 1000\n",
      "Created a chunk of size 7752, which is longer than the specified 1000\n",
      "Created a chunk of size 4735, which is longer than the specified 1000\n",
      "Created a chunk of size 2413, which is longer than the specified 1000\n",
      "Created a chunk of size 4118, which is longer than the specified 1000\n",
      "Created a chunk of size 1808, which is longer than the specified 1000\n",
      "Created a chunk of size 2816, which is longer than the specified 1000\n",
      "Created a chunk of size 5153, which is longer than the specified 1000\n",
      "Created a chunk of size 1735, which is longer than the specified 1000\n",
      "Created a chunk of size 2029, which is longer than the specified 1000\n",
      "Created a chunk of size 3485, which is longer than the specified 1000\n",
      "Created a chunk of size 2235, which is longer than the specified 1000\n",
      "Created a chunk of size 1987, which is longer than the specified 1000\n",
      "Created a chunk of size 2632, which is longer than the specified 1000\n",
      "Created a chunk of size 2128, which is longer than the specified 1000\n",
      "Created a chunk of size 2365, which is longer than the specified 1000\n",
      "Created a chunk of size 6993, which is longer than the specified 1000\n",
      "Created a chunk of size 1116, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1424, which is longer than the specified 1000\n",
      "Created a chunk of size 4146, which is longer than the specified 1000\n",
      "Created a chunk of size 2788, which is longer than the specified 1000\n",
      "Created a chunk of size 5125, which is longer than the specified 1000\n",
      "Created a chunk of size 1315, which is longer than the specified 1000\n",
      "Created a chunk of size 2468, which is longer than the specified 1000\n",
      "Created a chunk of size 1792, which is longer than the specified 1000\n",
      "Created a chunk of size 1898, which is longer than the specified 1000\n",
      "Created a chunk of size 1337, which is longer than the specified 1000\n",
      "Created a chunk of size 2050, which is longer than the specified 1000\n",
      "Created a chunk of size 1414, which is longer than the specified 1000\n",
      "Created a chunk of size 2022, which is longer than the specified 1000\n",
      "Created a chunk of size 1219, which is longer than the specified 1000\n",
      "Created a chunk of size 1206, which is longer than the specified 1000\n",
      "Created a chunk of size 1304, which is longer than the specified 1000\n",
      "Created a chunk of size 2301, which is longer than the specified 1000\n",
      "Created a chunk of size 1220, which is longer than the specified 1000\n",
      "Created a chunk of size 4706, which is longer than the specified 1000\n",
      "Created a chunk of size 5622, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n",
      "Created a chunk of size 1216, which is longer than the specified 1000\n",
      "Created a chunk of size 2588, which is longer than the specified 1000\n",
      "Created a chunk of size 2774, which is longer than the specified 1000\n",
      "Created a chunk of size 1064, which is longer than the specified 1000\n",
      "Created a chunk of size 1647, which is longer than the specified 1000\n",
      "Created a chunk of size 2116, which is longer than the specified 1000\n",
      "Created a chunk of size 1022, which is longer than the specified 1000\n",
      "Created a chunk of size 1261, which is longer than the specified 1000\n",
      "Created a chunk of size 1722, which is longer than the specified 1000\n",
      "Created a chunk of size 6213, which is longer than the specified 1000\n",
      "Created a chunk of size 1653, which is longer than the specified 1000\n",
      "Created a chunk of size 2448, which is longer than the specified 1000\n",
      "Created a chunk of size 1546, which is longer than the specified 1000\n",
      "Created a chunk of size 1097, which is longer than the specified 1000\n",
      "Created a chunk of size 4263, which is longer than the specified 1000\n",
      "Created a chunk of size 4484, which is longer than the specified 1000\n",
      "Created a chunk of size 3786, which is longer than the specified 1000\n",
      "Created a chunk of size 5191, which is longer than the specified 1000\n",
      "Created a chunk of size 1469, which is longer than the specified 1000\n",
      "Created a chunk of size 7335, which is longer than the specified 1000\n",
      "Created a chunk of size 2169, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7a0c7c079ca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('./datasets/hamlet.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9d1816-5549-4277-a718-969b29edb050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ham. To be, or not to be- that is the question:\\n    Whether 'tis nobler in the mind to suffer\\n    The slings and arrows of outrageous fortune\\n    Or to take arms against a sea of troubles,\\n    And by opposing end them. To die- to sleep-\\n    No more; and by a sleep to say we end\\n    The heartache, and the thousand natural shocks\\n    That flesh is heir to. 'Tis a consummation  \\n    Devoutly to be wish'd. To die- to sleep.\\n    To sleep- perchance to dream: ay, there's the rub!\\n    For in that sleep of death what dreams may come\\n    When we have shuffled off this mortal coil,\\n    Must give us pause. There's the respect\\n    That makes calamity of so long life.\\n    For who would bear the whips and scorns of time,\\n    Th' oppressor's wrong, the proud man's contumely,\\n    The pangs of despis'd love, the law's delay,\\n    The insolence of office, and the spurns\\n    That patient merit of th' unworthy takes,\\n    When he himself might his quietus make\\n    With a bare bodkin? Who would these fardels bear,\\n    To grunt and sweat under a weary life,\\n    But that the dread of something after death-\\n    The undiscover'd country, from whose bourn\\n    No traveller returns- puzzles the will,\\n    And makes us rather bear those ills we have\\n    Than fly to others that we know not of?\\n    Thus conscience does make cowards of us all,  \\n    And thus the native hue of resolution\\n    Is sicklied o'er with the pale cast of thought,\\n    And enterprises of great pith and moment\\n    With this regard their currents turn awry\\n    And lose the name of action.- Soft you now!\\n    The fair Ophelia!- Nymph, in thy orisons\\n    Be all my sins rememb'red.\\n  Oph. Good my lord,\\n    How does your honour for this many a day?\\n  Ham. I humbly thank you; well, well, well.\\n  Oph. My lord, I have remembrances of yours\\n    That I have longed long to re-deliver.\\n    I pray you, now receive them.\\n  Ham. No, not I!\\n    I never gave you aught.\\n  Oph. My honour'd lord, you know right well you did,\\n    And with them words of so sweet breath compos'd\\n    As made the things more rich. Their perfume lost,\\n    Take these again; for to the noble mind\\n    Rich gifts wax poor when givers prove unkind.  \\n    There, my lord.\\n  Ham. Ha, ha! Are you honest?\\n  Oph. My lord?\\n  Ham. Are you fair?\\n  Oph. What means your lordship?\\n  Ham. That if you be honest and fair, your honesty should admit no\\n    discourse to your beauty.\\n  Oph. Could beauty, my lord, have better commerce than with honesty?\\n  Ham. Ay, truly; for the power of beauty will sooner transform\\n    honesty from what it is to a bawd than the force of honesty can\\n    translate beauty into his likeness. This was sometime a paradox,\\n    but now the time gives it proof. I did love you once.\\n  Oph. Indeed, my lord, you made me believe so.\\n  Ham. You should not have believ'd me; for virtue cannot so\\n    inoculate our old stock but we shall relish of it. I loved you\\n    not.\\n  Oph. I was the more deceived.\\n  Ham. Get thee to a nunnery! Why wouldst thou be a breeder of\\n    sinners? I am myself indifferent honest, but yet I could accuse\\n    me of such things that it were better my mother had not borne me.  \\n    I am very proud, revengeful, ambitious; with more offences at my\\n    beck than I have thoughts to put them in, imagination to give\\n    them shape, or time to act them in. What should such fellows as I\\n    do, crawling between earth and heaven? We are arrant knaves all;\\n    believe none of us. Go thy ways to a nunnery. Where's your\\n    father?\\n  Oph. At home, my lord.\\n  Ham. Let the doors be shut upon him, that he may play the fool\\n    nowhere but in's own house. Farewell.\\n  Oph. O, help him, you sweet heavens!\\n  Ham. If thou dost marry, I'll give thee this plague for thy dowry:\\n    be thou as chaste as ice, as pure as snow, thou shalt not escape\\n    calumny. Get thee to a nunnery. Go, farewell. Or if thou wilt\\n    needs marry, marry a fool; for wise men know well enough what\\n    monsters you make of them. To a nunnery, go; and quickly too.\\n    Farewell.\\n  Oph. O heavenly powers, restore him!\\n  Ham. I have heard of your paintings too, well enough. God hath\\n    given you one face, and you make yourselves another. You jig, you\\n    amble, and you lisp; you nickname God's creatures and make your  \\n    wantonness your ignorance. Go to, I'll no more on't! it hath made\\n    me mad. I say, we will have no moe marriages. Those that are\\n    married already- all but one- shall live; the rest shall keep as\\n    they are. To a nunnery, go.                            Exit.\\n  Oph. O, what a noble mind is here o'erthrown!\\n    The courtier's, scholar's, soldier's, eye, tongue, sword,\\n    Th' expectancy and rose of the fair state,\\n    The glass of fashion and the mould of form,\\n    Th' observ'd of all observers- quite, quite down!\\n    And I, of ladies most deject and wretched,\\n    That suck'd the honey of his music vows,\\n    Now see that noble and most sovereign reason,\\n    Like sweet bells jangled, out of tune and harsh;\\n    That unmatch'd form and feature of blown youth\\n    Blasted with ecstasy. O, woe is me\\n    T' have seen what I have seen, see what I see!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `similarity_search()`\n",
    "query = \"This is the excellent foppery of the world, that when we are sick in fortune (often the surfeits of our own behavior) we make guilty of our disasters the sun, the moon, and stars, as if we were villains on necessity; fools by heavenly compulsion; knaves, thieves, and treachers by spherical predominance; drunkards, liars, and adulterers by an enforced obedience of planetary influence; and all that we are evil in, by a divine thrusting on. An admirable evasion of whoremaster man, to lay his goatish disposition on the charge of a star! My father compounded with my mother under the Dragon's tail, and my nativity was under Ursa Major, so that it follows I am rough and lecherous. I should have been that I am, had the maidenliest star in the firmament twinkled on my bastardizing.\"\n",
    "docs = db.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e187f332-cd64-441f-ab52-923e7a918b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ham. To be, or not to be- that is the question:\\n    Whether 'tis nobler in the mind to suffer\\n    The slings and arrows of outrageous fortune\\n    Or to take arms against a sea of troubles,\\n    And by opposing end them. To die- to sleep-\\n    No more; and by a sleep to say we end\\n    The heartache, and the thousand natural shocks\\n    That flesh is heir to. 'Tis a consummation  \\n    Devoutly to be wish'd. To die- to sleep.\\n    To sleep- perchance to dream: ay, there's the rub!\\n    For in that sleep of death what dreams may come\\n    When we have shuffled off this mortal coil,\\n    Must give us pause. There's the respect\\n    That makes calamity of so long life.\\n    For who would bear the whips and scorns of time,\\n    Th' oppressor's wrong, the proud man's contumely,\\n    The pangs of despis'd love, the law's delay,\\n    The insolence of office, and the spurns\\n    That patient merit of th' unworthy takes,\\n    When he himself might his quietus make\\n    With a bare bodkin? Who would these fardels bear,\\n    To grunt and sweat under a weary life,\\n    But that the dread of something after death-\\n    The undiscover'd country, from whose bourn\\n    No traveller returns- puzzles the will,\\n    And makes us rather bear those ills we have\\n    Than fly to others that we know not of?\\n    Thus conscience does make cowards of us all,  \\n    And thus the native hue of resolution\\n    Is sicklied o'er with the pale cast of thought,\\n    And enterprises of great pith and moment\\n    With this regard their currents turn awry\\n    And lose the name of action.- Soft you now!\\n    The fair Ophelia!- Nymph, in thy orisons\\n    Be all my sins rememb'red.\\n  Oph. Good my lord,\\n    How does your honour for this many a day?\\n  Ham. I humbly thank you; well, well, well.\\n  Oph. My lord, I have remembrances of yours\\n    That I have longed long to re-deliver.\\n    I pray you, now receive them.\\n  Ham. No, not I!\\n    I never gave you aught.\\n  Oph. My honour'd lord, you know right well you did,\\n    And with them words of so sweet breath compos'd\\n    As made the things more rich. Their perfume lost,\\n    Take these again; for to the noble mind\\n    Rich gifts wax poor when givers prove unkind.  \\n    There, my lord.\\n  Ham. Ha, ha! Are you honest?\\n  Oph. My lord?\\n  Ham. Are you fair?\\n  Oph. What means your lordship?\\n  Ham. That if you be honest and fair, your honesty should admit no\\n    discourse to your beauty.\\n  Oph. Could beauty, my lord, have better commerce than with honesty?\\n  Ham. Ay, truly; for the power of beauty will sooner transform\\n    honesty from what it is to a bawd than the force of honesty can\\n    translate beauty into his likeness. This was sometime a paradox,\\n    but now the time gives it proof. I did love you once.\\n  Oph. Indeed, my lord, you made me believe so.\\n  Ham. You should not have believ'd me; for virtue cannot so\\n    inoculate our old stock but we shall relish of it. I loved you\\n    not.\\n  Oph. I was the more deceived.\\n  Ham. Get thee to a nunnery! Why wouldst thou be a breeder of\\n    sinners? I am myself indifferent honest, but yet I could accuse\\n    me of such things that it were better my mother had not borne me.  \\n    I am very proud, revengeful, ambitious; with more offences at my\\n    beck than I have thoughts to put them in, imagination to give\\n    them shape, or time to act them in. What should such fellows as I\\n    do, crawling between earth and heaven? We are arrant knaves all;\\n    believe none of us. Go thy ways to a nunnery. Where's your\\n    father?\\n  Oph. At home, my lord.\\n  Ham. Let the doors be shut upon him, that he may play the fool\\n    nowhere but in's own house. Farewell.\\n  Oph. O, help him, you sweet heavens!\\n  Ham. If thou dost marry, I'll give thee this plague for thy dowry:\\n    be thou as chaste as ice, as pure as snow, thou shalt not escape\\n    calumny. Get thee to a nunnery. Go, farewell. Or if thou wilt\\n    needs marry, marry a fool; for wise men know well enough what\\n    monsters you make of them. To a nunnery, go; and quickly too.\\n    Farewell.\\n  Oph. O heavenly powers, restore him!\\n  Ham. I have heard of your paintings too, well enough. God hath\\n    given you one face, and you make yourselves another. You jig, you\\n    amble, and you lisp; you nickname God's creatures and make your  \\n    wantonness your ignorance. Go to, I'll no more on't! it hath made\\n    me mad. I say, we will have no moe marriages. Those that are\\n    married already- all but one- shall live; the rest shall keep as\\n    they are. To a nunnery, go.                            Exit.\\n  Oph. O, what a noble mind is here o'erthrown!\\n    The courtier's, scholar's, soldier's, eye, tongue, sword,\\n    Th' expectancy and rose of the fair state,\\n    The glass of fashion and the mould of form,\\n    Th' observ'd of all observers- quite, quite down!\\n    And I, of ladies most deject and wretched,\\n    That suck'd the honey of his music vows,\\n    Now see that noble and most sovereign reason,\\n    Like sweet bells jangled, out of tune and harsh;\\n    That unmatch'd form and feature of blown youth\\n    Blasted with ecstasy. O, woe is me\\n    T' have seen what I have seen, see what I see!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `similarity_search_by_vector()`\n",
    "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebd77f6-b065-4d85-bcc8-ad3d08f9e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7a0c6819b5c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c63e047-93a9-4b96-8ec7-017cfb9f0cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"King. Full thirty times hath Phoebus' cart gone round\\n      Neptune's salt wash and Tellus' orbed ground,\\n      And thirty dozed moons with borrowed sheen\\n      About the world have times twelve thirties been,\\n      Since love our hearts, and Hymen did our hands,\\n      Unite comutual in most sacred bands.\\n    Queen. So many journeys may the sun and moon\\n      Make us again count o'er ere love be done!\\n      But woe is me! you are so sick of late,\\n      So far from cheer and from your former state.\\n      That I distrust you. Yet, though I distrust,\\n      Discomfort you, my lord, it nothing must;\\n      For women's fear and love holds quantity,\\n      In neither aught, or in extremity.\\n      Now what my love is, proof hath made you know;\\n      And as my love is siz'd, my fear is so.\\n      Where love is great, the littlest doubts are fear;\\n      Where little fears grow great, great love grows there.\\n    King. Faith, I must leave thee, love, and shortly too;  \\n      My operant powers their functions leave to do.\\n      And thou shalt live in this fair world behind,\\n      Honour'd, belov'd, and haply one as kind\\n      For husband shalt thou-\\n    Queen. O, confound the rest!\\n      Such love must needs be treason in my breast.\\n      When second husband let me be accurst!\\n      None wed the second but who killed the first.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `similarity_search()`\n",
    "query = \"This is the excellent foppery of the world, that when we are sick in fortune (often the surfeits of our own behavior) we make guilty of our disasters the sun, the moon, and stars, as if we were villains on necessity; fools by heavenly compulsion; knaves, thieves, and treachers by spherical predominance; drunkards, liars, and adulterers by an enforced obedience of planetary influence; and all that we are evil in, by a divine thrusting on. An admirable evasion of whoremaster man, to lay his goatish disposition on the charge of a star! My father compounded with my mother under the Dragon's tail, and my nativity was under Ursa Major, so that it follows I am rough and lecherous. I should have been that I am, had the maidenliest star in the firmament twinkled on my bastardizing.\"\n",
    "docs = db.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca11b0b0-ef11-49fc-852f-52b08e5475ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King. Full thirty times hath Phoebus' cart gone round\n",
      "      Neptune's salt wash and Tellus' orbed ground,\n",
      "      And thirty dozed moons with borrowed sheen\n",
      "      About the world have times twelve thirties been,\n",
      "      Since love our hearts, and Hymen did our hands,\n",
      "      Unite comutual in most sacred bands.\n",
      "    Queen. So many journeys may the sun and moon\n",
      "      Make us again count o'er ere love be done!\n",
      "      But woe is me! you are so sick of late,\n",
      "      So far from cheer and from your former state.\n",
      "      That I distrust you. Yet, though I distrust,\n",
      "      Discomfort you, my lord, it nothing must;\n",
      "      For women's fear and love holds quantity,\n",
      "      In neither aught, or in extremity.\n",
      "      Now what my love is, proof hath made you know;\n",
      "      And as my love is siz'd, my fear is so.\n",
      "      Where love is great, the littlest doubts are fear;\n",
      "      Where little fears grow great, great love grows there.\n",
      "    King. Faith, I must leave thee, love, and shortly too;  \n",
      "      My operant powers their functions leave to do.\n",
      "      And thou shalt live in this fair world behind,\n",
      "      Honour'd, belov'd, and haply one as kind\n",
      "      For husband shalt thou-\n",
      "    Queen. O, confound the rest!\n",
      "      Such love must needs be treason in my breast.\n",
      "      When second husband let me be accurst!\n",
      "      None wed the second but who killed the first.\n"
     ]
    }
   ],
   "source": [
    "# `similarity_search_by_vector()`\n",
    "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64d814-ebe2-4ee9-8a0c-daea7263c077",
   "metadata": {},
   "source": [
    "## 2-11. Retrievers\n",
    "\n",
    "Base interface:\n",
    "\n",
    "1. `langchain_core.retrievers.BaseRetriever`\n",
    "    - `invoke(input, config=None, **kwargs)`: Invokes the retriever to get relevant documents.\n",
    "\n",
    "**Multi-Query**\n",
    "\n",
    "2. `langchain.retrievers.multi_query.MultiQueryRetriever`:\n",
    "   - `from_llm()`\n",
    "3. `langchain.retrievers.multi_query.LineListOutputParser`: Output parser for a list of lines for `MultiQueryRetriever`. \n",
    "\n",
    "The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.\n",
    "\n",
    "Under the hood, MultiQueryRetriever generates queries using a specific prompt. To customize this prompt:\n",
    "\n",
    "- Make a PromptTemplate with an input variable for the question;\n",
    "- Implement an output parser like the one below to split the result into a list of queries.\n",
    "\n",
    "The prompt and output parser together must support the generation of a list of queries.\n",
    "\n",
    "**Contextual Compression**\n",
    "\n",
    "4. `langchain.retrievers.contextual_compression.ContextualCompressionRetriever`: Wraps a base retriever and compresses the results.\n",
    "5. `langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor`\n",
    "\n",
    "**Parent Document**\n",
    "\n",
    "6. `langchain.retrievers.parent_document_retriever.ParentDocumentRetriever`\n",
    "\n",
    "**Ensemble Retrieval**\n",
    "\n",
    "7. `langchain.retrievers.ensemble.EnsembleRetriever`\n",
    "\n",
    "**Self-Query**\n",
    "\n",
    "8. `SelfQueryRetriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "138b4a69-d82a-459b-af49-f22b95388476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x76710ff9d460>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "db\n",
    "# db.get()['documents']\n",
    "# db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad054079-3146-4a23-9753-0efdb30ae31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiQueryRetriever(retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x76710ff9d460>), llm_chain=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7670c33b39b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7670c33b2750>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| LineListOutputParser())"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the LLM to use for query generation\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(), llm=llm\n",
    ")\n",
    "retriever_from_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06463ad3-5c7c-4e04-89af-595b72a8b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies are commonly used for Task Decomposition?', '3. What are the various techniques for breaking down tasks in Task Decomposition?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2707b2da-6756-48d5-807a-8ffc855b05ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The approaches to Task Decomposition are:\\n1. By LLM with simple prompting like \"Steps for XYZ. 1.\", \"What are the subgoals for achieving XYZ?\"\\n2. By using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel\\n3. With human inputs.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm.predict(text=prompt_template.format_prompt(\n",
    "    context=unique_docs,\n",
    "    question=question\n",
    ").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f43d92c-d5cc-4c54-91b1-20919d11a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import LineListOutputParser\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "# Chain\n",
    "llm_chain = prompt_template | llm | output_parser\n",
    "\n",
    "# Other inputs\n",
    "question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80bfeae9-c0b7-452e-83b5-8fa49ca0b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide insights from the course on regression analysis?', '2. How is regression discussed in the course material?', '3. What topics related to regression are covered in the course?', '4. What information does the course offer about regression techniques?', '5. In what way does the course address the topic of regression?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='\\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = MultiQueryRetriever(\n",
    "    retriever=db.as_retriever(), llm_chain=llm_chain\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.invoke(\"What does the course say about regression?\")\n",
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b165c664-a2a1-4920-9455-abe27e7a4ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1307, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4126, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2446, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 7752, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4735, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2413, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4118, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1808, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2816, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 5153, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1735, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2029, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 3485, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2235, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1987, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2632, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2128, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2365, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 6993, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1116, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1560, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1424, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4146, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2788, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 5125, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1315, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2468, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1792, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1898, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1337, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2050, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1414, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2022, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1219, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1206, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1304, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2301, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1220, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4706, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 5622, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1545, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1216, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2588, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2774, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1064, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1647, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2116, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1022, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1261, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1722, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 6213, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1653, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2448, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1546, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1097, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4263, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 4484, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 3786, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 5191, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1469, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 7335, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2169, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"King. Full thirty times hath Phoebus' cart gone round\\n      Neptune's salt wash and Tellus' orbed ground,\\n      And thirty dozed moons with borrowed sheen\\n      About the world have times twelve thirties been,\\n      Since love our hearts, and Hymen did our hands,\\n      Unite comutual in most sacred bands.\\n    Queen. So many journeys may the sun and moon\\n      Make us again count o'er ere love be done!\\n      But woe is me! you are so sick of late,\\n      So far from cheer and from your former state.\\n      That I distrust you. Yet, though I distrust,\\n      Discomfort you, my lord, it nothing must;\\n      For women's fear and love holds quantity,\\n      In neither aught, or in extremity.\\n      Now what my love is, proof hath made you know;\\n      And as my love is siz'd, my fear is so.\\n      Where love is great, the littlest doubts are fear;\\n      Where little fears grow great, great love grows there.\\n    King. Faith, I must leave thee, love, and shortly too;  \\n      My operant powers their functions leave to do.\\n      And thou shalt live in this fair world behind,\\n      Honour'd, belov'd, and haply one as kind\\n      For husband shalt thou-\\n    Queen. O, confound the rest!\\n      Such love must needs be treason in my breast.\\n      When second husband let me be accurst!\\n      None wed the second but who killed the first.\"),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"King. Now must your conscience my acquittance seal,\\n    And You must put me in your heart for friend,\\n    Sith you have heard, and with a knowing ear,\\n    That he which hath your noble father slain\\n    Pursued my life.\\n  Laer. It well appears. But tell me\\n    Why you proceeded not against these feats\\n    So crimeful and so capital in nature,\\n    As by your safety, wisdom, all things else,\\n    You mainly were stirr'd up.\\n  King. O, for two special reasons,\\n    Which may to you, perhaps, seein much unsinew'd,\\n    But yet to me they are strong. The Queen his mother\\n    Lives almost by his looks; and for myself,-\\n    My virtue or my plague, be it either which,-\\n    She's so conjunctive to my life and soul\\n    That, as the star moves not but in his sphere,  \\n    I could not but by her. The other motive\\n    Why to a public count I might not go\\n    Is the great love the general gender bear him,\\n    Who, dipping all his faults in their affection,\\n    Would, like the spring that turneth wood to stone,\\n    Convert his gives to graces; so that my arrows,\\n    Too slightly timber'd for so loud a wind,\\n    Would have reverted to my bow again,\\n    And not where I had aim'd them.\\n  Laer. And so have I a noble father lost;\\n    A sister driven into desp'rate terms,\\n    Whose worth, if praises may go back again,\\n    Stood challenger on mount of all the age\\n    For her perfections. But my revenge will come.\\n  King. Break not your sleeps for that. You must not think\\n    That we are made of stuff so flat and dull\\n    That we can let our beard be shook with danger,\\n    And think it pastime. You shortly shall hear more.\\n    I lov'd your father, and we love ourself,\\n    And that, I hope, will teach you to imagine-\"),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"Ham. To be, or not to be- that is the question:\\n    Whether 'tis nobler in the mind to suffer\\n    The slings and arrows of outrageous fortune\\n    Or to take arms against a sea of troubles,\\n    And by opposing end them. To die- to sleep-\\n    No more; and by a sleep to say we end\\n    The heartache, and the thousand natural shocks\\n    That flesh is heir to. 'Tis a consummation  \\n    Devoutly to be wish'd. To die- to sleep.\\n    To sleep- perchance to dream: ay, there's the rub!\\n    For in that sleep of death what dreams may come\\n    When we have shuffled off this mortal coil,\\n    Must give us pause. There's the respect\\n    That makes calamity of so long life.\\n    For who would bear the whips and scorns of time,\\n    Th' oppressor's wrong, the proud man's contumely,\\n    The pangs of despis'd love, the law's delay,\\n    The insolence of office, and the spurns\\n    That patient merit of th' unworthy takes,\\n    When he himself might his quietus make\\n    With a bare bodkin? Who would these fardels bear,\\n    To grunt and sweat under a weary life,\\n    But that the dread of something after death-\\n    The undiscover'd country, from whose bourn\\n    No traveller returns- puzzles the will,\\n    And makes us rather bear those ills we have\\n    Than fly to others that we know not of?\\n    Thus conscience does make cowards of us all,  \\n    And thus the native hue of resolution\\n    Is sicklied o'er with the pale cast of thought,\\n    And enterprises of great pith and moment\\n    With this regard their currents turn awry\\n    And lose the name of action.- Soft you now!\\n    The fair Ophelia!- Nymph, in thy orisons\\n    Be all my sins rememb'red.\\n  Oph. Good my lord,\\n    How does your honour for this many a day?\\n  Ham. I humbly thank you; well, well, well.\\n  Oph. My lord, I have remembrances of yours\\n    That I have longed long to re-deliver.\\n    I pray you, now receive them.\\n  Ham. No, not I!\\n    I never gave you aught.\\n  Oph. My honour'd lord, you know right well you did,\\n    And with them words of so sweet breath compos'd\\n    As made the things more rich. Their perfume lost,\\n    Take these again; for to the noble mind\\n    Rich gifts wax poor when givers prove unkind.  \\n    There, my lord.\\n  Ham. Ha, ha! Are you honest?\\n  Oph. My lord?\\n  Ham. Are you fair?\\n  Oph. What means your lordship?\\n  Ham. That if you be honest and fair, your honesty should admit no\\n    discourse to your beauty.\\n  Oph. Could beauty, my lord, have better commerce than with honesty?\\n  Ham. Ay, truly; for the power of beauty will sooner transform\\n    honesty from what it is to a bawd than the force of honesty can\\n    translate beauty into his likeness. This was sometime a paradox,\\n    but now the time gives it proof. I did love you once.\\n  Oph. Indeed, my lord, you made me believe so.\\n  Ham. You should not have believ'd me; for virtue cannot so\\n    inoculate our old stock but we shall relish of it. I loved you\\n    not.\\n  Oph. I was the more deceived.\\n  Ham. Get thee to a nunnery! Why wouldst thou be a breeder of\\n    sinners? I am myself indifferent honest, but yet I could accuse\\n    me of such things that it were better my mother had not borne me.  \\n    I am very proud, revengeful, ambitious; with more offences at my\\n    beck than I have thoughts to put them in, imagination to give\\n    them shape, or time to act them in. What should such fellows as I\\n    do, crawling between earth and heaven? We are arrant knaves all;\\n    believe none of us. Go thy ways to a nunnery. Where's your\\n    father?\\n  Oph. At home, my lord.\\n  Ham. Let the doors be shut upon him, that he may play the fool\\n    nowhere but in's own house. Farewell.\\n  Oph. O, help him, you sweet heavens!\\n  Ham. If thou dost marry, I'll give thee this plague for thy dowry:\\n    be thou as chaste as ice, as pure as snow, thou shalt not escape\\n    calumny. Get thee to a nunnery. Go, farewell. Or if thou wilt\\n    needs marry, marry a fool; for wise men know well enough what\\n    monsters you make of them. To a nunnery, go; and quickly too.\\n    Farewell.\\n  Oph. O heavenly powers, restore him!\\n  Ham. I have heard of your paintings too, well enough. God hath\\n    given you one face, and you make yourselves another. You jig, you\\n    amble, and you lisp; you nickname God's creatures and make your  \\n    wantonness your ignorance. Go to, I'll no more on't! it hath made\\n    me mad. I say, we will have no moe marriages. Those that are\\n    married already- all but one- shall live; the rest shall keep as\\n    they are. To a nunnery, go.                            Exit.\\n  Oph. O, what a noble mind is here o'erthrown!\\n    The courtier's, scholar's, soldier's, eye, tongue, sword,\\n    Th' expectancy and rose of the fair state,\\n    The glass of fashion and the mould of form,\\n    Th' observ'd of all observers- quite, quite down!\\n    And I, of ladies most deject and wretched,\\n    That suck'd the honey of his music vows,\\n    Now see that noble and most sovereign reason,\\n    Like sweet bells jangled, out of tune and harsh;\\n    That unmatch'd form and feature of blown youth\\n    Blasted with ecstasy. O, woe is me\\n    T' have seen what I have seen, see what I see!\"),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"God bless you, sir!\\n  Pol. My lord, the Queen would speak with you, and presently.\\n  Ham. Do you see yonder cloud that's almost in shape of a camel?\\n  Pol. By th' mass, and 'tis like a camel indeed.\\n  Ham. Methinks it is like a weasel.\\n  Pol. It is back'd like a weasel.\\n  Ham. Or like a whale.\\n  Pol. Very like a whale.\\n  Ham. Then will I come to my mother by-and-by.- They fool me to the\\n    top of my bent.- I will come by-and-by.\\n  Pol. I will say so.                                      Exit.\\n  Ham. 'By-and-by' is easily said.- Leave me, friends.\\n                                        [Exeunt all but Hamlet.]  \\n    'Tis now the very witching time of night,\\n    When churchyards yawn, and hell itself breathes out\\n    Contagion to this world. Now could I drink hot blood\\n    And do such bitter business as the day\\n    Would quake to look on. Soft! now to my mother!\\n    O heart, lose not thy nature; let not ever\\n    The soul of Nero enter this firm bosom.\\n    Let me be cruel, not unnatural;\\n    I will speak daggers to her, but use none.\\n    My tongue and soul in this be hypocrites-\\n    How in my words somever she be shent,\\n    To give them seals never, my soul, consent!             Exit.\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "documents = TextLoader(\"./datasets/hamlet.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"This is the excellent foppery of the world, that when we are sick in fortune (often the surfeits of our own behavior) we make guilty of our disasters the sun, the moon, and stars, as if we were villains on necessity; fools by heavenly compulsion; knaves, thieves, and treachers by spherical predominance; drunkards, liars, and adulterers by an enforced obedience of planetary influence; and all that we are evil in, by a divine thrusting on. An admirable evasion of whoremaster man, to lay his goatish disposition on the charge of a star! My father compounded with my mother under the Dragon's tail, and my nativity was under Ursa Major, so that it follows I am rough and lecherous. I should have been that I am, had the maidenliest star in the firmament twinkled on my bastardizing.\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b91dce2f-1886-429c-a616-d3999a36f779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './datasets/hamlet.txt'}, page_content='- \"So far from cheer and from your former state.\"\\n- \"That I distrust you.\"\\n- \"Yet, though I distrust, Discomfort you, my lord, it nothing must;\"\\n- \"For women\\'s fear and love holds quantity,\"\\n- \"In neither aught, or in extremity.\"\\n- \"Now what my love is, proof hath made you know;\"\\n- \"And as my love is siz\\'d, my fear is so.\"\\n- \"Where love is great, the littlest doubts are fear;\"\\n- \"Where little fears grow great, great love grows there.\"'),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"- King. O, for two special reasons,\\n    Which may to you, perhaps, seein much unsinew'd,\\n    But yet to me they are strong. The Queen his mother\\n    Lives almost by his looks; and for myself,-\\n    My virtue or my plague, be it either which,-\\n    She's so conjunctive to my life and soul\\n    That, as the star moves not but in his sphere,\"),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content='- \"The courtier\\'s, scholar\\'s, soldier\\'s, eye, tongue, sword,\\n    Th\\' expectancy and rose of the fair state,\\n    The glass of fashion and the mould of form,\\n    Th\\' observ\\'d of all observers- quite, quite down!\"\\n- \"I, of ladies most deject and wretched,\\n    That suck\\'d the honey of his music vows,\"\\n- \"That unmatch\\'d form and feature of blown youth\\n    Blasted with ecstasy.\"'),\n",
       " Document(metadata={'source': './datasets/hamlet.txt'}, page_content=\"- Ham. Then will I come to my mother by-and-by.- They fool me to the\\n    top of my bent.- I will come by-and-by.\\n  Pol. I will say so.                                      Exit.\\n  Ham. 'By-and-by' is easily said.- Leave me, friends.\\n                                        [Exeunt all but Hamlet.]  \\n    'Tis now the very witching time of night,\\n    When churchyards yawn, and hell itself breathes out\\n    Contagion to this world. Now could I drink hot blood\\n    And do such bitter business as the day\\n    Would quake to look on. Soft! now to my mother!\")]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"This is the excellent foppery of the world, that when we are sick in fortune (often the surfeits of our own behavior) we make guilty of our disasters the sun, the moon, and stars, as if we were villains on necessity; fools by heavenly compulsion; knaves, thieves, and treachers by spherical predominance; drunkards, liars, and adulterers by an enforced obedience of planetary influence; and all that we are evil in, by a divine thrusting on. An admirable evasion of whoremaster man, to lay his goatish disposition on the charge of a star! My father compounded with my mother under the Dragon's tail, and my nativity was under Ursa Major, so that it follows I am rough and lecherous. I should have been that I am, had the maidenliest star in the firmament twinkled on my bastardizing.\"\n",
    ")\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489a725-1de2-42d0-b96a-f15dcc1c824b",
   "metadata": {},
   "source": [
    "## 2-12. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba983ead-2478-4bf6-8679-3f6c8b6e4099",
   "metadata": {},
   "source": [
    "## 2-13. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48f1b9-d15a-470c-b656-72b0237f236e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
