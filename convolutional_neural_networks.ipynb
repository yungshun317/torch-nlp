{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6fa10-e701-4e5c-b736-4b591d0040bc",
   "metadata": {},
   "source": [
    "# 1. Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a4398-bb91-4bac-8ab2-cf9d327d92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcfc5b-defc-483b-b9a4-410a64f63b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data matrix\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d8a51-1074-4512-bb1c-1084d893f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels (number IDs) and remove from data\n",
    "labels = data[:,0]\n",
    "data = data[:,1:]\n",
    "\n",
    "print(labels.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7be353-29e1-41fb-a45b-fce583bda80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few random digits\n",
    "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  # pick a random image\n",
    "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
    "\n",
    "  # create the image (must be reshaped!)\n",
    "  img = np.reshape(data[randimg2show,:],(28,28))\n",
    "  ax.imshow(img,cmap='gray')\n",
    "\n",
    "  # title\n",
    "  ax.set_title('The number %i'%labels[randimg2show])\n",
    "\n",
    "plt.suptitle('How humans see the data',fontsize=20)\n",
    "plt.tight_layout(rect=[0,0,1,.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef3563-d4b6-48a2-96f3-b1bea3b40842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few random digits\n",
    "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  # pick a random image\n",
    "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
    "\n",
    "  # create the image\n",
    "  ax.plot(data[randimg2show,:],'ko')\n",
    "\n",
    "  # title\n",
    "  ax.set_title('The number %i'%labels[randimg2show])\n",
    "\n",
    "plt.suptitle('How the FFN model sees the data',fontsize=20)\n",
    "plt.tight_layout(rect=[0,0,1,.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cde77-307c-4e6a-ab1c-cb12bbfc4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some example 7s\n",
    "\n",
    "# find indices of all the 7's in the dataset\n",
    "the7s = np.where(labels==7)[0]\n",
    "\n",
    "# draw the first 12\n",
    "fig,axs = plt.subplots(2,6,figsize=(15,6))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  img = np.reshape(data[the7s[i],:],(28,28))\n",
    "  ax.imshow(img,cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Example 7's\",fontsize=20)\n",
    "plt.tight_layout(rect=[0,0,1,.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62c3e9-ffe1-4777-b951-33cdeab758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how similar are all the 7's? \n",
    "\n",
    "# how many 7's are there?\n",
    "print(data[the7s,:].shape)\n",
    "\n",
    "\n",
    "# let's see how they relate to each other by computing spatial correlations\n",
    "C = np.corrcoef(data[the7s,:])\n",
    "\n",
    "# and visualize\n",
    "fig,ax = plt.subplots(1,3,figsize=(16,6))\n",
    "ax[0].imshow(C,vmin=0,vmax=1)\n",
    "ax[0].set_title(\"Correlation across all 7's\")\n",
    "\n",
    "# extract the unique correlations and show as a scatterplot\n",
    "uniqueCs = np.triu(C,k=1).flatten()\n",
    "ax[1].hist(uniqueCs[uniqueCs!=0],bins=100)\n",
    "ax[1].set_title('All unique correlations')\n",
    "ax[1].set_xlabel(\"Correlations of 7's\")\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "# show all 7's together\n",
    "aveAll7s = np.reshape( np.mean(data[the7s,:],axis=0) ,(28,28))\n",
    "ax[2].imshow(aveAll7s,cmap='gray')\n",
    "ax[2].set_title(\"All 7's averaged together\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c21c3-c3d4-472b-a715-4c91c9229ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].hist(data.flatten(),50)\n",
    "ax[0].set_xlabel('Pixel intensity values')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Histogram of original data')\n",
    "\n",
    "ax[1].hist(dataNorm.flatten(),50)\n",
    "ax[1].set_xlabel('Pixel intensity values')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Histogram of normalized data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b27fcb-aecd-486e-9e5b-cd8f81b424ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( dataNorm ).float()\n",
    "labelsT = torch.tensor( labels ).long() # long = int64\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59059237-3ef9-4872-bd81-018c296113b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTNet():\n",
    "\n",
    "  class mnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,64)\n",
    "      \n",
    "      ### hidden layer\n",
    "      self.fc1 = nn.Linear(64,32)\n",
    "      self.fc2 = nn.Linear(32,32)\n",
    "\n",
    "      ### output layer\n",
    "      self.output = nn.Linear(32,10)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.fc1(x) )\n",
    "      x = F.relu( self.fc2(x) )\n",
    "      return torch.log_softmax( self.output(x),axis=1 )\n",
    "      # NEW HERE: log-softmax the output, because I'm using NLLLoss instead of CrossEntropyLoss\n",
    "  \n",
    "  # create the model instance\n",
    "  net = mnistNet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.NLLLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29213c-41e9-436c-ae1f-a7990e1c3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with one batch\n",
    "net,lossfun,optimizer = createTheMNISTNet()\n",
    "\n",
    "X,y = next(iter(train_loader))\n",
    "yHat = net(X)\n",
    "\n",
    "# values are log-probability of each number (0-9)\n",
    "# print(torch.exp(yHat))\n",
    "\n",
    "# now let's compute the loss\n",
    "loss = lossfun(yHat,y)\n",
    "print(' ')\n",
    "print('Loss:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532892a-d7db-430c-9d09-a842ad094fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 60\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTNet()\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros(numepochs)\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f3abe-6129-40c9-b6d2-6948990984e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc,testAcc,losses,net = function2trainTheModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230285d7-044d-4a79-9a12-73e1e27fdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(trainAcc,label='Train')\n",
    "ax[1].plot(testAcc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886a933-d150-4079-9e6a-0eac4dbd1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model through for the test data\n",
    "X,y = next(iter(test_loader))\n",
    "predictions = net(X).detach()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f32617-c530-4134-98e4-cda01110b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence for all numbers from one sample\n",
    "sample2show = 120\n",
    "\n",
    "plt.bar(range(10),predictions[sample2show]) # try adding exp!\n",
    "plt.xticks(range(10))\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Evidence for that number')\n",
    "plt.title('True number was %s' %y[sample2show].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dea5c5-2cd2-453f-a5f0-1e6d2402bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the errors\n",
    "errors = np.where( torch.max(predictions,axis=1)[1] != y )[0]\n",
    "print(errors)\n",
    "\n",
    "# Evidence for all numbers from one sample\n",
    "sample2show = 10\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
    "\n",
    "ax[0].bar(range(10),np.exp(predictions[errors[sample2show]]))\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[0].set_xlabel('Number')\n",
    "ax[0].set_ylabel('Evidence for that number')\n",
    "ax[0].set_title('True number: %s, model guessed %s' \n",
    "                %( y[errors[sample2show]].item(), torch.argmax(predictions[errors[sample2show]]).item() ))\n",
    "\n",
    "ax[1].imshow( np.reshape(X[errors[sample2show],:],(28,28)) ,cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33808902-9fd8-401b-ae58-81ce8788c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3]\n",
    "# create a class for the model\n",
    "def createTheMNISTNet():\n",
    "\n",
    "  class mnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,64)\n",
    "      \n",
    "      ### hidden layer\n",
    "      self.fc1 = nn.Linear(64,32)\n",
    "      self.fc2 = nn.Linear(32,32)\n",
    "\n",
    "      ### output layer\n",
    "      self.output = nn.Linear(32,10)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.fc1(x) )\n",
    "      x = F.relu( self.fc2(x) )\n",
    "      return self.output(x)\n",
    "  \n",
    "  # create the model instance\n",
    "  net = mnistNet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28806a0e-dc17-4428-9691-bab622468e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### exploring the \"innards\" of the model\n",
    "\n",
    "# create a temp model to explore\n",
    "net = createTheMNISTNet()[0]\n",
    "\n",
    "# summary of the entire model\n",
    "print('Summary of model:')\n",
    "print(net)\n",
    "print(' ')\n",
    "\n",
    "# # explore one of the layers\n",
    "# print('Summary of input layer:')\n",
    "# print( vars(net.input) )\n",
    "# print(' ')\n",
    "\n",
    "# # check out the matrix of weights\n",
    "# print('Input layer weights:')\n",
    "# print( net.input.weight.shape )\n",
    "# print( net.input.weight )\n",
    "# print(' ')\n",
    "\n",
    "# # finally, extract the weights and make a histogram\n",
    "# w = net.input.weight.detach().flatten()\n",
    "# plt.hist(w,40)\n",
    "# plt.xlabel('Weight value')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distribution of initialized input-layer weights')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fcb807-d67e-40bd-af63-6b81ace1b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns a histogram of all weights (across all layers)\n",
    "\n",
    "def weightsHistogram(net):\n",
    "\n",
    "  # initialize weight vector\n",
    "  W = np.array([])\n",
    "  \n",
    "  # concatenate each set of weights\n",
    "  for layer in net.parameters():\n",
    "    W = np.concatenate((W,layer.detach().flatten().numpy() ))\n",
    "\n",
    "  # compute their histogram (note: range is hard-coded)\n",
    "  histy,histx = np.histogram(W,bins=np.linspace(-.8,.8,101),density=True)\n",
    "  histx = (histx[1:]+histx[:-1])/2\n",
    "  return histx,histy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test it!\n",
    "histx,histy = weightsHistogram(net)\n",
    "plt.plot(histx,histy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f90efc-b2d2-4d28-a4d8-b07dd6cb585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 100\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTNet()\n",
    "\n",
    "  # initialize losses and accuracies\n",
    "  losses    = torch.zeros(numepochs)\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "\n",
    "  # initialize histogram variables\n",
    "  histx = np.zeros((numepochs,100))\n",
    "  histy = np.zeros((numepochs,100))\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # get the weights distribution at the start of this epoch\n",
    "    histx,histy[epochi,:] = weightsHistogram(net)\n",
    "  \n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches        = torch.argmax(yHat,axis=1) == y # booleans (false/true)\n",
    "      matchesNumeric = matches.float()                # convert to numbers (0/1)\n",
    "      accuracyPct    = 100*torch.mean(matchesNumeric) # average and x100\n",
    "      batchAcc.append( accuracyPct )                  # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net,histx,histy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d356e73-5222-434b-891c-480e518e755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc,testAcc,losses,net,histx,histy = function2trainTheModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79863250-a63d-4cad-9ac3-d8f0752b6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(trainAcc,label='Train')\n",
    "ax[1].plot(testAcc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6642aa-787b-4137-b649-8201ef4aaeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the histogram of the weights\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "for i in range(histy.shape[0]):\n",
    "  ax[0].plot(histx,histy[i,:],color=[1-i/100,.3,i/100])\n",
    "\n",
    "ax[0].set_title('Histograms of weights')\n",
    "ax[0].set_xlabel('Weight value')\n",
    "ax[0].set_ylabel('Density')\n",
    "\n",
    "\n",
    "ax[1].imshow(histy,vmin=0,vmax=3,\n",
    "             extent=[histx[0],histx[-1],0,99],aspect='auto',origin='lower',cmap='hot')\n",
    "ax[1].set_xlabel('Weight value')\n",
    "ax[1].set_ylabel('Training epoch')\n",
    "ax[1].set_title('Image of weight histograms')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dab669-8258-4549-94c1-842326b48d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] Scrambled MNIST\n",
    "# randomly scramble the data,\n",
    "# preserving the re-ordering for each image\n",
    "eggs = np.random.permutation(data.shape[1])\n",
    "scrambled = dataNorm[:,eggs]\n",
    "\n",
    "\n",
    "# show a few random digits\n",
    "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  # pick a random image\n",
    "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
    "\n",
    "  # create the image (must be reshaped!)\n",
    "  img = np.reshape(scrambled[randimg2show,:],(28,28))\n",
    "  ax.imshow(img,cmap='gray')\n",
    "\n",
    "  # title\n",
    "  ax.set_title('The number %i'%labels[randimg2show])\n",
    "\n",
    "plt.suptitle('The scrambled data',fontsize=20)\n",
    "plt.tight_layout(rect=[0,0,1,.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e45806-efd8-40e3-9323-8c9a3317adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( scrambled ).float()\n",
    "labelsT = torch.tensor( labels ).long()\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ae039-9e89-40ef-bbfd-977ec19b3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc,testAcc,losses,net = function2trainTheModel()\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(trainAcc,label='Train')\n",
    "ax[1].plot(testAcc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd91ba-9dd0-460f-bac3-27557ab7d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Shifted\n",
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( dataNorm ).float()\n",
    "labelsT = torch.tensor( labels ).long()\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d2857-dbf0-45fb-b675-094ca92368bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see how to shift a vectorized image\n",
    "\n",
    "# grab one image data\n",
    "tmp = test_loader.dataset.tensors[0][0,:]\n",
    "tmp = tmp.reshape(28,28) # reshape to 2D image\n",
    "\n",
    "# shift the image (pytorch calls it \"rolling\")\n",
    "tmpS = torch.roll(tmp,8,dims=1)\n",
    "\n",
    "\n",
    "# now show them both\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,6))\n",
    "ax[0].imshow(tmp, cmap='gray')\n",
    "ax[0].set_title('Original')\n",
    "\n",
    "ax[1].imshow(tmpS, cmap='gray')\n",
    "ax[1].set_title('Shifted (rolled)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba545d-0e49-4b1e-bcef-190b15d84619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now repeat for all images in the test set\n",
    "\n",
    "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
    "  \n",
    "  # get the image\n",
    "  img = test_loader.dataset.tensors[0][i,:]\n",
    "  \n",
    "  # reshape and roll by max. 10 pixels\n",
    "  randroll = np.random.randint(-10,11)\n",
    "  img = torch.roll( img.reshape(28,28) ,randroll,dims=1 )\n",
    "\n",
    "  # re-vectorize and put back into the matrix\n",
    "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n",
    "\n",
    "\n",
    "# Note: now run the previous cell again to confirm the shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d3906-b404-4f89-905d-d8d145f3cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc,testAcc,losses,net = function2trainTheModel()\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(trainAcc,label='Train')\n",
    "ax[1].plot(testAcc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a099b60-a2d7-46d2-9d66-8ad10d01c56a",
   "metadata": {},
   "source": [
    "# 2. Convolution\n",
    "## 2-1. Convolution in SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9eafa-2fe4-46b1-9b92-3b5a4419c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "imgN = 20\n",
    "image = np.random.randn(imgN,imgN)\n",
    "\n",
    "# convolution kernel\n",
    "kernelN = 7\n",
    "Y,X = np.meshgrid(np.linspace(-3,3,kernelN),np.linspace(-3,3,kernelN))\n",
    "kernel = np.exp( -(X**2+Y**2)/7 )\n",
    "\n",
    "\n",
    "# let's see what they look like\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,6))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "ax[1].imshow(kernel)\n",
    "ax[1].set_title('Convolution kernel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02943d-0683-484c-a261-f1a0595e6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the convolution\n",
    "convoutput = np.zeros((imgN,imgN))\n",
    "halfKr = kernelN//2\n",
    "\n",
    "for rowi in range(halfKr,imgN-halfKr):\n",
    "  for coli in range(halfKr,imgN-halfKr):\n",
    "\n",
    "    # cut out a piece of the image\n",
    "    pieceOfImg = image[rowi-halfKr:rowi+halfKr+1,:]      # get the rows\n",
    "    pieceOfImg = pieceOfImg[:,coli-halfKr:coli+halfKr+1] # extract the columns\n",
    "\n",
    "    # dot product: element-wise multiply and sum (and flip the kernel for \"real convolution\")\n",
    "    dotprod = np.sum( pieceOfImg*kernel[::-1,::-1] )\n",
    "\n",
    "    # store the result for this pixel\n",
    "    convoutput[rowi,coli] = dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5996ecf-cedd-42b2-b8d1-3da8f6dd7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scipy\n",
    "convoutput2 = convolve2d(image,kernel,mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbdff6-9abc-4a35-82c9-13454a44a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "ax[0,0].imshow(image)\n",
    "ax[0,0].set_title('Image')\n",
    "\n",
    "ax[0,1].imshow(kernel)\n",
    "ax[0,1].set_title('Convolution kernel')\n",
    "\n",
    "ax[1,0].imshow(convoutput)\n",
    "ax[1,0].set_title('Manual convolution')\n",
    "\n",
    "ax[1,1].imshow(convoutput2)\n",
    "ax[1,1].set_title(\"Scipy's convolution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c3719-26e3-460d-800e-1316d1272069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] Convolution with different kernels in a real image\n",
    "# read a pic from the web\n",
    "bathtub = imread('https://upload.wikimedia.org/wikipedia/commons/6/61/De_nieuwe_vleugel_van_het_Stedelijk_Museum_Amsterdam.jpg')\n",
    "\n",
    "# check the size\n",
    "print(bathtub.shape)\n",
    "\n",
    "# let's see what the famous Bathtub Museum looks like\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(bathtub);\n",
    "\n",
    "# transform image to 2D for convenience (not necessary for convolution!)\n",
    "bathtub = np.mean(bathtub,axis=2)\n",
    "bathtub = bathtub/np.max(bathtub)\n",
    "\n",
    "# check the size again\n",
    "print(bathtub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473f3d4-636e-41e3-a086-96ac71f507b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-craft two convolution kernels\n",
    "\n",
    "# vertical kernel\n",
    "VK = np.array([ [1,0,-1],\n",
    "                [1,0,-1],\n",
    "                [1,0,-1] ])\n",
    "\n",
    "# horizontal kernel\n",
    "HK = np.array([ [ 1, 1, 1],\n",
    "                [ 0, 0, 0],\n",
    "                [-1,-1,-1] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ddab6-e19c-4c77-a2ac-3627b4b2bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(16,8))\n",
    "\n",
    "ax[0,0].imshow(VK)\n",
    "ax[0,0].set_title('Vertical kernel')\n",
    "\n",
    "ax[0,1].imshow(HK)\n",
    "ax[0,1].set_title('Horizontal kernel')\n",
    "\n",
    "\n",
    "# run convolution and show the result\n",
    "convres = convolve2d(bathtub,VK,mode='same')\n",
    "ax[1,0].imshow(convres,cmap='gray',vmin=0,vmax=.01)\n",
    "\n",
    "convres = convolve2d(bathtub,HK,mode='same')\n",
    "ax[1,1].imshow(convres,cmap='gray',vmin=0,vmax=.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600261c2-8482-4dea-97fa-397ae986b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] PyTorch\n",
    "# first, translate everything into a tensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "VK_t = torch.tensor(VK).view(1,1,3,3).double()\n",
    "HK_t = torch.tensor(HK).view(1,1,3,3).double()\n",
    "bathtub_t = torch.tensor(bathtub).view(1,1,bathtub.shape[0],bathtub.shape[1])\n",
    "\n",
    "print(VK_t.shape)\n",
    "print(bathtub_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f8ae6-e431-49d6-ad18-dd24d05a5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = F.conv2d(bathtub_t,VK_t)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60e6b9-e031-4ef2-aa63-30917ac52eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(16,8))\n",
    "\n",
    "ax[0,0].imshow(VK)\n",
    "ax[0,0].set_title('Vertical kernel')\n",
    "\n",
    "ax[0,1].imshow(HK)\n",
    "ax[0,1].set_title('Horizontal kernel')\n",
    "\n",
    "\n",
    "# run convolution and show the result\n",
    "convres = F.conv2d(bathtub_t,VK_t)\n",
    "img = torch.squeeze(convres.detach())\n",
    "ax[1,0].imshow(img,cmap='gray',vmin=0,vmax=.01)\n",
    "\n",
    "convres = F.conv2d(bathtub_t,HK_t)\n",
    "img = torch.squeeze(convres.detach())\n",
    "ax[1,1].imshow(img,cmap='gray',vmin=0,vmax=.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0482d-1291-4ef7-a718-0c1cb881a5df",
   "metadata": {},
   "source": [
    "# 2-2. Convolution in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e6a1a-fdbb-4b09-bf77-4cb844107e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conv2d class instance with parameters\n",
    "\n",
    "# parameters\n",
    "inChans  = 3 # RGB\n",
    "outChans = 15\n",
    "krnSize  = 5 # should be an odd number\n",
    "stride   = 1\n",
    "padding  = 0\n",
    "\n",
    "# create the instance (cf nn.Linear)\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# let's have a look at it\n",
    "print(c)\n",
    "print(' ')\n",
    "\n",
    "# check out its weight tensor; what are the dimensions?\n",
    "print( 'Size of weights: ' + str(c.weight.shape) )\n",
    "print( 'Size of bias: ' + str(c.bias.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a93471-32fc-472a-9455-d65ff8deaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do these kernels look like?\n",
    "\n",
    "fig,axs = plt.subplots(3,5,figsize=(10,5))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(torch.squeeze(c.weight[i,0,:,:]).detach(),cmap='Purples')\n",
    "  ax.set_title('L1(0)->L2(%s)'%i)\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ce1c9-9c19-4945-8662-eabdbc0108c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the image (N, RGB, height, width)\n",
    "imsize = (1,3,64,64)\n",
    "\n",
    "img = torch.rand(imsize)\n",
    "\n",
    "# pytorch wants channels first, but matplotlib wants channels last.\n",
    "# therefore, tensors must be permuted to visualize\n",
    "img2view = img.permute(2,3,1,0).numpy()\n",
    "print(img.shape)\n",
    "print(img2view.shape)\n",
    "\n",
    "plt.imshow(np.squeeze(img2view));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08855c43-3220-4e79-871f-47b375f78173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve the image with the filter bank (set of 'outChans' kernels)\n",
    "convRes = c(img)\n",
    "\n",
    "print(img.shape)\n",
    "print(convRes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc89c1-e033-406c-b3ab-bf833e629fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the convolved images look like? (Hint: think of the bathtub picture.)\n",
    "\n",
    "fig,axs = plt.subplots(3,5,figsize=(10,5))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract this \"layer\" of the convolution result\n",
    "  I = torch.squeeze(convRes[0,i,:,:]).detach()\n",
    "\n",
    "  # and visualize it\n",
    "  ax.imshow(I,cmap='Purples')\n",
    "  ax.set_title('Conv. w/ filter %s'%i)\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890059ce-5734-431a-8d39-b3ed12deb52f",
   "metadata": {},
   "source": [
    "# 2-3. Transpose Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad90456-3a68-4d8e-bd1a-2d09f7ecf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ConvTranspose2d class instance with parameters\n",
    "\n",
    "# parameters\n",
    "inChans  = 3 # RGB\n",
    "outChans = 15\n",
    "krnSize  = 5 # should be an odd number\n",
    "stride   = 1\n",
    "padding  = 0\n",
    "\n",
    "# create the instance\n",
    "c = nn.ConvTranspose2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# let's have a look at it\n",
    "print(c)\n",
    "print(' ')\n",
    "\n",
    "# check out its weight tensor; what are the dimensions?\n",
    "print( 'Size of weights: ' + str(c.weight.shape) )\n",
    "print( 'Size of bias: ' + str(c.bias.shape) )\n",
    "\n",
    "# tip: Compare the sizes of these weights with those of \"forward\" convolution (DUDL_convolution_conv2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88309d-5b31-409e-b140-e19fbe41efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do these kernels look like?\n",
    "\n",
    "fig,axs = plt.subplots(3,5,figsize=(10,5))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(torch.squeeze(c.weight[0,i,:,:]).detach(),cmap='Purples')\n",
    "  ax.set_title('L1(0)->L2(%s)'%i)\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b59e09-26a7-443c-bfd8-995f01d8058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the image (N, RGB, width, height)\n",
    "imsize = (1,3,64,64)\n",
    "\n",
    "img = torch.rand(imsize)\n",
    "\n",
    "# pytorch wants channels first, but matplotlib wants channels last.\n",
    "# therefore, tensors must be permuted to visualize\n",
    "img2view = img.permute(2,3,1,0).numpy()\n",
    "print(img.shape)\n",
    "print(img2view.shape)\n",
    "\n",
    "plt.imshow(np.squeeze(img2view));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828f0a4-4034-4475-9dc9-0d1d93a17366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve the image with the filter bank (set of 'outChans' kernels)\n",
    "convRes = c(img)\n",
    "\n",
    "print(img.shape)\n",
    "print(convRes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5fdb4-b4df-4582-bf80-3fbbcc59bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the convolved images look like? (Hint: think of the bathtub picture.)\n",
    "\n",
    "fig,axs = plt.subplots(3,5,figsize=(10,5))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract this \"layer\" of the convolution result\n",
    "  I = torch.squeeze(convRes[0,i,:,:]).detach()\n",
    "\n",
    "  # and visualize it\n",
    "  ax.imshow(I,cmap='Purples')\n",
    "  ax.set_title('Conv. w/ filter %s'%i)\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af970583-98b0-4b3c-90b1-ebac8472a597",
   "metadata": {},
   "source": [
    "# 3. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055574e-8769-4bca-b45d-204da8eb28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pool class instance with parameters\n",
    "\n",
    "# parameters\n",
    "poolSize = 3\n",
    "stride   = 3\n",
    "\n",
    "# create the instance\n",
    "p2 = nn.MaxPool2d(poolSize,stride=3)\n",
    "p3 = nn.MaxPool3d(poolSize,stride=3)\n",
    "\n",
    "# let's have a look at them\n",
    "print(p2)\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b1454-7d6c-43f9-b633-8e5e3a55730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image and apply maxpooling\n",
    "\n",
    "# create a 2D and a 3D image\n",
    "img2 = torch.randn(1,1,30,30)\n",
    "img3 = torch.randn(1,3,30,30)\n",
    "\n",
    "\n",
    "# all combinations of image and maxpool dimensionality\n",
    "img2Pool2 = p2(img2)\n",
    "print(f'2D image, 2D maxpool: {img2Pool2.shape}\\n' )\n",
    "\n",
    "# img2Pool3 = p3(img2)\n",
    "# print(f'2D image, 3D maxpool: {img2Pool3.shape}\\n' )\n",
    "\n",
    "img3Pool2 = p2(img3)\n",
    "print(f'3D image, 2D maxpool: {img3Pool2.shape}\\n' )\n",
    "\n",
    "img3Pool3 = p3(img3)\n",
    "print(f'3D image, 3D maxpool: {img3Pool3.shape}\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b548f83-d221-4557-9461-a5cd6aefb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "littlenet = nn.Sequential(\n",
    "    \n",
    "    ## the conv-pool block\n",
    "    nn.Conv2d(3,10,5,3,2), # convolution layer\n",
    "    nn.ReLU(),             # activation function\n",
    "    nn.AvgPool3d(3,3),     # average-pool\n",
    "    \n",
    "    ## the FFN block\n",
    "    nn.Flatten(),          # vectorize to get from image to linear\n",
    "    nn.Linear(588,1),      # FC linear layer\n",
    "    nn.Sigmoid()           # output activation\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e066410-2d43-4569-9980-1564a419b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with a bit of data\n",
    "img = torch.rand(1,3,128,128)\n",
    "littlenet(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f207817-be06-4513-880e-dd47471ac115",
   "metadata": {},
   "source": [
    "# 4. Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847569f8-57e6-45ac-acd7-76b9c95e2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the CIFAR10 dataset\n",
    "cdata = torchvision.datasets.CIFAR10(root='cifar10', download=True)\n",
    "\n",
    "print(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df243d-24ab-4fe8-a81f-e78d96cbf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the shape of the dataset\n",
    "print( cdata.data.shape )\n",
    "\n",
    "# the unique categories\n",
    "print( cdata.classes )\n",
    "\n",
    "# .targets is a list of targets converted to ints\n",
    "print( len(cdata.targets) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c286e4f-a93d-4b10-a75d-bffa9a59ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a few random images\n",
    "\n",
    "fig,axs = plt.subplots(5,5,figsize=(10,10))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "\n",
    "  # select a random picture\n",
    "  randidx = np.random.choice(len(cdata.targets))\n",
    "\n",
    "  # extract that image\n",
    "  pic = cdata.data[randidx,:,:,:]\n",
    "  # and its label\n",
    "  label = cdata.classes[cdata.targets[randidx]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic)\n",
    "  ax.text(16,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bdd95-068d-42ea-bb2a-377323e7433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = T.Compose([ T.ToTensor(),\n",
    "                 T.Resize(32*4),\n",
    "                 T.Grayscale(num_output_channels=1)  ])\n",
    "\n",
    "# include the transform in the dataset\n",
    "cdata.transform = Ts\n",
    "\n",
    "# you can also apply the transforms immediately when loading in the data\n",
    "# cdata = torchvision.datasets.CIFAR10(root='cifar10', download=True, transform=Ts)\n",
    "\n",
    "\n",
    "# Important! Adding a transform doesn't change the image data:\n",
    "print(cdata.data[123,:,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051534a-da7d-42b2-ad9c-dd445a470632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the transform\n",
    "\n",
    "# option 1a: apply the transform \"externally\" to an image\n",
    "img1 = Ts( cdata.data[123,:,:,:] )\n",
    "\n",
    "# option 1b: use the embedded transform\n",
    "img2 = cdata.transform( cdata.data[123,:,:,:] )\n",
    "\n",
    "# let's see what we've done!\n",
    "fig,ax = plt.subplots(1,3,figsize=(10,3))\n",
    "ax[0].imshow(cdata.data[123,:,:,:])\n",
    "ax[1].imshow(torch.squeeze(img1))\n",
    "ax[2].imshow(torch.squeeze(img2),cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de15c61-fa0f-45cf-b7db-9843bccfff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note about ToTensor() and normalization:\n",
    "??T.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554b61a-0f6e-480f-ba4f-c5d6c6534727",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks (CNNs)\n",
    "## 5-1. Image Classification with LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9c18fb-d73c-4bd2-831b-e3df065207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6bd9c9-8ef2-49a2-8bce-9b1342bc655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Pixel Value: 0 \n",
      "Max Pixel Value: 255\n",
      "Mean Pixel Value 33.31842041015625 \n",
      "Pixel Values Std: 78.56748962402344\n",
      "Scaled Mean Pixel Value 0.13066047430038452 \n",
      "Scaled Pixel Values Std: 0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', train=True, download=True)\n",
    "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(train_dataset.data.min(), train_dataset.data.max()))\n",
    "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(train_dataset.data.float().mean(), train_dataset.data.float().std()))\n",
    "print('Scaled Mean Pixel Value {} \\nScaled Pixel Values Std: {}'.format(train_dataset.data.float().mean() / 255, train_dataset.data.float().std() / 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a0e7af-5bc1-4d21-a777-51170fa044a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Pixel Value: 0 \n",
      "Max Pixel Value: 255\n",
      "Mean Pixel Value 33.791221618652344 \n",
      "Pixel Values Std: 79.17247009277344\n",
      "Scaled Mean Pixel Value 0.1325145959854126 \n",
      "Scaled Pixel Values Std: 0.3104802668094635\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='.', train=False, download=True)\n",
    "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(test_dataset.data.min(), test_dataset.data.max()))\n",
    "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(test_dataset.data.float().mean(), test_dataset.data.float().std()))\n",
    "print('Scaled Mean Pixel Value {} \\nScaled Pixel Values Std: {}'.format(test_dataset.data.float().mean() / 255, test_dataset.data.float().std() / 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a935a5-5147-4553-84ee-e7e05d2da9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = torchvision.transforms.Compose([\n",
    "                                                  torchvision.transforms.Resize((32,32)),\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = torchvision.transforms.Compose([\n",
    "                                                  torchvision.transforms.Resize((32,32)),\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6dc8bec-0f2f-4be2-9aa0-c7907db49392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1d9f80-73bf-4b79-acf0-47b270aba109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02edf854-9d97-42ae-8200-8dbab7b22f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "K = len(set(train_dataset.targets.numpy()))\n",
    "print(\"number of classes:\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9478c522-7783-405f-b219-f8d078cbdb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_layers): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        # http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n",
    "        # \"No zero padding, non-unit strides\"\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, K)\n",
    "        )\n",
    "  \n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense_layers(out)\n",
    "        return out\n",
    "\n",
    "model = LeNet5(num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44dbc429-5ff1-4d31-95a1-e6f41f3afb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a loss function for multi-class classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an optimizer for multi-class classification\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f8c0ff-5064-4fe5-a400-a27540e19fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [400/938], Loss: 0.0209\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0154\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0931\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0954\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0467\n",
      "Epoch [3/10], Step [800/938], Loss: 0.1767\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0360\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0144\n",
      "Epoch [5/10], Step [400/938], Loss: 0.1090\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0107\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0227\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0312\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0247\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0248\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0679\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0147\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0694\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0181\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0727\n"
     ]
    }
   ],
   "source": [
    "# Loop through data\n",
    "for epoch in range(num_epochs):\n",
    "  # Training\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(images)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "\n",
    "      # Backward & optimize\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (i + 1) % 400 == 0:\n",
    "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d5beca-1904-44bf-bef3-c494ab9602d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.56 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a512193-998a-41f1-8a16-5de2368a3cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
