{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880ff17a-cf78-46b0-aca5-43cd45f9bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38757e-bf69-47a7-8182-a2006c970080",
   "metadata": {},
   "source": [
    "# 1. Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfde8c-dd1b-417d-b616-3be3f234cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "    super().__init__()\n",
    "\n",
    "    # Assume d_v = d_k\n",
    "    self.d_k = d_k\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "    # final linear layer\n",
    "    self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "    # causal mask\n",
    "    # make it so that diagonal is 0 too\n",
    "    # this way we don't have to shift the inputs to make targets\n",
    "    self.causal = causal\n",
    "    if causal:\n",
    "      cm = torch.tril(torch.ones(max_len, max_len))\n",
    "      self.register_buffer(\n",
    "          \"causal_mask\",\n",
    "          cm.view(1, 1, max_len, max_len)\n",
    "      )\n",
    "\n",
    "  def forward(self, q, k, v, pad_mask=None):\n",
    "    q = self.query(q) # N x T x (hd_k)\n",
    "    k = self.key(k)   # N x T x (hd_k)\n",
    "    v = self.value(v) # N x T x (hd_v)\n",
    "\n",
    "    N = q.shape[0]\n",
    "    T_output = q.shape[1]\n",
    "    T_input = k.shape[1]\n",
    "\n",
    "    # change the shape to:\n",
    "    # (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "    # in order for matrix multiply to work properly\n",
    "    q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    # compute attention weights\n",
    "    # (N, h, T, d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
    "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "    if pad_mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          pad_mask[:, None, None, :] == 0, float('-inf'))\n",
    "    if self.causal:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          self.causal_mask[:, :, :T_output, :T_input] == 0, float('-inf'))\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # compute attention-weighted values\n",
    "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
    "    A = attn_weights @ v\n",
    "\n",
    "    # reshape it back before final linear layer\n",
    "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
    "    A = A.contiguous().view(N, T_output, self.d_k * self.n_heads) # (N, T, h*d_k)\n",
    "\n",
    "    # projection\n",
    "    return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd4579-97be-49a5-bccf-b94f773ea7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "    x = self.ln2(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660bf26-dc2c-4e59-96d3-0c45fb660ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.ln3 = nn.LayerNorm(d_model)\n",
    "    self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "    self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    # self-attention on decoder input\n",
    "    x = self.ln1(\n",
    "        dec_input + self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
    "\n",
    "    # multi-head attention including encoder output\n",
    "    x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
    "\n",
    "    x = self.ln3(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb268c55-2eb9-44d0-9d0a-f3c28c6b600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    exp_term = torch.arange(0, d_model, 2)\n",
    "    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(1, max_len, d_model)\n",
    "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x.shape: N x T x D\n",
    "    x = x + self.pe[:, :x.size(1), :]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ed3fc-8879-439b-836d-1cac5f81154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "              #  n_classes,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        EncoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    # self.fc = nn.Linear(d_model, n_classes)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(x, pad_mask)\n",
    "\n",
    "    # many-to-one (x has the shape N x T x D)\n",
    "    # x = x[:, 0, :]\n",
    "\n",
    "    x = self.ln(x)\n",
    "    # x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039a219-45d7-40c3-a81f-650fddffad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        DecoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    self.fc = nn.Linear(d_model, vocab_size)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    x = self.embedding(dec_input)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(enc_output, x, enc_mask, dec_mask)\n",
    "    x = self.ln(x)\n",
    "    x = self.fc(x) # many-to-many\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d42abc-e76d-499e-93f5-2335565b1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "  \n",
    "  def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "    enc_output = self.encoder(enc_input, enc_mask)\n",
    "    dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
    "    return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d94af-5082-47cb-98fa-b542f9820349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "encoder = Encoder(vocab_size=20_000,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=10_000,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1761e9c-2006-4631-90b6-dc3850c1b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079bdfb-1844-4f8d-b170-a3a2f996afcb",
   "metadata": {},
   "source": [
    "## 1-1. Transformers\n",
    "\n",
    "- [Supported Models & Frameworks](https://huggingface.co/docs/transformers/index#supported-models-and-frameworks)\n",
    "### 1-1-1. Pipelines\n",
    "- [Complete List of Supported Tasks](https://huggingface.co/docs/transformers/v4.44.0/en/main_classes/pipelines#transformers.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f7b534-dcce-49bb-a9e4-e77cd229bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tf-keras \n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52cce711-675c-4f08-92dc-94a6174d6a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f4a1dbf2b14680a392b903fe58172c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f5694e3f69404fa5ab98663f6efc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb85ba1a9034aa5bdbe00a54d6211aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_classification.TextClassificationPipeline"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "type(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90fce79-03b0-4566-a71e-0501beb0d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998759031295776}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is a dictionary\n",
    "classifier(\"This is such a great movie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ce99b-4718-4fa5-be2b-cee251d7e158",
   "metadata": {},
   "source": [
    "### 1-1-2. Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1f8b4f-369d-4ca7-af24-5cbf68708be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292480eb-6d72-41d1-ba77-c2cc5a7b4377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13e40586bb44aba9bf4a3fa67469a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84bf07de1774c06b385611cd277728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9309390dac454bcc953ec69d3d171ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7530e8bcb57f41359357d6cd0f96977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f7b3a3bba64b9e8b8a88a206b563b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28d6f2199864ffe90be76f8c262b14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f358d052bfdf46408bdd70b0b8e1306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5486156e-fe89-4804-87db-d8da07f370bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25f1878-bee8-4048-953c-d59f84291b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "idx: int32\n",
       "----\n",
       "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
       "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
       "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4558f922-6ad9-465f-bd7c-2e305584f503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8cf0a4-39bf-4ab7-9840-bfc085c68808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d9bc28cf64d9b9b16e4a089079423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22131436804f63b052dec0264053b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16dbce17f844673a53dc12abd43ec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c999c9cd39bb4411934371883556a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'])\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d515f6b-3769-43e6-916f-fca294f7a22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bde47a18bc4a91b372330e7c17925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f414a499ef40440f8c58716f4b50b467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0ea13c27744a31a5ede242980c28f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "type(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95f2797-056f-43bc-9514-8248d4ff314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=my_trainer/runs/Aug12_22-42-33_yungshun317-Titan-18-HX-A14VIG,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=my_trainer,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=my_trainer,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.EPOCH,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  'my_trainer',\n",
    "  eval_strategy='epoch',\n",
    "  save_strategy='epoch',\n",
    "  num_train_epochs=1,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e6cae1-8ecd-4efa-8e18-233b580ef77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54dd42b8b634e5e84eb7c42ebad2d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb643e8-9dc2-4091-9495-7e4d86a67b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f728925-aff4-4841-bfe0-b92b30e8901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffde041c-e5f6-4183-91bd-37e47ecc0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install evaluate\n",
    "import evaluate\n",
    "\n",
    "# Metric for validation error\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c222023-bf0d-48b1-a846-349f0cfae16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 03:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.509155</td>\n",
       "      <td>0.883028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.07386701579104694, metrics={'train_runtime': 194.6434, 'train_samples_per_second': 346.012, 'train_steps_per_second': 43.253, 'total_flos': 517212489917652.0, 'train_loss': 0.07386701579104694, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "063f5e9a-9529-4dbc-b21e-9baa13ff53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('models/finetuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38cb7889-f9f1-4ee3-96c3-ffb08eda8aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9997863173484802}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = pipeline(\"sentiment-analysis\", model='models/finetuned_model', device=0)\n",
    "finetuned_model('This is such a great movie!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "292f822a-1fed-46a4-8fd4-de1ad309779c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9997292160987854}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model('This is such a bad movie!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d55f06db-45cf-4029-afe3-c7408fd4c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat models/finetuned_model/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddde7e38-f876-4bc6-8af4-37dedb9b5c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.988}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "n_samples = 500\n",
    "\n",
    "# The labels in test set of `glue/sst2` are `-1`\n",
    "X = raw_datasets['train'].data[\"sentence\"].to_pylist()[:n_samples]\n",
    "y = raw_datasets['train'].data[\"label\"].to_pylist()[:n_samples]\n",
    "\n",
    "results = finetuned_model(X)\n",
    "predictions = [0 if res[\"label\"] == \"LABEL_0\" else 1 for res in results]\n",
    "print(metric.compute(predictions=predictions, references=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847dd8c-5032-4218-a459-5701f60145c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
