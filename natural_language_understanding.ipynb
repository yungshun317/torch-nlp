{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97425852-84d3-45dc-a789-9f727d15d792",
   "metadata": {},
   "source": [
    "# 1. Hidden Markov Models (HMMs)\n",
    "## 1-1. Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a5526-4d29-46ab-93f9-b73cf381086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fefa3b-c585-4a8a-bf68-b1d9cec25bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  'edgar_allan_poe.txt',\n",
    "  'robert_frost.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e201025b-3e3e-4ee1-b99f-01773ec2da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LO! Death hath rear'd himself a throne\n",
      "In a strange city, all alone,\n",
      "Far down within the dim west\n",
      "Where the good, and the bad, and the worst, and the best,\n",
      "Have gone to their eternal rest.\n",
      "â€‰\n",
      "There shrines, and palaces, and towers\n",
      "Are not like any thing of ours\n",
      "Oh no! O no! ours never loom\n",
      "To heaven with that ungodly gloom!\n"
     ]
    }
   ],
   "source": [
    "!head datasets/edgar_allan_poe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c03049-b8e1-40b0-bd6a-3800dee7dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth; \n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim\n",
      "Because it was grassy and wanted wear,\n",
      "Though as for that the passing there\n"
     ]
    }
   ],
   "source": [
    "!head datasets/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a118c-a791-4f05-9665-92f1626e5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into lists\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for label, f in enumerate(input_files):\n",
    "  print(f\"{f} corresponds to label {label}\")\n",
    "\n",
    "  for line in open(f):\n",
    "    line = line.rstrip().lower()\n",
    "    if line:\n",
    "      # remove punctuation\n",
    "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "      input_texts.append(line)\n",
    "      labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174db1cc-91b5-4e5c-937b-177a25eb79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9e552-8754-42a3-bdac-7abc47c38bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Ytrain), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e989d-2acc-4835-9f42-a2115bbe3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661e55d-4f8a-4503-bd53-dbca4d70ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bec53b-0173-412c-b202-62c532e875c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "word2idx = {'<unk>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b04ee1-c2ad-4527-a8ed-275acdf0f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate word2idx\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "      if token not in word2idx:\n",
    "        word2idx[token] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fd36d-cac2-43b6-88ac-39efcba25c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938261a8-d567-4df9-9aee-0a3085846ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafdd3e-4554-4577-82be-98b8288591f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into integer format\n",
    "train_text_int = []\n",
    "test_text_int = []\n",
    "\n",
    "for text in train_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx[token] for token in tokens]\n",
    "  train_text_int.append(line_as_int)\n",
    "\n",
    "for text in test_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx.get(token, 0) for token in tokens]\n",
    "  test_text_int.append(line_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36aa5b7-f218-45e8-a5d6-eacedccbde88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_int[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91c76d-8a47-4556-84a4-38d51f0bb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize A and pi matrices - for both classes\n",
    "V = len(word2idx)\n",
    "\n",
    "A0 = np.ones((V, V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V, V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec03039-8057-4b8e-8cec-b1e2d0445672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute counts for A and pi\n",
    "def compute_counts(text_as_int, A, pi):\n",
    "  for tokens in text_as_int:\n",
    "    last_idx = None\n",
    "    for idx in tokens:\n",
    "      if last_idx is None:\n",
    "        # it's the first word in a sentence\n",
    "        pi[idx] += 1\n",
    "      else:\n",
    "        # the last word exists, so count a transition\n",
    "        A[last_idx, idx] += 1\n",
    "\n",
    "      # update last idx\n",
    "      last_idx = idx\n",
    "\n",
    "\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 0], A0, pi0)\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 1], A1, pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161f5ca-d0f9-4495-a46b-f51c0296ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize A and pi so they are valid probability matrices\n",
    "# convince yourself that this is equivalent to the formulas shown before\n",
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a796db3-7c39-4e3d-be4e-9ca01d72d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log A and pi since we don't need the actual probs\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c81d25-0d27-42b0-b6a2-17f4e9dd8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute priors\n",
    "count0 = sum(y == 0 for y in Ytrain)\n",
    "count1 = sum(y == 1 for y in Ytrain)\n",
    "total = len(Ytrain)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "p0, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae8748-c3d4-47c6-948d-eb5c74347c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "class Classifier:\n",
    "  def __init__(self, logAs, logpis, logpriors):\n",
    "    self.logAs = logAs\n",
    "    self.logpis = logpis\n",
    "    self.logpriors = logpriors\n",
    "    self.K = len(logpriors) # number of classes\n",
    "\n",
    "  def _compute_log_likelihood(self, input_, class_):\n",
    "    logA = self.logAs[class_]\n",
    "    logpi = self.logpis[class_]\n",
    "\n",
    "    last_idx = None\n",
    "    logprob = 0\n",
    "    for idx in input_:\n",
    "      if last_idx is None:\n",
    "        # it's the first token\n",
    "        logprob += logpi[idx]\n",
    "      else:\n",
    "        logprob += logA[last_idx, idx]\n",
    "      \n",
    "      # update last_idx\n",
    "      last_idx = idx\n",
    "    \n",
    "    return logprob\n",
    "  \n",
    "  def predict(self, inputs):\n",
    "    predictions = np.zeros(len(inputs))\n",
    "    for i, input_ in enumerate(inputs):\n",
    "      posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] \\\n",
    "             for c in range(self.K)]\n",
    "      pred = np.argmax(posteriors)\n",
    "      predictions[i] = pred\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc0eb1-798c-4000-8cdd-3154047a7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1], [logpi0, logpi1], [logp0, logp1])\n",
    "\n",
    "Ptrain = clf.predict(train_text_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain == Ytrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c87fa-c64b-4ae4-884d-dae2a3b35d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptest = clf.predict(test_text_int)\n",
    "print(f\"Test acc: {np.mean(Ptest == Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f407b85-7e72-4274-8eca-ff1cd2d7f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm = confusion_matrix(Ytrain, Ptrain)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d715d2-9f7a-4d79-97dc-fbac5beb36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(Ytest, Ptest)\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df91a9-c3e7-4706-af62-9c1f8909596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytrain, Ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34bcec-3302-484e-a4e3-ee032a4aa829",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytest, Ptest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb084b05-74ae-4a40-b831-3799f9a3fc9d",
   "metadata": {},
   "source": [
    "## 1-2. Poetry Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea87f67-82cb-4736-a1f5-cb13e5678724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c9f1c-c5b6-4970-be81-30d6aaa0a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} # start of a phrase\n",
    "first_order = {} # second word only\n",
    "second_order = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcedcd8-b4f2-4ea0-a9ce-fd9bbc8570bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a0f22-be46-4cd5-bbea-bc3222b9840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "  if k not in d:\n",
    "    d[k] = []\n",
    "  d[k].append(v)\n",
    "\n",
    "# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e403b3-4afe-419e-b00e-be40c661bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('robert_frost.txt'):\n",
    "  tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "  T = len(tokens)\n",
    "  for i in range(T):\n",
    "    t = tokens[i]\n",
    "    if i == 0:\n",
    "      # measure the distribution of the first word\n",
    "      initial[t] = initial.get(t, 0.) + 1\n",
    "    else:\n",
    "      t_1 = tokens[i-1]\n",
    "      if i == T - 1:\n",
    "        # measure probability of ending the line\n",
    "        add2dict(second_order, (t_1, t), 'END')\n",
    "      if i == 1:\n",
    "        # measure distribution of second word\n",
    "        # given only first word\n",
    "        add2dict(first_order, t_1, t)\n",
    "      else:\n",
    "        t_2 = tokens[i-2]\n",
    "        add2dict(second_order, (t_2, t_1), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2944ffe-6156-4c9e-b6b6-36437034b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94394cfa-3e65-4f20-9a9b-e5dcb1640150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...]\n",
    "# into {cat: 0.5, dog: 0.4, mouse: 0.1}\n",
    "\n",
    "def list2pdict(ts):\n",
    "  # turn each list of possibilities into a dictionary of probabilities\n",
    "  d = {}\n",
    "  n = len(ts)\n",
    "  for t in ts:\n",
    "    d[t] = d.get(t, 0.) + 1\n",
    "  for t, c in d.items():\n",
    "    d[t] = c / n\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447dbe5-435b-46cf-91a9-bec01f2c0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_1, ts in first_order.items():\n",
    "  # replace list with dictionary of probabilities\n",
    "  first_order[t_1] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0891ce-39d2-48c7-a402-82eff0715a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, ts in second_order.items():\n",
    "  second_order[k] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023298d-3bbc-42c1-bc93-a0df5e501d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "  # print \"d:\", d\n",
    "  p0 = np.random.random()\n",
    "  # print \"p0:\", p0\n",
    "  cumulative = 0\n",
    "  for t, p in d.items():\n",
    "    cumulative += p\n",
    "    if p0 < cumulative:\n",
    "      return t\n",
    "  assert(False) # should never get here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb166941-3d74-4d3b-9e99-7cabc4e80330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "  for i in range(4): # generate 4 lines\n",
    "    sentence = []\n",
    "\n",
    "    # initial word\n",
    "    w0 = sample_word(initial)\n",
    "    sentence.append(w0)\n",
    "\n",
    "    # sample second word\n",
    "    w1 = sample_word(first_order[w0])\n",
    "    sentence.append(w1)\n",
    "\n",
    "    # second-order transitions until END\n",
    "    while True:\n",
    "      w2 = sample_word(second_order[(w0, w1)])\n",
    "      if w2 == 'END':\n",
    "        break\n",
    "      sentence.append(w2)\n",
    "      w0 = w1\n",
    "      w1 = w2\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78797395-1d09-4206-8f07-5c923993f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5dadf-ba16-4334-a011-be008cc4d30e",
   "metadata": {},
   "source": [
    "## 1-3. Article Spinner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a783867-68b5-484c-8553-716874556918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0b0d8-a012-4de2-b4e2-fd81d24ae52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/bbc_text_cls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fac801-60cc-4f03-bd28-8fc7c009ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(df['labels'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e05abf-8c5d-4d94-bf63-a4c553fc2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a label whose data we want to train from\n",
    "label = 'business'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6733273-9271-4e03-9690-cdf750d644b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[df['labels'] == label]['text']\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ff387-0b37-4615-b899-b37b9dcd6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect counts\n",
    "probs = {} # key: (w(t-1), w(t+1)), value: {w(t): count(w(t))}\n",
    "\n",
    "for doc in texts:\n",
    "  lines = doc.split(\"\\n\")\n",
    "  for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    for i in range(len(tokens) - 2):\n",
    "      t_0 = tokens[i]\n",
    "      t_1 = tokens[i + 1]\n",
    "      t_2 = tokens[i + 2]\n",
    "      key = (t_0, t_2)\n",
    "      if key not in probs:\n",
    "        probs[key] = {}\n",
    "      \n",
    "      # add count for middle token\n",
    "      if t_1 not in probs[key]:\n",
    "        probs[key][t_1] = 1\n",
    "      else:\n",
    "        probs[key][t_1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037292f-947b-478d-ae48-cb4919f5c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize probabilities\n",
    "for key, d in probs.items():\n",
    "  # d should represent a distribution\n",
    "  total = sum(d.values())\n",
    "  for k, v in d.items():\n",
    "    d[k] = v / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dcda6-4a9f-424e-9c8d-433316387c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e70e6d-abdd-4361-b9ac-f67ad9e8fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.iloc[0].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05d6b0-c5c9-4023-b16c-4415b4d2af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spin_document(doc):\n",
    "  # split the document into lines (paragraphs)\n",
    "  lines = doc.split(\"\\n\")\n",
    "  output = []\n",
    "  for line in lines:\n",
    "    if line:\n",
    "      new_line = spin_line(line)\n",
    "    else:\n",
    "      new_line = line\n",
    "    output.append(new_line)\n",
    "  return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc8920-c5e6-446f-8ef7-5d802a66efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f46902-7905-46c1-9fd0-b34cef1dc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.iloc[0].split(\"\\n\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8a3d2-1129-485f-93b9-28b6b581d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer.detokenize(word_tokenize(texts.iloc[0].split(\"\\n\")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0da00-9556-4511-96be-5470940d16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "  p0 = np.random.random()\n",
    "  cumulative = 0\n",
    "  for t, p in d.items():\n",
    "    cumulative += p\n",
    "    if p0 < cumulative:\n",
    "      return t\n",
    "  assert(False) # should never get here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d655db4-4006-4bc4-93d9-339314cef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spin_line(line):\n",
    "  tokens = word_tokenize(line)\n",
    "  i = 0\n",
    "  output = [tokens[0]]\n",
    "  while i < (len(tokens) - 2):\n",
    "    t_0 = tokens[i]\n",
    "    t_1 = tokens[i + 1]\n",
    "    t_2 = tokens[i + 2]\n",
    "    key = (t_0, t_2)\n",
    "    p_dist = probs[key]\n",
    "    if len(p_dist) > 1 and np.random.random() < 0.3:\n",
    "      # let's replace the middle word\n",
    "      middle = sample_word(p_dist)\n",
    "      output.append(t_1)\n",
    "      output.append(\"<\" + middle + \">\")\n",
    "      output.append(t_2)\n",
    "\n",
    "      # we won't replace the 3rd token since the middle\n",
    "      # token was dependent on it\n",
    "      # instead, skip ahead 2 steps\n",
    "      i += 2\n",
    "    else:\n",
    "      # we won't replace this middle word\n",
    "      output.append(t_1)\n",
    "      i += 1\n",
    "  # append the final token - only if there was no replacement\n",
    "  if i == len(tokens) - 2:\n",
    "    output.append(tokens[-1])\n",
    "  return detokenizer.detokenize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49e3c9-8a98-4b17-a917-45b81a630876",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a0356-2b3e-4682-a1d6-5e496a9e84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(texts.shape[0])\n",
    "doc = texts.iloc[i]\n",
    "new_doc = spin_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3bb48-1838-4ebe-827c-1c77416c87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(\n",
    "    new_doc, replace_whitespace=False, fix_sentence_endings=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02120d95-4126-453d-95c4-74f609990961",
   "metadata": {},
   "source": [
    "## 1-4. Cipher Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e6cf1-a53e-4b0d-a033-70ced8eae3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create substitution cipher\n",
    "\n",
    "# one will act as the key, other as the value\n",
    "letters1 = list(string.ascii_lowercase)\n",
    "letters2 = list(string.ascii_lowercase)\n",
    "\n",
    "true_mapping = {}\n",
    "\n",
    "# shuffle second set of letters\n",
    "random.shuffle(letters2)\n",
    "\n",
    "# populate map\n",
    "for k, v in zip(letters1, letters2):\n",
    "  true_mapping[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f3f60-73c0-48d4-923e-d10736fb4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the language model\n",
    "\n",
    "# initialize Markov matrix\n",
    "M = np.ones((26, 26))\n",
    "\n",
    "# initial state distribution\n",
    "pi = np.zeros(26)\n",
    "\n",
    "# a function to update the Markov matrix\n",
    "def update_transition(ch1, ch2):\n",
    "  # ord('a') = 97, ord('b') = 98, ...\n",
    "  i = ord(ch1) - 97\n",
    "  j = ord(ch2) - 97\n",
    "  M[i,j] += 1\n",
    "\n",
    "# a function to update the initial state distribution\n",
    "def update_pi(ch):\n",
    "  i = ord(ch) - 97\n",
    "  pi[i] += 1\n",
    "\n",
    "# get the log-probability of a word / token\n",
    "def get_word_prob(word):\n",
    "  # print(\"word:\", word)\n",
    "  i = ord(word[0]) - 97\n",
    "  logp = np.log(pi[i])\n",
    "\n",
    "  for ch in word[1:]:\n",
    "    j = ord(ch) - 97\n",
    "    logp += np.log(M[i, j]) # update prob\n",
    "    i = j # update j\n",
    "\n",
    "  return logp\n",
    "\n",
    "# get the probability of a sequence of words\n",
    "def get_sequence_prob(words):\n",
    "  # if input is a string, split into an array of tokens\n",
    "  if type(words) == str:\n",
    "    words = words.split()\n",
    "\n",
    "  logp = 0\n",
    "  for word in words:\n",
    "    logp += get_word_prob(word)\n",
    "  return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1975c-99ca-47fa-a2d4-ed755a58fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a markov model based on an English dataset\n",
    "# is an edit of https://www.gutenberg.org/ebooks/2701\n",
    "# (I removed the front and back matter)\n",
    "\n",
    "# download the file\n",
    "if not os.path.exists('moby_dick.txt'):\n",
    "  print(\"Downloading moby dick...\")\n",
    "  r = requests.get('https://lazyprogrammer.me/course_files/moby_dick.txt')\n",
    "  with open('moby_dick.txt', 'w') as f:\n",
    "    f.write(r.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e4b68-9fa8-4502-a78f-35eb2c8c0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for replacing non-alpha characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# load in words\n",
    "for line in open('moby_dick.txt'):\n",
    "  line = line.rstrip()\n",
    "\n",
    "  # there are blank lines in the file\n",
    "  if line:\n",
    "    line = regex.sub(' ', line) # replace all non-alpha characters with space\n",
    "\n",
    "    # split the tokens in the line and lowercase\n",
    "    tokens = line.lower().split()\n",
    "\n",
    "    for token in tokens:\n",
    "      # update the model\n",
    "\n",
    "      # first letter\n",
    "      ch0 = token[0]\n",
    "      update_pi(ch0)\n",
    "\n",
    "      # other letters\n",
    "      for ch1 in token[1:]:\n",
    "        update_transition(ch0, ch1)\n",
    "        ch0 = ch1\n",
    "\n",
    "# normalize the probabilities\n",
    "pi /= pi.sum()\n",
    "M /= M.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e93135-9abb-4370-b239-dc95a3115b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a message\n",
    "\n",
    "# this is a random excerpt from Project Gutenberg's\n",
    "# The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
    "# https://www.gutenberg.org/ebooks/1661\n",
    "\n",
    "original_message = '''I then lounged down the street and found,\n",
    "as I expected, that there was a mews in a lane which runs down\n",
    "by one wall of the garden. I lent the ostlers a hand in rubbing\n",
    "down their horses, and received in exchange twopence, a glass of\n",
    "half-and-half, two fills of shag tobacco, and as much information\n",
    "as I could desire about Miss Adler, to say nothing of half a dozen\n",
    "other people in the neighbourhood in whom I was not in the least\n",
    "interested, but whose biographies I was compelled to listen to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472dab3-1911-4dcb-9000-ccda1432ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode a message\n",
    "def encode_message(msg):\n",
    "  # downcase\n",
    "  msg = msg.lower()\n",
    "\n",
    "  # replace non-alpha characters\n",
    "  msg = regex.sub(' ', msg)\n",
    "\n",
    "  # make the encoded message\n",
    "  coded_msg = []\n",
    "  for ch in msg:\n",
    "    coded_ch = ch # could just be a space\n",
    "    if ch in true_mapping:\n",
    "      coded_ch = true_mapping[ch]\n",
    "    coded_msg.append(coded_ch)\n",
    "\n",
    "  return ''.join(coded_msg)\n",
    "\n",
    "\n",
    "encoded_message = encode_message(original_message)\n",
    "\n",
    "\n",
    "# a function to decode a message\n",
    "def decode_message(msg, word_map):\n",
    "  decoded_msg = []\n",
    "  for ch in msg:\n",
    "    decoded_ch = ch # could just be a space\n",
    "    if ch in word_map:\n",
    "      decoded_ch = word_map[ch]\n",
    "    decoded_msg.append(decoded_ch)\n",
    "\n",
    "  return ''.join(decoded_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b7a98-072f-4dee-83df-708dcaa1b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an evolutionary algorithm to decode the message\n",
    "\n",
    "# this is our initialization point\n",
    "dna_pool = []\n",
    "for _ in range(20):\n",
    "  dna = list(string.ascii_lowercase)\n",
    "  random.shuffle(dna)\n",
    "  dna_pool.append(dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a505df-78cb-4a1b-837f-634a240f68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_offspring(dna_pool, n_children):\n",
    "  # make n_children per offspring\n",
    "  offspring = []\n",
    "\n",
    "  for dna in dna_pool:\n",
    "    for _ in range(n_children):\n",
    "      copy = dna.copy()\n",
    "      j = np.random.randint(len(copy))\n",
    "      k = np.random.randint(len(copy))\n",
    "\n",
    "      # switch\n",
    "      tmp = copy[j]\n",
    "      copy[j] = copy[k]\n",
    "      copy[k] = tmp\n",
    "      offspring.append(copy)\n",
    "\n",
    "  return offspring + dna_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2add4-a956-47fe-9b99-be53dcd8ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "scores = np.zeros(num_iters)\n",
    "best_dna = None\n",
    "best_map = None\n",
    "best_score = float('-inf')\n",
    "for i in range(num_iters):\n",
    "  if i > 0:\n",
    "    # get offspring from the current dna pool\n",
    "    dna_pool = evolve_offspring(dna_pool, 3)\n",
    "\n",
    "  # calculate score for each dna\n",
    "  dna2score = {}\n",
    "  for dna in dna_pool:\n",
    "    # populate map\n",
    "    current_map = {}\n",
    "    for k, v in zip(letters1, dna):\n",
    "      current_map[k] = v\n",
    "\n",
    "    decoded_message = decode_message(encoded_message, current_map)\n",
    "    score = get_sequence_prob(decoded_message)\n",
    "\n",
    "    # store it\n",
    "    # needs to be a string to be a dict key\n",
    "    dna2score[''.join(dna)] = score\n",
    "\n",
    "    # record the best so far\n",
    "    if score > best_score:\n",
    "      best_dna = dna\n",
    "      best_map = current_map\n",
    "      best_score = score\n",
    "\n",
    "  # average score for this generation\n",
    "  scores[i] = np.mean(list(dna2score.values()))\n",
    "\n",
    "  # keep the best 5 dna\n",
    "  # also turn them back into list of single chars\n",
    "  sorted_dna = sorted(dna2score.items(), key=lambda x: x[1], reverse=True)\n",
    "  dna_pool = [list(k) for k, v in sorted_dna[:5]]\n",
    "\n",
    "  if i % 200 == 0:\n",
    "    print(\"iter:\", i, \"score:\", scores[i], \"best so far:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5768841-0bd7-441e-9386-d9bd750f84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best score\n",
    "decoded_message = decode_message(encoded_message, best_map)\n",
    "\n",
    "print(\"LL of decoded message:\", get_sequence_prob(decoded_message))\n",
    "print(\"LL of true message:\", get_sequence_prob(regex.sub(' ', original_message.lower())))\n",
    "\n",
    "\n",
    "# which letters are wrong?\n",
    "for true, v in true_mapping.items():\n",
    "  pred = best_map[v]\n",
    "  if true != pred:\n",
    "    print(\"true: %s, pred: %s\" % (true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647c68c-3fc8-49c6-9639-70ab64db33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final decoded message\n",
    "print(\"Decoded message:\\n\", textwrap.fill(decoded_message))\n",
    "\n",
    "print(\"\\nTrue message:\\n\", original_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697293b8-2418-432b-99c8-d40a62e7fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61c523-fe36-405f-b0d9-992fede500cf",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes\n",
    "## 2-1. Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19df79-ced8-46fc-8de3-43fcfdefd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd481844-ca11-4cb7-8234-ad6917797fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "!wget https://lazyprogrammer.me/course_files/spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c093f-ba20-41f4-a7ec-fdb7dfb7cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file contains some invalid chars\n",
    "# depending on which version of pandas you have\n",
    "# an error may be thrown\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f4bbb-a3c5-4364-8508-340320befaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358eb0-7b7f-4100-a8fc-f459f753ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to something better\n",
    "df.columns = ['labels', 'data']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee03d3f-779b-4ff4-9e7d-1b290c3cef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddfe8a-b971-4dfd-b81e-7fd40dc756e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary labels\n",
    "df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})\n",
    "Y = df['b_labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31122e40-3a09-4780-9482-4aa124f56936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(\n",
    "    df['data'], Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efb377-d843-4e33-b39e-c4a31703ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try multiple ways of calculating features\n",
    "# featurizer = TfidfVectorizer(decode_error='ignore')\n",
    "# Xtrain = featurizer.fit_transform(df_train)\n",
    "# Xtest = featurizer.transform(df_test)\n",
    "\n",
    "featurizer = CountVectorizer(decode_error='ignore')\n",
    "Xtrain = featurizer.fit_transform(df_train)\n",
    "Xtest = featurizer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036dfbe-fa64-4ec7-864e-1f21ad8b4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a2f87-3d1a-446d-bb42-c6d578fe6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model, train it, print scores\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"train acc:\", model.score(Xtrain, Ytrain))\n",
    "print(\"test acc:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786ef27-9e9a-4c05-b3b0-45f4ba70a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrain = model.predict(Xtrain)\n",
    "Ptest = model.predict(Xtest)\n",
    "print(\"train F1:\", f1_score(Ytrain, Ptrain))\n",
    "print(\"test F1:\", f1_score(Ytest, Ptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425852e-50d0-49ff-9b18-0ae6cc8a6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob_train = model.predict_proba(Xtrain)[:,1]\n",
    "Prob_test = model.predict_proba(Xtest)[:,1]\n",
    "print(\"train AUC:\", roc_auc_score(Ytrain, Prob_train))\n",
    "print(\"test AUC:\", roc_auc_score(Ytest, Prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117ea6b-bebb-4859-ac34-5ff6390ba52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Ytrain, Ptrain)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b362fb-cae7-43f7-94f2-befcced9484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn is transitioning to V1 but it's not available on Colab\n",
    "# The changes modify how confusion matrices are plotted\n",
    "def plot_cm(cm):\n",
    "  classes = ['ham', 'spam']\n",
    "  df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "  ax = sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "  ax.set_xlabel(\"Predicted\")\n",
    "  ax.set_ylabel(\"Target\")\n",
    "\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf1799-3d2c-4091-9c72-0616aefd61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(Ytest, Ptest)\n",
    "plot_cm(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edaebaa-1a83-4c53-88ef-eb572b276d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "def visualize(label):\n",
    "  words = ''\n",
    "  for msg in df[df['labels'] == label]['data']:\n",
    "    msg = msg.lower()\n",
    "    words += msg + ' '\n",
    "  wordcloud = WordCloud(width=600, height=400).generate(words)\n",
    "  plt.imshow(wordcloud)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b060e9e-7c11-4f21-ad5c-de279b9afb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035f516-bca0-49c1-8c92-f568724fa9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcefba1-508c-4dd3-b9f5-19b1b3e0f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what we're getting wrong\n",
    "X = featurizer.transform(df['data'])\n",
    "df['predictions'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9d0a4-8621-480b-98f8-b1b470aebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# things that should be spam\n",
    "sneaky_spam = df[(df['predictions'] == 0) & (df['b_labels'] == 1)]['data']\n",
    "for msg in sneaky_spam:\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794705b-62ed-422e-a5eb-32839280c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# things that should not be spam\n",
    "# perhaps some are mislabeled?\n",
    "not_actually_spam = df[(df['predictions'] == 1) & (df['b_labels'] == 0)]['data']\n",
    "for msg in not_actually_spam:\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d53fed-7698-4d01-8ef2-e352c6dba216",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "## 3-1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b6df3-ab5b-444f-bb18-a121080c1153",
   "metadata": {},
   "source": [
    "# 4. PCA & SVD\n",
    "## 4-1. Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17353b33-512e-48ab-9764-1067c9ace7e4",
   "metadata": {},
   "source": [
    "# 5. Latent Dirichlet Allocation\n",
    "## 5-1. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556bc7d-212f-4d03-b208-032ec310def9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8163db1-e412-4e12-b60d-9d9210429445",
   "metadata": {},
   "source": [
    "# 6. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f0161-dde8-4914-b0ac-8316ba680ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
