{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff17a-cf78-46b0-aca5-43cd45f9bd55",
   "metadata": {
    "id": "880ff17a-cf78-46b0-aca5-43cd45f9bd55"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decc91c-8903-4365-a0fa-43b79fb45553",
   "metadata": {
    "id": "7decc91c-8903-4365-a0fa-43b79fb45553"
   },
   "source": [
    "# 1. Transfer Learning\n",
    "## 1-1. Feature Extraction\n",
    "## 1-2. Fine-Tuning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9f03c-639e-4b23-9622-c724939830d7",
   "metadata": {
    "id": "a1f9f03c-639e-4b23-9622-c724939830d7"
   },
   "outputs": [],
   "source": [
    "# [1] Freeze weights\n",
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# extract labels (number IDs) and remove from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1117be-5ddf-403a-b48f-7c9913292402",
   "metadata": {
    "id": "2e1117be-5ddf-403a-b48f-7c9913292402"
   },
   "outputs": [],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( dataNorm ).float()\n",
    "labelsT = torch.tensor( labels ).long()\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e60f3-7a01-4c75-9b6d-a5de22b9b898",
   "metadata": {
    "id": "069e60f3-7a01-4c75-9b6d-a5de22b9b898"
   },
   "outputs": [],
   "source": [
    "def createTheMNISTNet():\n",
    "\n",
    "  class mnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,64)\n",
    "\n",
    "      ### hidden layer\n",
    "      self.fc1 = nn.Linear(64,32)\n",
    "      self.fc2 = nn.Linear(32,32)\n",
    "\n",
    "      ### output layer\n",
    "      self.output = nn.Linear(32,10)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.fc1(x) )\n",
    "      x = F.relu( self.fc2(x) )\n",
    "      return self.output(x)\n",
    "\n",
    "  # create the model instance\n",
    "  net = mnistNet()\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer (using SGD to slow down learning!)\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527c916-6716-4d19-847c-142bcbeafd60",
   "metadata": {
    "id": "c527c916-6716-4d19-847c-142bcbeafd60"
   },
   "outputs": [],
   "source": [
    "# inspect the \"learning toggle\" of a layer\n",
    "N = createTheMNISTNet()[0]\n",
    "N.fc1.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c627e-959c-4647-97a0-c83061970e77",
   "metadata": {
    "id": "af5c627e-959c-4647-97a0-c83061970e77"
   },
   "outputs": [],
   "source": [
    "N = createTheMNISTNet()[0]\n",
    "\n",
    "# switch off all layers except input\n",
    "for p in N.named_parameters():\n",
    "  if 'input' not in p[0]:\n",
    "    p[1].requires_grad = False\n",
    "\n",
    "\n",
    "# check what we've done\n",
    "for p in N.named_parameters():\n",
    "  print('Requires_grad status in layer %s: %s' %(p[0],p[1].requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b0ffd-88fc-4449-a43e-ff20225fff8b",
   "metadata": {
    "id": "f43b0ffd-88fc-4449-a43e-ff20225fff8b"
   },
   "outputs": [],
   "source": [
    "def function2trainTheModel(net,lossfun,optimizer):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 100\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros(numepochs)\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # NEW: switch off learning in all-but-output layers during first 1/2 of training\n",
    "    if epochi<(numepochs/2):\n",
    "      for p in net.named_parameters():\n",
    "        if 'output' not in p[0]:\n",
    "          p[1].requires_grad = False\n",
    "    else:\n",
    "      for p in net.named_parameters():\n",
    "        p[1].requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # loop over training data batches\n",
    "    net.train()\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    net.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "\n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9537-37fe-4478-980c-aaff5eaf3387",
   "metadata": {
    "id": "5f0d9537-37fe-4478-980c-aaff5eaf3387"
   },
   "outputs": [],
   "source": [
    "# create the network\n",
    "net,lossfun,optimizer = createTheMNISTNet()\n",
    "\n",
    "# train the model\n",
    "trainAcc,testAcc,losses,net = function2trainTheModel(net,lossfun,optimizer)\n",
    "\n",
    "plt.plot(trainAcc,label='Train')\n",
    "plt.plot(testAcc,label='Test')\n",
    "plt.plot([len(trainAcc)/2, len(trainAcc)/2],[10,80],'k--',label='Learning switched on')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079bdfb-1844-4f8d-b170-a3a2f996afcb",
   "metadata": {
    "id": "c079bdfb-1844-4f8d-b170-a3a2f996afcb"
   },
   "source": [
    "# 2. Pre-Trained Transformer Models\n",
    "Transformer models can be grouped into three categories:\n",
    "- **Auto-Regressive:** GPT-like for **Natural Language Generation (NLG)** tasks.\n",
    "- **Auto-Encoding:** BERT-like for **Natural Language Understanding (NLU)**, or **Natural Langauage Inference (NLI)** tasks.\n",
    "- **Seq2Seq:** BART & T5-like for generative tasks that require an input, such as translation, summarization, or generative question answering.\n",
    "\n",
    "**Pre-Training** is the act of training a model from scratch. The weights are randomly initialized, and the training starts without any prior knowledge.\n",
    "\n",
    "The `transformers` library aims to provide a single application programming interface (API) through which any transformer model can be loaded, trained & saved. Its main features are ease of use, flexibility & simplicity.\n",
    "- [Supported Models & Frameworks](https://huggingface.co/docs/transformers/index#supported-models-and-frameworks)\n",
    "## 2-1. Pipelines\n",
    "1. `transformers.pipeline(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, kwargs)`: Returns an end-to-end object that performs a natural language processing task on one or several texts.\n",
    "    - Commonly used natural language processing pipelines inlcuding `feature-extraction`, `fill-mask`, `ner` (name entity recognition), `question anwsering`, `sentiment analysis`, `summarization`, `text-generation`, `translation` & `zero-shot-classification`.\n",
    "    - [Complete List of Supported Tasks](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7b534-dcce-49bb-a9e4-e77cd229bd20",
   "metadata": {
    "id": "d5f7b534-dcce-49bb-a9e4-e77cd229bd20",
    "outputId": "2838fd58-5716-40d2-8b66-77bf24b394d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_classification.TextClassificationPipeline"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip3 install tf-keras\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "type(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fce79-03b0-4566-a71e-0501beb0d585",
   "metadata": {
    "id": "e90fce79-03b0-4566-a71e-0501beb0d585",
    "outputId": "db458f0a-2e55-4f37-8614-21244376c04e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998759031295776}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is a dictionary\n",
    "classifier(\"This is such a great movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9aac3-1d15-4db3-bbcf-66d573bf76b5",
   "metadata": {
    "id": "a2d9aac3-1d15-4db3-bbcf-66d573bf76b5",
    "outputId": "ce8a91fc-5529-4a53-abfe-975b02bed2fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4e66e-e4a3-4251-85c4-14551d205e0a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b39425cea3074dddb20473375259badb",
      "213014993ed14eecb331751f6c66da4a",
      "123bf7b08fc44f868420fda8f6687ee4",
      "5ad28d66507d422a9c8e08ef21bba927",
      "5daefbe91292422d80c9e6d3ec33ba21",
      "676e97cb635241a58602d6ed4ed64175"
     ]
    },
    "id": "e9d4e66e-e4a3-4251-85c4-14551d205e0a",
    "outputId": "faa3d18f-53b2-4e15-85d0-cc883a9d3ccc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39425cea3074dddb20473375259badb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213014993ed14eecb331751f6c66da4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123bf7b08fc44f868420fda8f6687ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad28d66507d422a9c8e08ef21bba927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5daefbe91292422d80c9e6d3ec33ba21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e97cb635241a58602d6ed4ed64175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8719879388809204, 0.09406529366970062, 0.03394680842757225]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This is a course about the Transformers library.\", candidate_labels=[\"education\", \"politics\", \"business\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb653b-192e-462e-a1ed-b90c852da19c",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a10ab2b39a4142ceb7d5ffe74037a83d",
      "b16fb082065b4824b9d241d75c8c75b5",
      "4200fe7f4fe744d788a152ca9fd0f174",
      "f7034c02fca04ce885ad4b54ca56d311",
      "c96e6b00a4f4490dbbaf751df7cd3c85",
      "2c37a8dbaa8a4139b0bae9f5a74043df",
      "23f426fea6c34f109aec0ca9d95aeb72"
     ]
    },
    "id": "b7bb653b-192e-462e-a1ed-b90c852da19c",
    "outputId": "b8a57d1d-eb41-4aac-bab9-43f7d56579f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10ab2b39a4142ceb7d5ffe74037a83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16fb082065b4824b9d241d75c8c75b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4200fe7f4fe744d788a152ca9fd0f174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7034c02fca04ce885ad4b54ca56d311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96e6b00a4f4490dbbaf751df7cd3c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c37a8dbaa8a4139b0bae9f5a74043df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f426fea6c34f109aec0ca9d95aeb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to take control of your body by eliminating the effects of bad eating, body odor, and bad exercise. Learn to clean your body and gain control by avoiding unhealthy foods and reducing toxins from your body.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c495c-eec0-4be6-8f8f-86e0327a1777",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6c0eaa64de514fae813c5e785463aabb",
      "6e690a7c7bc840daa6b244af29c5e583",
      "18cd9b4047f34cd48ba1cabaddebb24a",
      "50c95aed8e4147229c36e2bf1992a262",
      "f4313dfbec874038a33d167837b370ae",
      "e69060ef55ee45a3931bda314abbd7ff",
      "f3b2f7d31b78452589d96012e0a80d08"
     ]
    },
    "id": "146c495c-eec0-4be6-8f8f-86e0327a1777",
    "outputId": "f58a3190-5803-43ce-db3d-1c539f04ec62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0eaa64de514fae813c5e785463aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e690a7c7bc840daa6b244af29c5e583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cd9b4047f34cd48ba1cabaddebb24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c95aed8e4147229c36e2bf1992a262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4313dfbec874038a33d167837b370ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69060ef55ee45a3931bda314abbd7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b2f7d31b78452589d96012e0a80d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to make your own vegan-friendly breakfast foods with a minimal effort.\\n\\n\\n\\n\\nHow to'},\n",
       " {'generated_text': 'In this course, we will teach you how to build the software yourself â€“ by using the language for your use on the Windows XP 7 PCs, Vista'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"In this course, we will teach you how to\", max_length=30, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffb717-1308-48cf-8aeb-316260d1a685",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cd2cf55195f24376888d33b97c0625a8",
      "6cf9352442ab4d44808cb08d06fef4ca",
      "96e056adf47348998b1d82c14e855e47",
      "bce9238c1a4d4e7d9782d85f5d670b3f",
      "bf57bbcadc534cfe96676309f1dd177d",
      "625e0eb0fd0743fda147e1f800febdc6"
     ]
    },
    "id": "a4ffb717-1308-48cf-8aeb-316260d1a685",
    "outputId": "44f13274-6fe7-4c93-a9f4-d20f1db11efa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2cf55195f24376888d33b97c0625a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf9352442ab4d44808cb08d06fef4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e056adf47348998b1d82c14e855e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce9238c1a4d4e7d9782d85f5d670b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf57bbcadc534cfe96676309f1dd177d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625e0eb0fd0743fda147e1f800febdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19198447465896606,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04209218546748161,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The `fill-mask` pipeline will predict missing words in a sentence\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc355a-34a1-48dd-a9e2-f95dc6f64b54",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d1b33e6fdcd844fdb8fcc8641db9292b",
      "129f9103b37d4b739a6a78ac8bccdc0c",
      "ec0847a23a9f49f1aa57f5e3ca663d4c",
      "533091d3f2fd442ca2c435a6c5298afd"
     ]
    },
    "id": "0ebc355a-34a1-48dd-a9e2-f95dc6f64b54",
    "outputId": "c7a00296-df29-43db-d4ee-26e56c18dafb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b33e6fdcd844fdb8fcc8641db9292b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f9103b37d4b739a6a78ac8bccdc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0847a23a9f49f1aa57f5e3ca663d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533091d3f2fd442ca2c435a6c5298afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9989868,\n",
       "  'word': 'Joe',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9953941,\n",
       "  'word': 'AI',\n",
       "  'start': 37,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9991089,\n",
       "  'word': 'New York',\n",
       "  'start': 43,\n",
       "  'end': 51}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Joe and I work at exalted AI in New York.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016aa41-794c-4122-9f75-b5480bbeb446",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "42796a9633b1492d8263dda766431c6f",
      "3ed364e2fa204c92b78a1b8cebe27903",
      "4979c8fea86d4635a5808e6a309ed390",
      "6f30c62c2b75431496fd89ed246093bb",
      "8ba2ceafb18a4528bbb61cde4d514003"
     ]
    },
    "id": "f016aa41-794c-4122-9f75-b5480bbeb446",
    "outputId": "24a5c473-e5aa-47a7-fa2e-670432659292"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42796a9633b1492d8263dda766431c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed364e2fa204c92b78a1b8cebe27903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4979c8fea86d4635a5808e6a309ed390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f30c62c2b75431496fd89ed246093bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba2ceafb18a4528bbb61cde4d514003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.30933716893196106,\n",
       " 'start': 29,\n",
       " 'end': 51,\n",
       " 'answer': 'exalted AI in New York'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(question=\"Where do I work?\", context=\"My name is Joe and I work at Exalted AI in New York.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f43df-3637-450a-aed5-cb73f99232a6",
   "metadata": {
    "id": "c67f43df-3637-450a-aed5-cb73f99232a6",
    "outputId": "65de0e9e-0d9d-47cc-d442-2162ce3a5ab8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a0a3c-5df5-4ccc-b733-0267783c40be",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1cba7f6cf0e2428a9c0a1290a5044c57",
      "af3af7e0d03342508255cb77ee96218e",
      "3f094924ef5440deab0ad251db48da47",
      "f5f6c040d3ea40c4b920ef692e50309f",
      "54d8ec97843848b490c0a895c5bee0c3",
      "3cca87724665402080c6004063d3b841",
      "ff844b8b11b04181ae7aa5f05cd657c1"
     ]
    },
    "id": "705a0a3c-5df5-4ccc-b733-0267783c40be",
    "outputId": "c03a05f7-1cfc-4878-d9fe-a673a31669ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cba7f6cf0e2428a9c0a1290a5044c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3af7e0d03342508255cb77ee96218e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f094924ef5440deab0ad251db48da47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f6c040d3ea40c4b920ef692e50309f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d8ec97843848b490c0a895c5bee0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cca87724665402080c6004063d3b841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff844b8b11b04181ae7aa5f05cd657c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004dbad-d47c-4505-9b8d-b00dc93e0ace",
   "metadata": {
    "id": "4004dbad-d47c-4505-9b8d-b00dc93e0ace"
   },
   "source": [
    "### 2-1-1. Tokenizers\n",
    "There is a fast state-of-the-art `tokenizers` Rust library, developed & recommended by Hugging Face to replace the Python implementation, focusing on performance & versatility.\n",
    "\n",
    "A **Tokenizer** is responsible for the following missions:\n",
    "- Subword tokenize & convert token strings to integers (indexes).\n",
    "- Add new tokens to the vocabulary in a way that is independent of the underlying structure, such as BPE & SentencePiece.\n",
    "- Manage special tokens, like mask & beginning-of-sentence. Add & assign them to attributes.\n",
    "\n",
    "Auto tokenizer class:\n",
    "1. `transformers.AutoTokenizer`: Generic tokenizer class.\n",
    "    - `from_pretrained(pretrained_model_name_or_path, *inputs, config, cache_dir, force_download, proxies, revision, subfolder, use_fast=True, tokenizer_type, trust_remote_code=False, **kwargs)`: Uses a fast Rust-based tokenizer if `use_fast` is `True`.\n",
    "\n",
    "Tokenizer classes:\n",
    "\n",
    "2. `transformers.PreTrainedTokenizer(model_max_length, padding_side, truncation_side, chat_template, model_input_names, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, clean_up_tokenization_spaces=True, split_special_tokens=False)`: The base tokenizer class.\n",
    "3. `transformers.*Tokenizer(model_max_length, padding_side, truncation_side, chat_template, model_input_names, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, clean_up_tokenization_spaces=True, split_special_tokens=False)`\n",
    "\n",
    "- All tokenizer classes inherit the following attributes & methods:\n",
    "    - `__call__(text=None, text_pair=None, text_target=None, text_pair_target=None, add_special_tokens=True, padding=False, truncation=False, max_length=None, stride=0, is_split_into_words=False, pad_to_multiple_of=None, padding_side=None, return_tensors=None, return_token_type_ids=None, return_attention_mask=None, return_overflowing_tokens=False, return_special_tokens_mask=False, return_offsets_mapping=False, return_length=False, verbose=True, **kwargs)`\n",
    "        - `padding` accepts values among `True` or `longest`, `max_length`, and `False` or `do_not_pad`.\n",
    "        - The `truncation` argument can be set as `True` or `longest_first`, `only_first`, `only_second`, and `False` or `do_not_truncate`. If truncation is activated, long sequences will be truncated to specified `max_length` or to the maximum acceptable input length for the model if not provided.\n",
    "        - `return_tensors` can be `tf` (`tensorflow.constant`), `pt` (`torch.Tensor`) or `np` (`numpy.ndarray`).\n",
    "    - `padding_side`: `right` or `left`.\n",
    "    - `truncation_side`: Also `right` or `left`.\n",
    "    - `tokenize(text, **kwargs)`\n",
    "    - `convert_tokens_to_ids(tokens)`\n",
    "    - `encode(text, text_pair=None, add_special_tokens=True, padding=False, truncation=None, max_length=None, stride=0, padding_side=None, return_tensors=None, **kwargs)`: Same as calling `tokenize` & `convert_tokens_to_ids` in sequence.\n",
    "    - `decode(token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=None, **kwargs)`: Similar to doing `convert_ids_to_tokens` and then combining tokens to a string.\n",
    "    - `convert_ids_to_tokens(ids, skip_special_tokens=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867af00-c3aa-4866-8f30-e2a39861b0a2",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "dd6ba574dbd8491a8dd78542823009fc",
      "9aa4a23433ae478fb90ffd96d6bcd721",
      "620ae8c537ab4f1a8061fc9ee0e91ace"
     ]
    },
    "id": "1867af00-c3aa-4866-8f30-e2a39861b0a2",
    "outputId": "bf65e1da-f03d-4466-8aea-da2118074dcc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6ba574dbd8491a8dd78542823009fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa4a23433ae478fb90ffd96d6bcd721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620ae8c537ab4f1a8061fc9ee0e91ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# `AutoTokenizer`\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780857e9-4f22-4e3c-b8f6-ee9d24851214",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9dc6751c57064434b90cfcd42c102476",
      "f0827f2e1f8042239fcff77191d9a86e",
      "414d20c72ccf4222a0448eb6f5744f11"
     ]
    },
    "id": "780857e9-4f22-4e3c-b8f6-ee9d24851214",
    "outputId": "a89ae261-9acb-407d-f69c-728120a5382c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc6751c57064434b90cfcd42c102476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0827f2e1f8042239fcff77191d9a86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414d20c72ccf4222a0448eb6f5744f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907f4b6-da61-45ad-90c8-eef31a97cfb3",
   "metadata": {
    "id": "e907f4b6-da61-45ad-90c8-eef31a97cfb3",
    "outputId": "45c07bbd-8df8-4e05-80bc-c7b4a6780f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81426e-0f8a-4d77-8233-4d322779be16",
   "metadata": {
    "id": "9e81426e-0f8a-4d77-8233-4d322779be16",
    "outputId": "4d32ea28-341b-4bdf-92c1-73091d3aded4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d4763-a027-4327-9954-cbce93084303",
   "metadata": {
    "id": "5e1d4763-a027-4327-9954-cbce93084303",
    "outputId": "f708e7de-a25a-4b51-91aa-c3f8e4934594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `*Tokenizer()`\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8c41b-c0db-4f49-904a-0a2943b57c07",
   "metadata": {
    "id": "fee8c41b-c0db-4f49-904a-0a2943b57c07"
   },
   "source": [
    "### 2-1-2. Models\n",
    "Generic auto model classes:\n",
    "1. `transformers.AutoModel`: The vector output by the transformer module generally has three dimension: $(batch\\_size, sequence\\_length, hidden\\_size)$ which, respectively, means the number of sequences processed at a time, the length of the numerical represention of the sequence, and the vector dimension of each model input.\n",
    "2. `transformers.AutoModelForPreTraining`: The generic pretraining class for instantiating a model with a pretraining head. Also has `from_config` & `from_pretrained` methods.\n",
    "\n",
    "- All auto classes have the following methods:\n",
    "    - `from_config(config)`\n",
    "    - `from_pretrained(pretrained_model_name_or_path, *model_args, config, state_dict, cache_dir, from_tf, force_download, proxies, output_loading_info, local_files_only, revision, trust_remote_code, code_revision, kwargs)`\n",
    "\n",
    "Model classes:\n",
    "\n",
    "3. `transformers.PreTrainedModel(config, *inputs, **kwargs)`: Base class for all models.\n",
    "4. `transformers.*Model(config, *inputs, **kwargs)`\n",
    "\n",
    "- All model classes implement the following attributes & methods:\n",
    "    - `from_pretrained(pretrained_model_name_or_path, *model_args, config=None, cache_dir=None, ignore_mismatched_sizes=False, force_download=False, local_files_only=False, token=None, revision=\"main\", use_safetensors=None, **kwargs)`: `config` can be either an instance of a class derived from `transformers.PreTrainedConfig` or a path valid as input. `**kwargs` are remaining keyword arguments that can be used to update the configuration object & initiate the model.\n",
    "    - `save_pretrained(save_directory, is_main_process=True, state_dict=None, save_function, push_to_hub=False, max_shard_size='5GB', safe_serialization=True, variant, token, save_peft_format=True, **kwargs)`: Outputs a configuration file & a state dictionary containing all model's weights.\n",
    "\n",
    "Configuration classes:\n",
    "\n",
    "5. `transformers.PreTrainedConfig(name_or_path=\"\", output_hidden_states=False, output_attentions=False, return_dict=True, is_encoder_decoder=False, is_decoder=False, cross_attention_hidden_size=None, add_cross_attention=False, tie_encoder_decoder=False, prune_heads={}, architectures=None, funetuning_task=None, id2label=None, label2id=None, num_labels=None, task_specific_params=None, problem_type=None, tokenizer_class=None, prefix=None, bos_token_id=None, pad_token_id=None, eos_token_id=None, decoder_start_token_id=None, sep_token_id=None, torchscript=False, tie_word_embeddings=True, torch_dtype=None, use_bfloat16=False, tf_legacy_loss=False)`: The base class implementing the common methods for loading & saving configurations.\n",
    "    - For fine-tuning tasks, `problem_type` can be one of `regression`, `single_label_classification` or `multi_label_classification`.\n",
    "7. `transformers.*Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c90547-d925-4090-b1d0-4105ec60bf8a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "163cff5da52a42ad98922c1545d54d18"
     ]
    },
    "id": "b0c90547-d925-4090-b1d0-4105ec60bf8a",
    "outputId": "7df458e8-3e48-4ab5-8155-b170f2043dc3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163cff5da52a42ad98922c1545d54d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# `AutoModel()`\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# High-dimensional vector\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93618755-d10d-4bf8-ab2c-0c52dbb21b3a",
   "metadata": {
    "id": "93618755-d10d-4bf8-ab2c-0c52dbb21b3a",
    "outputId": "26661a75-c2d6-4f71-c404-0477d4df084c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# `*Model()`\n",
    "# Model is randomly initialized\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Build the config\n",
    "config = BertConfig()\n",
    "print(config)\n",
    "\n",
    "# Build the model from the config\n",
    "model = BertModel(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e9791-293c-42eb-8a37-ffd5807a588f",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6fecd7080ec947b793858093a649ab6d",
      "0a29f000e169427d8d1bac3cffb61de6"
     ]
    },
    "id": "6d8e9791-293c-42eb-8a37-ffd5807a588f",
    "outputId": "854104e5-ba0f-473e-c984-a1f72e95325c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fecd7080ec947b793858093a649ab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a29f000e169427d8d1bac3cffb61de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee70df9-e4e8-4b71-9067-569921a6312e",
   "metadata": {
    "id": "1ee70df9-e4e8-4b71-9067-569921a6312e"
   },
   "outputs": [],
   "source": [
    "# This will create `model.safetensors` & `config.json` in the `models` folder\n",
    "model.save_pretrained(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7ad3b-a23b-4e90-aa00-db7a8236b479",
   "metadata": {
    "id": "0ef7ad3b-a23b-4e90-aa00-db7a8236b479"
   },
   "source": [
    "### 2-1-3. Heads\n",
    "The **Model Heads** take the high-dimensional vector of hidden states as input and project them onto a different dimension. Different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it. For example, for a model with a sequence classification head, we will not actually use the `AutoModel` class but `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0dc646-f39c-4f0d-b49b-5a5184a29efb",
   "metadata": {
    "id": "1f0dc646-f39c-4f0d-b49b-5a5184a29efb",
    "outputId": "b42ccb9d-8534-4805-9868-4a2f54048abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138f856-8c70-4b69-b7b8-068382444a49",
   "metadata": {
    "id": "9138f856-8c70-4b69-b7b8-068382444a49"
   },
   "source": [
    "### 2-1-4. Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4619e-a74e-4331-89ee-414643d75ab8",
   "metadata": {
    "id": "51f4619e-a74e-4331-89ee-414643d75ab8",
    "outputId": "7ee9aa74-a42f-4167-abb5-6cbdcf261143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df3ae5-d7e2-4fc8-8932-b1d4e08c77cd",
   "metadata": {
    "id": "f8df3ae5-d7e2-4fc8-8932-b1d4e08c77cd",
    "outputId": "d43425f7-380e-4e98-ba25-e6d7df7080cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd620b-a636-469c-aff6-7b4f590d1851",
   "metadata": {
    "id": "34cd620b-a636-469c-aff6-7b4f590d1851",
    "outputId": "1cd038f6-fec7-49bb-828e-f00185380e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf50f82-d09c-424f-b5ac-bf6ecc8e22e0",
   "metadata": {
    "id": "5cf50f82-d09c-424f-b5ac-bf6ecc8e22e0"
   },
   "source": [
    "## 2-2. Batching\n",
    "**Batching** is the act of sending multiple sentences through the model all at once. The following are the points to handle multiple sequences:\n",
    "- Models in `transformers` expect a batch of inputs (multiple sentences) by default.\n",
    "- Padding makes sure all our sentences have the same length by adding a special word called the **Padding Token** to the sentences with fewer values.\n",
    "- Attention masks are tensors with the exact same shape as the input `ids` tensor, filled with `0`s and `1`s: `1`s indicate the corresponding tokens should be attended to, and `0`s indicate the corresponding tokens should not be attended to (should be ignored by the attention layers of the model).\n",
    "- To solve the problem of very long sequences, apply **Truncation** or use a model with a longer supported sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95def405-605a-4a5d-a4ea-32d84657494e",
   "metadata": {
    "id": "95def405-605a-4a5d-a4ea-32d84657494e",
    "outputId": "0ef9fd45-95b3-40e8-f5a2-0fc824567e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n",
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Add a dimension on top of `ids` because `transformers` models expect multiple sentences by default\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b41f7-03e6-4a8c-981f-8cae30aa89ea",
   "metadata": {
    "id": "188b41f7-03e6-4a8c-981f-8cae30aa89ea",
    "outputId": "65933e40-c7b7-4c32-bda0-10fb894e5633"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The following list of lists cannot be converted to a tensor\n",
    "# batched_ids = [\n",
    "#     [200, 200, 200],\n",
    "#     [200, 200]\n",
    "# ]\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a348f-a605-4ea1-ac13-74943585d903",
   "metadata": {
    "id": "4f4a348f-a605-4ea1-ac13-74943585d903",
    "outputId": "ed6b56bf-d216-4454-8049-d46c96a56f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Attention masks\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9fd06-37d9-458d-9894-af41125b23f4",
   "metadata": {
    "id": "26f9fd06-37d9-458d-9894-af41125b23f4",
    "outputId": "79d6da73-1297-49c0-b718-291a6b537cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da0e49-dfb5-4c96-990e-40ae6e5a086a",
   "metadata": {
    "id": "f4da0e49-dfb5-4c96-990e-40ae6e5a086a",
    "outputId": "d207041b-74df-4d7a-f966-8ba328ada2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# From tokenizer to model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Padding, truncation\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ce99b-4718-4fa5-be2b-cee251d7e158",
   "metadata": {
    "id": "5d6ce99b-4718-4fa5-be2b-cee251d7e158"
   },
   "source": [
    "## 2-3. Fine-Tuning with Transformers\n",
    "### 2-3-1. Data Collators\n",
    "\n",
    "Typical preprocessing steps after loading data in `datasets.dataset_dict.DatasetDict` from the `datasets` library including:\n",
    "- Apply a tokenization function on all the datasets at once. The `datasets` library will add new feature fields to them. Usually use the `datasets.dataset_dict.DatasetDict.map` method.\n",
    "- Define a **Collate Function** which is an argument passed to build a `torch.utils.data.DataLoader`, by default converting your samples to `torch.Tensor` & concatenating them recursively, but for natural language processing (NLP) tasks, more operations are usually required. For example, the inputs are usually not of the same size so that we may have to perform **Dynamic Padding** on each batch.\n",
    "\n",
    "1. `transformers.DataCollatorWithPadding(tokenizer, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')`: A data collator dynamically pads the inputs received. Takes a `transformers.PreTrainedTokenizer` to know which padding token to use & whether the model expects pre padding or post padding.  \n",
    "\n",
    "Fine-tune BERT for semantic textual similarity with the [MRPC (Microsoft Research Paraphrase Corpus)](https://gluebenchmark.com/tasks) dataset from the [GLUE (General Language Understanding Evaluation) Benchmark](https://gluebenchmark.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f743c-e2ad-436c-ad78-27b0c4dd38aa",
   "metadata": {
    "id": "9c9f743c-e2ad-436c-ad78-27b0c4dd38aa",
    "outputId": "1257515f-0f41-4064-af16-3fe958ebb16f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model on two sentences for example\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c8731-9086-4e35-8100-3a6a7ef61cf5",
   "metadata": {
    "id": "f51c8731-9086-4e35-8100-3a6a7ef61cf5",
    "outputId": "eff90dda-b073-4ad8-f0c2-b74861e55419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7437-0e79-4e77-9221-e5fe3e0a8d69",
   "metadata": {
    "id": "608c7437-0e79-4e77-9221-e5fe3e0a8d69",
    "outputId": "9f267e2f-7cc9-4c5b-c210-aaf4fafba235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Single Example of the Raw Training Set:\n",
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "\n",
      "The Features Attribute of the Raw Training Set:\n",
      "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "print(\"A Single Example of the Raw Training Set:\")\n",
    "print(raw_train_dataset[0])\n",
    "print()\n",
    "print(\"The Features Attribute of the Raw Training Set:\")\n",
    "print(raw_train_dataset.features)\n",
    "# `sentence1` & `sentence2` are features in next sentence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5a667-21ba-4026-8a93-59f4f67230c7",
   "metadata": {
    "id": "25d5a667-21ba-4026-8a93-59f4f67230c7"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# Tokenize only one feature\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "# print(tokenized_sentences_1)\n",
    "# Return a overwhelming dictionary with keys containing `input_ids`, `attention_mask` & `token_type_ids`, and their values that are lists of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c4ba1-e2b5-4122-9008-f6a0fcc9cab4",
   "metadata": {
    "id": "ff9c4ba1-e2b5-4122-9008-f6a0fcc9cab4",
    "outputId": "ff85ed2c-373e-4612-bd63-fc752c2de75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs after Inputting Sentence 1 & Sentence 2:\n",
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Input `sentence1` & `sentence2`\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(\"Tokenized Inputs after Inputting Sentence 1 & Sentence 2:\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc06228-d54c-402c-8d09-0dc5caf787b1",
   "metadata": {
    "id": "0fc06228-d54c-402c-8d09-0dc5caf787b1",
    "outputId": "8b0e3a21-7f76-47d7-8326-0d0ac572f7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Model expects the inputs to be the form `[CLS] sentence1 [SEP] sentence2 [SEP]`\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "# Aligned with 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3c943-600e-41e2-8566-eebdefad1bc4",
   "metadata": {
    "id": "a3c3c943-600e-41e2-8566-eebdefad1bc4",
    "outputId": "fe6ae588-b8f8-44b4-8030-34b115da9911"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only works if you have enough RAM to store your whole dataset during the tokenization\n",
    "tokenized_dataset = tokenizer(raw_datasets[\"train\"][\"sentence1\"], raw_datasets[\"train\"][\"sentence2\"], padding=True, truncation=True)\n",
    "# print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1316130-0de5-4d59-8ba2-a148d77ec0a6",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "159593c039cc4931974b1ac65bd04f56",
      "9a21eb5baf994dfc872fad0b342282b3",
      "47535108d2b546c980f0115fbb32eb9a"
     ]
    },
    "id": "a1316130-0de5-4d59-8ba2-a148d77ec0a6",
    "outputId": "71ba3bdc-eba1-40c7-d22e-ff366e2f4f39"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159593c039cc4931974b1ac65bd04f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a21eb5baf994dfc872fad0b342282b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47535108d2b546c980f0115fbb32eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Better approach that keeps the data as a `Dataset`\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# `batched=True` to speed up the tokenization\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec84003-12ae-4f36-975f-3dcb18d2d712",
   "metadata": {
    "id": "5ec84003-12ae-4f36-975f-3dcb18d2d712",
    "outputId": "6a7c200d-71df-438d-ee8c-3c023e749837"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 00:23:16.322194: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 00:23:16.437840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 00:23:16.484108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 00:23:16.498814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 00:23:16.580612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 00:23:17.334796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "# Dynamic padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# `collate_fn` responsible for putting together samples inside a batch as a parameter of `torch.utils.data.DataLoader`\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a73130-b674-46e8-bb32-1088c3b7e9f4",
   "metadata": {
    "id": "85a73130-b674-46e8-bb32-1088c3b7e9f4",
    "outputId": "62606c15-0cfc-420c-cf0f-7894bdfc83cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of Each Entry in the Batch: [50, 59, 47, 67, 59, 50, 62, 32]\n"
     ]
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "# Remove the `idx`, `sentence1` & `sentence2` columns\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "print(\"Lengths of Each Entry in the Batch:\", [len(x) for x in samples[\"input_ids\"]])\n",
    "# Get samples of varying length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36330b-cd50-477e-865c-9892fbd15fbd",
   "metadata": {
    "id": "cc36330b-cd50-477e-865c-9892fbd15fbd",
    "outputId": "0a70e0b2-10f2-4bce-9d16-83fbe9146b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'labels': torch.Size([8])}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the samples after applying the collate function\n",
    "batch = data_collator(samples)\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "# All padded to the maximum length inside the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9af004-51ed-42db-8f19-774678a3c3b6",
   "metadata": {
    "id": "ee9af004-51ed-42db-8f19-774678a3c3b6"
   },
   "source": [
    "### 2-3-2. Trainer\n",
    "\n",
    "1. `transformers.Trainer(model=None, args=None, data_collator=None, train_dataset=None, eval_dataset=None, tokenizer=None, model_init=None, compute_metrics=None, callbacks=None, optimizers=(None, None), preprocess_logits_for_metrics=None)`: `args` defaults to an instance of `TrainingArguments`. `data_collator` defaults to `transformers.DataCollatorWithPadding` if no `tokenizer` is provided. Set `evaluation_strategy` to either `steps` (evaluate every `eval_steps`) or `epoch` (evaluate at the end of each epoch). You can provide a `compute_metrics` function to calculate a metric during said evaluation, otherwise the evaluation would just print the loss which is not a very intuitive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbff7f-2273-4e02-9b6c-746794b79ce1",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6479176f4a1645fb916642982711131a"
     ]
    },
    "id": "bacbff7f-2273-4e02-9b6c-746794b79ce1",
    "outputId": "f777e6ca-442a-4b3d-b8f4-f91eda9b25af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6479176f4a1645fb916642982711131a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wrap up all the processes before\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd0aab-d9ec-46d2-ae6f-6743626f6e1b",
   "metadata": {
    "id": "8dfd0aab-d9ec-46d2-ae6f-6743626f6e1b",
    "outputId": "688ca377-5a9c-4509-a422-e758ec629cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=test-trainer/runs/Oct02_02-03-38_yungshun317-Titan-18-HX-A14VIG,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=test-trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=test-trainer,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6acc98-57ab-4d49-b289-eb64f7acda23",
   "metadata": {
    "id": "ea6acc98-57ab-4d49-b289-eb64f7acda23",
    "outputId": "82593f80-b1ca-436f-ec7d-920f9ec97c41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.338400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3770664190155229, metrics={'train_runtime': 78.5738, 'train_samples_per_second': 140.047, 'train_steps_per_second': 17.525, 'total_flos': 405114969714960.0, 'train_loss': 0.3770664190155229, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with almost everything set by default\n",
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ce413-483b-47ec-ac2b-b379f7026a20",
   "metadata": {
    "id": "a39ce413-483b-47ec-ac2b-b379f7026a20",
    "outputId": "ff0e2a1f-0cc0-4a30-c015-17ae80d1e0bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions Shape: (408, 2)\n",
      "label_ids Shape: (408,)\n",
      "metrics: {'test_loss': 0.6020950078964233, 'test_runtime': 0.6222, 'test_samples_per_second': 655.688, 'test_steps_per_second': 81.961}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "# `predictions` is a named tuple with `predictions`, `label_ids` & `metrics` three fields\n",
    "print(\"predictions Shape:\", predictions.predictions.shape)\n",
    "print(\"label_ids Shape:\", predictions.label_ids.shape)\n",
    "print(\"metrics:\", predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297161ff-0017-46f8-888a-1b21df46b0b8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bcf1d8eecd3d4d7c8629c4c0a0b64a84"
     ]
    },
    "id": "297161ff-0017-46f8-888a-1b21df46b0b8",
    "outputId": "5a306838-d6fb-4123-a634-087574e29849"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf1d8eecd3d4d7c8629c4c0a0b64a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8578431372549019, 'f1': 0.8993055555555556}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Take the index with the maximum value on the second axis\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Load the dataset-specific metrics\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "print(metric.compute(predictions=preds, references=predictions.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d39c4-781b-45a0-992b-058730f77186",
   "metadata": {
    "id": "162d39c4-781b-45a0-992b-058730f77186",
    "outputId": "0973afdc-9cbe-4e20-e35a-560e014a1d34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422424</td>\n",
       "      <td>0.781863</td>\n",
       "      <td>0.854812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.362014</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.891697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.602095</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.899306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3770664190155229, metrics={'train_runtime': 85.7439, 'train_samples_per_second': 128.336, 'train_steps_per_second': 16.059, 'total_flos': 405114969714960.0, 'train_loss': 0.3770664190155229, 'epoch': 3.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with the `compute_metrics` parameter set\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade980a6-ef80-406f-bd9d-bc832128d6aa",
   "metadata": {
    "id": "ade980a6-ef80-406f-bd9d-bc832128d6aa"
   },
   "source": [
    "### 2-3-3. Learning Rate Schedulers\n",
    "\n",
    "Without using the `transformers.Trainer` class, we still can achieve the same results with PyTorch as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605fa8d-5da0-43b7-a79d-6baf15eb2f7c",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e451d9ec2f8447b0a661e82cd2a7c975"
     ]
    },
    "id": "c605fa8d-5da0-43b7-a79d-6baf15eb2f7c",
    "outputId": "f9b14b8c-c052-49e9-ed4f-5c947171ab4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e451d9ec2f8447b0a661e82cd2a7c975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Remove the columns corresponding to values the model does not expect\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Rename the `label` column to `labels`\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# Set the format of datasets so that they return `torch.Tensor` instead of lists\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(tokenized_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad6175-2c8e-4fd6-864d-c4c992d79669",
   "metadata": {
    "id": "92ad6175-2c8e-4fd6-864d-c4c992d79669",
    "outputId": "57376e6c-f91d-4d1f-e826-0ed2de93509c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 81]), 'token_type_ids': torch.Size([8, 81]), 'attention_mask': torch.Size([8, 81])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df3601-a137-48a0-a0ef-066ff97113c7",
   "metadata": {
    "id": "d8df3601-a137-48a0-a0ef-066ff97113c7",
    "outputId": "18476fdc-4cdb-4824-9630-4b1f8a045e5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9363, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# All transformer models will return the loss when `labels` are provided\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351c3c3-cd54-4206-b9f0-001cf62b5060",
   "metadata": {
    "id": "d351c3c3-cd54-4206-b9f0-001cf62b5060",
    "outputId": "92fce5c1-fada-46a8-c4d3-55eb379339d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35141e0b-883f-4f36-82e1-b31e23448a38",
   "metadata": {
    "id": "35141e0b-883f-4f36-82e1-b31e23448a38",
    "outputId": "690aed15-69b5-4263-c939-d158d9f2cb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "# The learning rate scheduler used by default is a linear decay\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7b468-2813-4925-b4f7-d703cb6d4047",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d8b113cc52124b07a7d0c3065abce360"
     ]
    },
    "id": "e7f7b468-2813-4925-b4f7-d703cb6d4047",
    "outputId": "b6269f67-c6ef-4482-a6f6-f730e476fe16"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b113cc52124b07a7d0c3065abce360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device agnostic code\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# The training oop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf91954-1b41-40c9-a795-c35233d9ae50",
   "metadata": {
    "id": "fbf91954-1b41-40c9-a795-c35233d9ae50",
    "outputId": "2367ca2a-8c9d-41d4-aa92-3791c4b10647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8259803921568627, 'f1': 0.8777969018932874}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# The evaluation loop\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c7454-32e8-4881-b76b-a09e8870005c",
   "metadata": {
    "id": "505c7454-32e8-4881-b76b-a09e8870005c",
    "outputId": "495ae9b8-c7fd-4793-e7b2-3346a761bc05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "# Enable distributed training with `accelerate`\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocess data\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Remove the columns corresponding to values the model does not expect\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Rename the `label` column to `labels`\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# Set the format of datasets so that they return `torch.Tensor` instead of lists\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Training\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81f899-c58d-4602-a6cf-2371236ffab1",
   "metadata": {
    "id": "ee81f899-c58d-4602-a6cf-2371236ffab1",
    "outputId": "103949ac-e52b-4084-886e-e4780081354d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: accelerate config default [-h]\n",
      "                                                    [--config_file SAVE_LOCATION]\n",
      "                                                    [--mixed_precision {no,fp16,bf16}]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --config_file SAVE_LOCATION, --config-file SAVE_LOCATION\n",
      "                        The path to use to store the config file. Will default\n",
      "                        to a file named default_config.yaml in the cache\n",
      "                        location, which is the content of the environment\n",
      "                        `HF_HOME` suffixed with 'accelerate', or if you don't\n",
      "                        have such an environment variable, your cache\n",
      "                        directory ('~/.cache' or the content of\n",
      "                        `XDG_CACHE_HOME`) suffixed with 'huggingface'.\n",
      "  --mixed_precision {no,fp16,bf16}, --mixed-precision {no,fp16,bf16}\n",
      "                        Whether or not to use mixed precision training. Choose\n",
      "                        between FP16 and BF16 (bfloat16) training. BF16\n",
      "                        training is only supported on Nvidia Ampere GPUs and\n",
      "                        PyTorch 1.10 or later.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a86e43-c5b4-4503-81c0-ff69ed821a23",
   "metadata": {
    "id": "14a86e43-c5b4-4503-81c0-ff69ed821a23",
    "outputId": "6041c946-b43d-424f-9d43-428e4b592a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/yungshun317/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12330392-1039-49c3-a7e8-63db7fbe9acd",
   "metadata": {
    "id": "12330392-1039-49c3-a7e8-63db7fbe9acd",
    "outputId": "b1afd5a2-6c14-4dd8-f65e-2d277abdfb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"debug\": false,\n",
      "  \"distributed_type\": \"NO\",\n",
      "  \"downcast_bf16\": false,\n",
      "  \"enable_cpu_affinity\": false,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"mixed_precision\": \"no\",\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 1,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"same_network\": false,\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"use_cpu\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/yungshun317/.cache/huggingface/accelerate/default_config.yaml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c64485-b1e1-41ca-a7c2-a7010ad81a52",
   "metadata": {
    "id": "e3c64485-b1e1-41ca-a7c2-a7010ad81a52",
    "outputId": "c71962e1-f743-4998-923c-6f57b6e22588"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-02 03:31:05.991312: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 03:31:05.998033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 03:31:06.005568: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 03:31:06.007852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 03:31:06.013439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 03:31:06.417390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1377/1377 [01:02<00:00, 21.94it/s]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91997bbe-a7ab-4908-be80-ca69a9005dd5",
   "metadata": {
    "id": "91997bbe-a7ab-4908-be80-ca69a9005dd5"
   },
   "source": [
    "### 2-3-4. Text Classification with DistilBERT\n",
    "Fine-tune DistilBERT for sequence classification also on GLUE SST-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f8b4f-369d-4ca7-af24-5cbf68708be1",
   "metadata": {
    "id": "2d1f8b4f-369d-4ca7-af24-5cbf68708be1"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292480eb-6d72-41d1-ba77-c2cc5a7b4377",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a13e40586bb44aba9bf4a3fa67469a3d",
      "b84bf07de1774c06b385611cd277728d",
      "9309390dac454bcc953ec69d3d171ddc",
      "7530e8bcb57f41359357d6cd0f96977a",
      "04f7b3a3bba64b9e8b8a88a206b563b7",
      "a28d6f2199864ffe90be76f8c262b14d",
      "f358d052bfdf46408bdd70b0b8e1306f"
     ]
    },
    "id": "292480eb-6d72-41d1-ba77-c2cc5a7b4377",
    "outputId": "cff2a390-6b39-4643-d165-d1b02b85bb72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13e40586bb44aba9bf4a3fa67469a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84bf07de1774c06b385611cd277728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9309390dac454bcc953ec69d3d171ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7530e8bcb57f41359357d6cd0f96977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f7b3a3bba64b9e8b8a88a206b563b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28d6f2199864ffe90be76f8c262b14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f358d052bfdf46408bdd70b0b8e1306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486156e-fe89-4804-87db-d8da07f370bc",
   "metadata": {
    "id": "5486156e-fe89-4804-87db-d8da07f370bc",
    "outputId": "98957184-a8a2-4831-da9b-020b52352b96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f1878-bee8-4048-953c-d59f84291b96",
   "metadata": {
    "id": "a25f1878-bee8-4048-953c-d59f84291b96",
    "outputId": "79206fd5-e812-4c4c-f67e-3952fd01bd82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "idx: int32\n",
       "----\n",
       "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
       "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
       "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558f922-6ad9-465f-bd7c-2e305584f503",
   "metadata": {
    "id": "4558f922-6ad9-465f-bd7c-2e305584f503",
    "outputId": "8a35ce64-933e-40ff-cd71-1bd2ee85c114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cf0a4-39bf-4ab7-9840-bfc085c68808",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "915d9bc28cf64d9b9b16e4a089079423",
      "6f22131436804f63b052dec0264053b8",
      "f16dbce17f844673a53dc12abd43ec6d",
      "c999c9cd39bb4411934371883556a53c"
     ]
    },
    "id": "da8cf0a4-39bf-4ab7-9840-bfc085c68808",
    "outputId": "1e9b401d-6cfa-4f67-eb94-7ae09474ae56"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d9bc28cf64d9b9b16e4a089079423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22131436804f63b052dec0264053b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16dbce17f844673a53dc12abd43ec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c999c9cd39bb4411934371883556a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'])\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d515f6b-3769-43e6-916f-fca294f7a22e",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e6bde47a18bc4a91b372330e7c17925b",
      "f414a499ef40440f8c58716f4b50b467",
      "0f0ea13c27744a31a5ede242980c28f9"
     ]
    },
    "id": "5d515f6b-3769-43e6-916f-fca294f7a22e",
    "outputId": "72a58e78-43d0-4b9a-bd0a-8451859b14c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bde47a18bc4a91b372330e7c17925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f414a499ef40440f8c58716f4b50b467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0ea13c27744a31a5ede242980c28f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "type(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f2797-056f-43bc-9514-8248d4ff314c",
   "metadata": {
    "id": "b95f2797-056f-43bc-9514-8248d4ff314c",
    "outputId": "100494a0-0440-4a56-f466-e4e33069dddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=my_trainer/runs/Aug12_22-42-33_yungshun317-Titan-18-HX-A14VIG,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=my_trainer,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=my_trainer,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.EPOCH,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  'my_trainer',\n",
    "  eval_strategy='epoch',\n",
    "  save_strategy='epoch',\n",
    "  num_train_epochs=1,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6cae1-8ecd-4efa-8e18-233b580ef77d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d54dd42b8b634e5e84eb7c42ebad2d4f"
     ]
    },
    "id": "54e6cae1-8ecd-4efa-8e18-233b580ef77d",
    "outputId": "fbaa58ab-15b1-4c9a-d5f2-e422f16cb806"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54dd42b8b634e5e84eb7c42ebad2d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb643e8-9dc2-4091-9495-7e4d86a67b76",
   "metadata": {
    "id": "fdb643e8-9dc2-4091-9495-7e4d86a67b76",
    "outputId": "574dc76d-341e-4e5f-d130-3d00c825bbc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f728925-aff4-4841-bfe0-b92b30e8901c",
   "metadata": {
    "id": "9f728925-aff4-4841-bfe0-b92b30e8901c",
    "outputId": "54258cd4-3036-42c3-92d2-d384c3891797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "â”œâ”€DistilBertModel: 1-1                                  --\n",
       "â”‚    â””â”€Embeddings: 2-1                                  --\n",
       "â”‚    â”‚    â””â”€Embedding: 3-1                              23,440,896\n",
       "â”‚    â”‚    â””â”€Embedding: 3-2                              393,216\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-3                              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-4                                --\n",
       "â”‚    â””â”€Transformer: 2-2                                 --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-5                             42,527,232\n",
       "â”œâ”€Linear: 1-2                                           590,592\n",
       "â”œâ”€Linear: 1-3                                           1,538\n",
       "â”œâ”€Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde041c-e5f6-4183-91bd-37e47ecc0606",
   "metadata": {
    "id": "ffde041c-e5f6-4183-91bd-37e47ecc0606"
   },
   "outputs": [],
   "source": [
    "# !pip3 install evaluate\n",
    "import evaluate\n",
    "\n",
    "# Metric for validation error\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c222023-bf0d-48b1-a846-349f0cfae16a",
   "metadata": {
    "id": "6c222023-bf0d-48b1-a846-349f0cfae16a",
    "outputId": "ad31fc2e-ba33-4c6f-da6f-8383bf65a675"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 03:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.509155</td>\n",
       "      <td>0.883028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.07386701579104694, metrics={'train_runtime': 194.6434, 'train_samples_per_second': 346.012, 'train_steps_per_second': 43.253, 'total_flos': 517212489917652.0, 'train_loss': 0.07386701579104694, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f5e9a-9529-4dbc-b21e-9baa13ff53e0",
   "metadata": {
    "id": "063f5e9a-9529-4dbc-b21e-9baa13ff53e0"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('models/finetuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb7889-f9f1-4ee3-96c3-ffb08eda8aab",
   "metadata": {
    "id": "38cb7889-f9f1-4ee3-96c3-ffb08eda8aab",
    "outputId": "b2a4b5dd-dfc0-4e99-d7eb-390456786386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9997863173484802}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = pipeline(\"sentiment-analysis\", model='models/finetuned_model', device=0)\n",
    "finetuned_model('This is such a great movie!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f822a-1fed-46a4-8fd4-de1ad309779c",
   "metadata": {
    "id": "292f822a-1fed-46a4-8fd4-de1ad309779c",
    "outputId": "48e9312d-5eb6-4357-fdb7-77513552b4bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9997292160987854}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model('This is such a bad movie!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f06db-45cf-4029-afe3-c7408fd4c263",
   "metadata": {
    "id": "d55f06db-45cf-4029-afe3-c7408fd4c263",
    "outputId": "a124db01-c57e-4bb5-ff36-500f654924c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat models/finetuned_model/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde7e38-f876-4bc6-8af4-37dedb9b5c3d",
   "metadata": {
    "id": "ddde7e38-f876-4bc6-8af4-37dedb9b5c3d",
    "outputId": "a332d8a1-693f-47e3-d79e-4845af1eb4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.988}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "n_samples = 500\n",
    "\n",
    "# The labels in test set of `glue/sst2` are `-1`\n",
    "X = raw_datasets['train'].data[\"sentence\"].to_pylist()[:n_samples]\n",
    "y = raw_datasets['train'].data[\"label\"].to_pylist()[:n_samples]\n",
    "\n",
    "results = finetuned_model(X)\n",
    "predictions = [0 if res[\"label\"] == \"LABEL_0\" else 1 for res in results]\n",
    "print(metric.compute(predictions=predictions, references=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09616e-5a92-4bc0-9f65-ef3cd4ffcc78",
   "metadata": {
    "id": "1a09616e-5a92-4bc0-9f65-ef3cd4ffcc78"
   },
   "source": [
    "# 3. Datasets\n",
    "\n",
    "Main classes:\n",
    "\n",
    "1. `datasets.arrow_dataset.Dataset`: The base class is backed by an Apache Arrow table.\n",
    "2. `datasets.dataset_dict.DatasetDict`: A dictionary with split names as keys & `datasets.arrow_dataset.Dataset` objects as values. Also has `datasets.arrow_dataset.Dataset`'s transform methods to process all the splits at once.\n",
    "\n",
    "## 3-1. Load\n",
    "1. `datasets.load_dataset()`\n",
    "\n",
    "[SQuAD-it](https://github.com/crux82/squad-it/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736f877-b46a-4394-bab1-78a900a3e61e",
   "metadata": {
    "id": "4736f877-b46a-4394-bab1-78a900a3e61e",
    "outputId": "000cc107-d5e6-4ccf-e4ca-a7504e717c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-02 13:01:26--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 20.27.177.113\n",
      "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2024-10-02 13:01:26--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: â€˜datasets/squad_it/SQuAD_it-train.json.gzâ€™\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  9.82MB/s    in 0.8s    \n",
      "\n",
      "2024-10-02 13:01:27 (9.82 MB/s) - â€˜datasets/squad_it/SQuAD_it-train.json.gzâ€™ saved [7725286/7725286]\n",
      "\n",
      "--2024-10-02 13:01:27--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 20.27.177.113\n",
      "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2024-10-02 13:01:27--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: â€˜datasets/squad_it/SQuAD_it-test.json.gzâ€™\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  3.69MB/s    in 0.3s    \n",
      "\n",
      "2024-10-02 13:01:28 (3.69 MB/s) - â€˜datasets/squad_it/SQuAD_it-test.json.gzâ€™ saved [1051245/1051245]\n",
      "\n",
      "datasets/squad_it/SQuAD_it-test.json.gz:\t 87.5% -- created datasets/squad_it/SQuAD_it-test.json\n",
      "datasets/squad_it/SQuAD_it-train.json.gz:\t 82.3% -- created datasets/squad_it/SQuAD_it-train.json\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz --directory-prefix=datasets/squad_it\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz --directory-prefix=datasets/squad_it\n",
    "!gzip -dkv datasets/squad_it/SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d9c43-03d2-4825-a8e4-066557528615",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8096ae7b70db4f3d954275890c5048ac"
     ]
    },
    "id": "a28d9c43-03d2-4825-a8e4-066557528615",
    "outputId": "5ee64867-ddd2-41a4-8b6e-25b1274c72e4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8096ae7b70db4f3d954275890c5048ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 442\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load a local dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"datasets/squad_it/SQuAD_it-train.json\", field=\"data\")\n",
    "print(squad_it_dataset)\n",
    "# View one of the examples by indexing into the `train` split\n",
    "# print(squad_it_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb2713-19d2-4d0d-ba65-dae760f3b97f",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "edd6a10859b14ac1826e617e79ed86fe",
      "50df031a47174a33bfd242f1a6f0d3c0"
     ]
    },
    "id": "5fdb2713-19d2-4d0d-ba65-dae760f3b97f",
    "outputId": "b46ab899-2a36-49bc-d65d-133de13018bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6a10859b14ac1826e617e79ed86fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df031a47174a33bfd242f1a6f0d3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 442\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Provide a dictionary to the `data_files` arguments\n",
    "data_files = {\"train\": \"datasets/squad_it/SQuAD_it-train.json\", \"test\": \"datasets/squad_it/SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "print(squad_it_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3508cc-cd65-49e9-b8db-c76b13c914a1",
   "metadata": {
    "id": "ec3508cc-cd65-49e9-b8db-c76b13c914a1",
    "outputId": "4d7472d5-172e-45e9-91f0-c663fe8c2876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 442\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load a remote dataset\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "print(squad_it_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c5ed0-2e06-422d-b34e-3073fc19002c",
   "metadata": {
    "id": "fb0c5ed0-2e06-422d-b34e-3073fc19002c"
   },
   "source": [
    "## 3-2. Data Wrangling\n",
    "The `datasets` library provides functions to manipulate the contents of `datasets.dataset_dict.DatasetDict` & `datasets.arrow_dataset.Dataset` objects.\n",
    "\n",
    "1. `shuffle(seed=None, generator=None, keep_in_memory=False, load_from_cache_file=None, indices_cache_file_name=None, writer_batch_size=1000, new_fingerprint=None)`:\n",
    "2. `select(indices, keep_in_memory=False, indices_cache_file_name=None, writer_batch_size=1000, new_fingerprint=None)`:\n",
    "3. `unique()`:\n",
    "4. `sort(column_names, reverse=False, null_placement=\"at_end\", keep_in_memory=False, load_from_cache_file=None, indices_cache_file_name=None, writer_batch_size=1000, new_fingerprint=None)`:\n",
    "5. `map(function=None, with_indices=False, with_rank=False, input_columns=None, batched=False, batch_size=1000, drop_last_batch=False, remove_columns=None, keep_in_memory=False, load_from_cache_file=None, cache_file_names=None, writer_batch_size=1000, features=None, disable_nullable=False, fn_kwargs=None, num_proc=None, desc=None)`: Applies a function to all the elements in the table, individually or in batches, and updates the table if function does updated examples. The transformation is applied to all the datasets of the dataset dictionary. Setting `batched` to `True` & `num_proc`, maximum number of processes when generating cache, could achieve a massive performance overhaul.\n",
    "7. `filter()`:\n",
    "\n",
    "Python `html` module:\n",
    "\n",
    "7. `html.unescape()`: Unescapes HTML character codes.\n",
    "\n",
    "[Drug Reviews](https://archive.ics.uci.edu/dataset/462/drug+review+dataset+drugs+com) from UC Irvine Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63862a93-efc4-42e5-b9c4-209fca591d64",
   "metadata": {
    "id": "63862a93-efc4-42e5-b9c4-209fca591d64",
    "outputId": "6907ee4c-b13e-43bd-ea2d-75711413a39e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-02 19:53:44--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: â€˜datasets/drug_reviews/drugsCom_raw.zipâ€™\n",
      "\n",
      "drugsCom_raw.zip        [           <=>      ]  41.00M  2.66MB/s    in 19s     \n",
      "\n",
      "2024-10-02 19:54:03 (2.21 MB/s) - â€˜datasets/drug_reviews/drugsCom_raw.zipâ€™ saved [42989872]\n",
      "\n",
      "Archive:  datasets/drug_reviews/drugsCom_raw.zip\n",
      "  inflating: datasets/drug_reviews/drugsComTest_raw.tsv  \n",
      "  inflating: datasets/drug_reviews/drugsComTrain_raw.tsv  \n"
     ]
    }
   ],
   "source": [
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\" --directory-prefix=datasets/drug_reviews\n",
    "!unzip datasets/drug_reviews/drugsCom_raw.zip -d datasets/drug_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de2253-b47b-4221-95bf-86da0eb1b12a",
   "metadata": {
    "id": "d8de2253-b47b-4221-95bf-86da0eb1b12a",
    "outputId": "ebd844a9-cffb-4847-b3d5-7b101cf2d7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 161297\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 53766\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"datasets/drug_reviews/drugsComTrain_raw.tsv\", \"test\": \"datasets/drug_reviews/drugsComTest_raw.tsv\"}\n",
    "# Separated by tab character\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "print(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2eec6a-3011-4d1e-9bcc-87f9a36c6bdc",
   "metadata": {
    "id": "ef2eec6a-3011-4d1e-9bcc-87f9a36c6bdc",
    "outputId": "84331c59-ce23-441d-8535-e494d710986d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab a small random sample for data analysis\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9eb76-2470-4ff8-8fd1-afce67e54a36",
   "metadata": {
    "id": "80c9eb76-2470-4ff8-8fd1-afce67e54a36"
   },
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    # Verify the number of `ids` matches the number of rows in each split\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71100464-2b07-49a4-9c47-385a37939aad",
   "metadata": {
    "id": "71100464-2b07-49a4-9c47-385a37939aad",
    "outputId": "61e797f0-9273-47bc-e5d6-259571c8e857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 161297\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 53766\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "print(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ecaf6-ccc7-4c76-ae28-7590088eb1b9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9280fd6548ea4e09879b68e1995e54ae",
      "65813599cf234da8bc4d9896988e7027",
      "f4d419c101af443082edb1359db23a63",
      "e6f4ac76ee924d55a0e680ce1e188f95"
     ]
    },
    "id": "ca4ecaf6-ccc7-4c76-ae28-7590088eb1b9",
    "outputId": "b3b5570e-64b3-4775-88f1-c1eea0011609"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9280fd6548ea4e09879b68e1995e54ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65813599cf234da8bc4d9896988e7027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d419c101af443082edb1359db23a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f4ac76ee924d55a0e680ce1e188f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left ventricular dysfunction', 'adhd', 'birth control']\n"
     ]
    }
   ],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "# Some of the entries in the `condition` column are `None`, which cannot be lowercased as they are not strings\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "\n",
    "# Check that lowercasing worked\n",
    "print(drug_dataset[\"train\"][\"condition\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38414ab-839b-4d08-b9c5-35d768b3bba8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bbde27f68e8849f782431a5d60ddab78",
      "8f73b3fa841945769b333b167b55221a"
     ]
    },
    "id": "b38414ab-839b-4d08-b9c5-35d768b3bba8",
    "outputId": "2bf45a72-bf46-4046-dc05-6bf32033fba1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbde27f68e8849f782431a5d60ddab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f73b3fa841945769b333b167b55221a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient_id': 206461, 'drugName': 'Valsartan', 'condition': 'left ventricular dysfunction', 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"', 'rating': 9.0, 'date': 'May 20, 2012', 'usefulCount': 27, 'review_length': 17}\n"
     ]
    }
   ],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# Inspect the first training example\n",
    "print(drug_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cac739-e63e-4623-b41f-20affd283b85",
   "metadata": {
    "id": "44cac739-e63e-4623-b41f-20affd283b85",
    "outputId": "8d3e917e-d963-4d85-ff87-31ff2dd1df1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient_id': [111469, 13653, 53602], 'drugName': ['Ledipasvir / sofosbuvir', 'Amphetamine / dextroamphetamine', 'Alesse'], 'condition': ['hepatitis c', 'adhd', 'birth control'], 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'], 'rating': [10.0, 10.0, 10.0], 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'], 'usefulCount': [41, 3, 0], 'review_length': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(drug_dataset[\"train\"].sort(\"review_length\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15580d-06d1-44a5-970c-90f0cc3df63d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "29f1ecb17132460f9de85e43bce36db8",
      "7bf8a3dae3524f658a85f3049826dc6a"
     ]
    },
    "id": "da15580d-06d1-44a5-970c-90f0cc3df63d",
    "outputId": "37b119f4-b7a4-46c9-eabf-ad0cd4d2c146"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f1ecb17132460f9de85e43bce36db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf8a3dae3524f658a85f3049826dc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0750c-9709-4823-9a4e-f110d0486946",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f8f9dcd2960f472fa7e27dc37969bc93",
      "0e64dc151ba94d33b9455a49bdd84871"
     ]
    },
    "id": "5bb0750c-9709-4823-9a4e-f110d0486946",
    "outputId": "0c23424d-b7c2-4eea-8dfc-9bba83d7b879"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f9dcd2960f472fa7e27dc37969bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e64dc151ba94d33b9455a49bdd84871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 138514\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# `html.unescape()`\n",
    "import html\n",
    "\n",
    "# Unescape HTML characters in the corpus\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n",
    "print(drug_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a49094-e045-418d-9095-7cdd0b232851",
   "metadata": {
    "id": "83a49094-e045-418d-9095-7cdd0b232851"
   },
   "source": [
    "### 3-2-1. Truncation & Overflowing Tokens\n",
    "The mismatched length problem caused by applying truncation & setting `return_overflowing_tokens=True` in tokenizers can be dealt with the following ways:\n",
    "- Remove the original columns by setting the `remove_columns` argument in the `datasets.dataset_dict.DatasetDict.map` method.\n",
    "- Make the original columns the same size as the new ones. This can be achieved by taking advantage of the `overflow_to_sample_mappings` field the tokenizer returns. We can associate each key present in the original dataset with a list of values of the right size by repeating the values of each example as many times as it generates new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bcb42-2c6b-4166-b2f0-dd1289372aaa",
   "metadata": {
    "id": "1b9bcb42-2c6b-4166-b2f0-dd1289372aaa",
    "outputId": "d0665423-ea0c-453a-98f2-e047b7439226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with Overflowing Tokens:\n",
      "{'input_ids': [[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0]}\n",
      "Overflowing Example Size after Tokenization: [128, 49]\n",
      "\n",
      "Example without Overflowing Tokens:\n",
      "{'input_ids': [[101, 107, 1188, 1110, 1139, 1148, 1159, 1606, 1251, 1532, 1104, 3485, 1654, 119, 146, 112, 182, 5171, 146, 1355, 1114, 1103, 10085, 117, 146, 1138, 1151, 1113, 1122, 1111, 129, 1808, 119, 1335, 1148, 1135, 10558, 1139, 181, 21883, 2572, 1133, 1115, 4841, 25984, 119, 1109, 1178, 1205, 5570, 1110, 1115, 1122, 1189, 1139, 6461, 2039, 113, 126, 118, 127, 1552, 1106, 1129, 6129, 114, 146, 1215, 1106, 1178, 1138, 6461, 1111, 124, 118, 125, 1552, 12477, 1775, 1145, 1189, 1139, 172, 4515, 3491, 5827, 1111, 1103, 1148, 1160, 1552, 1104, 1139, 1669, 117, 146, 1309, 1125, 172, 4515, 3491, 1196, 1606, 3485, 1654, 119, 2189, 1190, 1115, 1107, 2816, 1114, 1103, 10085, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0]}\n",
      "Common Example Size after Tokenization: [116]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Set `max_length=128` for truncation & `return_overflowing_tokens=True` to tokenize long reviews into more than one example\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "# tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "# ArrowInvalid: Column 8 named input_ids expected length 1000 but got length 1463\n",
    "# Because `batch_size=1000`\n",
    "\n",
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "print(\"Example with Overflowing Tokens:\")\n",
    "print(result)\n",
    "print(\"Overflowing Example Size after Tokenization:\", [len(inp) for inp in result[\"input_ids\"]])\n",
    "# The first example became two features because it was tokenized to more than the maximum number of tokens we specified\n",
    "\n",
    "result = tokenize_and_split(drug_dataset[\"train\"][2])\n",
    "print(\"\\nExample without Overflowing Tokens:\")\n",
    "print(result)\n",
    "print(\"Common Example Size after Tokenization:\", [len(inp) for inp in result[\"input_ids\"]])\n",
    "# The example shorter than `max_length` will be only one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105c4a7-a207-45d5-8519-91d72b5a593d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "62a30ce4b26046d0ab01ff96b58ac32d"
     ]
    },
    "id": "a105c4a7-a207-45d5-8519-91d72b5a593d",
    "outputId": "db7c28a0-bc56-4b1f-ae55-5230d62fef24"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a30ce4b26046d0ab01ff96b58ac32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset Size: 206772\n",
      "Original Dataset Size: 138514\n"
     ]
    }
   ],
   "source": [
    "# Remove the columns from the old dataset\n",
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
    ")\n",
    "print(\"Tokenized Dataset Size:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"Original Dataset Size:\", len(drug_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143ed96-3438-4913-b5e8-1d6e10dac1c7",
   "metadata": {
    "id": "6143ed96-3438-4913-b5e8-1d6e10dac1c7",
    "outputId": "f497218e-fb03-41fb-8f16-c653b1d4eeb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset Column Names:\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping']\n",
      "\n",
      "Original Dataset Column Names:\n",
      "['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Dataset Column Names:\")\n",
    "print(tokenized_dataset[\"train\"].column_names)\n",
    "\n",
    "# If not remove them, their lengths will be different from the lengths of new fields after tokenization\n",
    "print(\"\\nOriginal Dataset Column Names:\")\n",
    "print(drug_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20fa3a-783e-4dfc-aa64-0372299c7873",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2590c151ccfa4e9a959a7c8f5ea3c790",
      "52a3201bbd6b47e1ba7e2933c3950ce3"
     ]
    },
    "id": "9c20fa3a-783e-4dfc-aa64-0372299c7873",
    "outputId": "61c9f822-97d0-4da4-c0da-f310bc90ed06"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2590c151ccfa4e9a959a7c8f5ea3c790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a3201bbd6b47e1ba7e2933c3950ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 206772\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 68876\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# `overflow_to_sample_mapping` gives us a mapping from a new feature index to the index of the sample it originated from\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    # Update the original columns to use new keys & values\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c523008-da39-450a-b085-cb6669fc3ed0",
   "metadata": {
    "id": "2c523008-da39-450a-b085-cb6669fc3ed0"
   },
   "source": [
    "### 3-2-2. Conversion\n",
    "\n",
    "1. `set_format(type=None, columns=one, output_all_columns=False, **format_kwargs)`: Enables the conversion between various third-party libraries like `pandas`. This function only changes the output format of `datasets.dataset_dict.DatasetDict` or `datasets.arrow_dataset.Dataset` so that you can easily switch to another format without affecting the underlying data format. Output type can be selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caef83-b23d-4438-96a6-0bf2f67c79c0",
   "metadata": {
    "id": "e6caef83-b23d-4438-96a6-0bf2f67c79c0",
    "outputId": "b4940caf-ca65-475c-f542-f11ca822253a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                  drugName                     condition  \\\n",
       "0       95260                Guanfacine                          adhd   \n",
       "1       92703                    Lybrel                 birth control   \n",
       "2      138000                Ortho Evra                 birth control   \n",
       "3       35696  Buprenorphine / naloxone             opiate dependence   \n",
       "4      155963                    Cialis  benign prostatic hyperplasia   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion between `pandas` & `datasets`\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "drug_dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7255c7-17bc-42e1-82b9-a0b2eb6a25cf",
   "metadata": {
    "id": "9d7255c7-17bc-42e1-82b9-a0b2eb6a25cf",
    "outputId": "43013ca7-fd3a-4ee7-c406-1becd6199a65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff5b3b-a458-4203-9089-5dd223484582",
   "metadata": {
    "id": "d7ff5b3b-a458-4203-9089-5dd223484582",
    "outputId": "ccfc34a5-701d-41f8-a128-caf9ebd6a9be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                  drugName                     condition  \\\n",
       "0       95260                Guanfacine                          adhd   \n",
       "1       92703                    Lybrel                 birth control   \n",
       "2      138000                Ortho Evra                 birth control   \n",
       "3       35696  Buprenorphine / naloxone             opiate dependence   \n",
       "4      155963                    Cialis  benign prostatic hyperplasia   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = drug_dataset[\"train\"][:]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990eea5-a01e-41aa-9175-e27b85014f84",
   "metadata": {
    "id": "a990eea5-a01e-41aa-9175-e27b85014f84",
    "outputId": "578906e9-01de-4024-c3c0-73f4316764fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a8783-c614-4b83-b4a3-80bb78a22bb6",
   "metadata": {
    "id": "523a8783-c614-4b83-b4a3-80bb78a22bb6",
    "outputId": "978e49e4-981b-4ec6-f18a-236918208cec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birth control</td>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acne</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pain</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency  count\n",
       "0  birth control  27655\n",
       "1     depression   8023\n",
       "2           acne   5209\n",
       "3        anxiety   4991\n",
       "4           pain   4744"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3d35-7f9f-4e0d-8e2c-36c2daeead90",
   "metadata": {
    "id": "d34a3d35-7f9f-4e0d-8e2c-36c2daeead90",
    "outputId": "bb76badd-e604-484a-81b3-07ba39df88c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['frequency', 'count'],\n",
      "    num_rows: 819\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "print(freq_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3baf71-87dc-4340-8911-187b8160a963",
   "metadata": {
    "id": "2d3baf71-87dc-4340-8911-187b8160a963",
    "outputId": "604dbc78-7661-49ae-cb58-66540d28e764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 138514\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "drug_dataset.reset_format()\n",
    "print(drug_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620abf10-f1c3-40a6-a39f-d5a5ad7ddbaf",
   "metadata": {
    "id": "620abf10-f1c3-40a6-a39f-d5a5ad7ddbaf"
   },
   "source": [
    "### 3-2-3. Validation\n",
    "1. `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2a332-f4f3-4c7f-b2a0-16c69ad489fe",
   "metadata": {
    "id": "0bf2a332-f4f3-4c7f-b2a0-16c69ad489fe",
    "outputId": "06b1f58d-119f-4b0b-eae9-cceec7bb7f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "print(drug_dataset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7a5f2-a7c7-4bfd-8ed4-2522c8eba5d8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e53bddc22d2a4d78ad4e3e675d524c0e",
      "867d886247d448af96b6437200a05f4a",
      "d975cd141514426cab246b4a25ee1662"
     ]
    },
    "id": "f4b7a5f2-a7c7-4bfd-8ed4-2522c8eba5d8",
    "outputId": "b71fe66c-a486-46e0-c42d-7880b5ab0526"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53bddc22d2a4d78ad4e3e675d524c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867d886247d448af96b6437200a05f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d975cd141514426cab246b4a25ee1662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset_clean.save_to_disk(\"datasets/drug_reviews_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d586a-0f1f-4aa0-8d1e-59c83ee8c5d9",
   "metadata": {
    "id": "7b5d586a-0f1f-4aa0-8d1e-59c83ee8c5d9",
    "outputId": "2fe08887-c225-4e50-ad04-0ebb7eb0dab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_reviews_splits\n",
      "â”œâ”€â”€ train\n",
      "â”‚   â”œâ”€â”€ data-00000-of-00001.arrow\n",
      "â”‚   â”œâ”€â”€ dataset_info.json\n",
      "â”‚   â””â”€â”€ state.json\n",
      "â”œâ”€â”€ test\n",
      "â”‚   â”œâ”€â”€ data-00000-of-00001.arrow\n",
      "â”‚   â”œâ”€â”€ dataset_info.json\n",
      "â”‚   â””â”€â”€ state.json\n",
      "â”œâ”€â”€ validation\n",
      "â”‚   â”œâ”€â”€ data-00000-of-00001.arrow\n",
      "â”‚   â”œâ”€â”€ dataset_info.json\n",
      "â”‚   â””â”€â”€ state.json\n",
      "â””â”€â”€ dataset_dict.json\n",
      "\n",
      "3 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "# Visualize a directory tree\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "\n",
    "space = '    '\n",
    "branch = 'â”‚   '\n",
    "tee = 'â”œâ”€â”€ '\n",
    "last = 'â””â”€â”€ '\n",
    "\n",
    "def tree(dir_path: Path, level: int=-1, limit_to_directories: bool=False,\n",
    "         length_limit: int=1000):\n",
    "    \"\"\"Given a directory Path object print a visual tree structure\"\"\"\n",
    "    dir_path = Path(dir_path) # accept string coerceable to Path\n",
    "    files = 0\n",
    "    directories = 0\n",
    "    def inner(dir_path: Path, prefix: str='', level=-1):\n",
    "        nonlocal files, directories\n",
    "        if not level:\n",
    "            return # 0, stop iterating\n",
    "        if limit_to_directories:\n",
    "            contents = [d for d in dir_path.iterdir() if d.is_dir()]\n",
    "        else:\n",
    "            contents = list(dir_path.iterdir())\n",
    "        pointers = [tee] * (len(contents) - 1) + [last]\n",
    "        for pointer, path in zip(pointers, contents):\n",
    "            if path.is_dir():\n",
    "                yield prefix + pointer + path.name\n",
    "                directories += 1\n",
    "                extension = branch if pointer == tee else space\n",
    "                yield from inner(path, prefix=prefix+extension, level=level-1)\n",
    "            elif not limit_to_directories:\n",
    "                yield prefix + pointer + path.name\n",
    "                files += 1\n",
    "    print(dir_path.name)\n",
    "    iterator = inner(dir_path, level=level)\n",
    "    for line in islice(iterator, length_limit):\n",
    "        print(line)\n",
    "    if next(iterator, None):\n",
    "        print(f'... length_limit, {length_limit}, reached, counted:')\n",
    "    print(f'\\n{directories} directories' + (f', {files} files' if files else ''))\n",
    "\n",
    "tree(Path.home() / \"workspace/py/torch-nlp/datasets/drug_reviews_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a187397-0609-4933-beea-7964117fe051",
   "metadata": {
    "id": "5a187397-0609-4933-beea-7964117fe051",
    "outputId": "c2626842-254f-48a8-a7a0-54eb420e8479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"datasets/drug_reviews_splits\")\n",
    "print(drug_dataset_reloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8af759-1366-4c0a-83da-02fd5327b207",
   "metadata": {
    "id": "fe8af759-1366-4c0a-83da-02fd5327b207"
   },
   "source": [
    "## 3-3. Memory Mapping & Streaming\n",
    "**Memory Mapping** allows programs to work with datasets larger than physical RAM while also allowing multiple programs running concurrently to believe they have access to the entire memory space or more of the machine by on-disk cache for fast lookup.\n",
    "\n",
    "Popular open source datasets for fine-tuning large language models (LLMs).\n",
    "- The [Wikimedia Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) dataset is built from the [Wikipedia Dumps](https://dumps.wikimedia.org/) with one subset per language, each containing a single train split.\n",
    "- [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50658787-9c3a-4a8f-978d-3ebeb1402621",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6a6192eb5b944edf925b56afe7c6ac60",
      "408f0e8f05704d21ab6ff979722424df",
      "5b1f731b9be94a5bba9339fe6ae37da1"
     ]
    },
    "id": "50658787-9c3a-4a8f-978d-3ebeb1402621",
    "outputId": "2a1390e8-3437-4782-eaaa-52da1d679777"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6192eb5b944edf925b56afe7c6ac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/36.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f0e8f05704d21ab6ff979722424df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f731b9be94a5bba9339fe6ae37da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/41 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpsutil\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtimeit\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mem_before \u001b[38;5;241m=\u001b[39m mem_before \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess(os\u001b[38;5;241m.\u001b[39mgetpid())\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m wiki \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20220301.en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m mem_after \u001b[38;5;241m=\u001b[39m mem_after \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess(os\u001b[38;5;241m.\u001b[39mgetpid())\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAM memory used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(mem_after\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mmem_before)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/load.py:2616\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2616\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2626\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2627\u001b[0m )\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/builder.py:1029\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1028\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/builder.py:1102\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m   1101\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1102\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py:962\u001b[0m, in \u001b[0;36mWikipedia._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    957\u001b[0m parquet_urls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    958\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-of-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_shards)\n\u001b[1;32m    960\u001b[0m ]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# Use dictionary since testing mock always returns the same result.\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparquet_urls\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m files \u001b[38;5;241m=\u001b[39m downloaded_files[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    964\u001b[0m is_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/download/download_manager.py:257\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    255\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 257\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    267\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/utils/py_utils.py:495\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m     num_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[1;32m    494\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 495\u001b[0m         \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_proc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m<\u001b[39m parallel_min_length:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/datasets/download/download_manager.py:300\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     max_workers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    297\u001b[0m         config\u001b[38;5;241m.\u001b[39mHF_DATASETS_MULTITHREADING_MAX_WORKERS \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    298\u001b[0m     )  \u001b[38;5;66;03m# enable multithreading if files are small\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# contains the ranks of subprocesses\u001b[39;49;00m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_DATASETS_STACK_MULTIPROCESSING_DOWNLOAD_PROGRESS_BARS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    316\u001b[0m     ]\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:49\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m lock_name \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlock_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 238\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os, psutil, timeit\n",
    "\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\n",
    "mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "print(f\"RAM memory used: {(mem_after - mem_before)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e700b-fde1-4bbc-b4d3-df297da45610",
   "metadata": {
    "id": "c19e700b-fde1-4bbc-b4d3-df297da45610"
   },
   "outputs": [],
   "source": [
    "code_snippet = \"\"\"batch_size = 1000\n",
    "for idx in range(0, len(wiki), batch_size):\n",
    "    batch = wiki[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "size = wiki.dataset_size / 2 ** 30\n",
    "print(f\"Iterated over {len(wiki)} examples (about {size_gb:.1f} GB) in {time:.1f}s, i.e. {size_gb / time:.3f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc4996-09ef-4ed2-8034-38ab364371ec",
   "metadata": {
    "id": "debc4996-09ef-4ed2-8034-38ab364371ec"
   },
   "outputs": [],
   "source": [
    "wiki_streamed = load_dataset(\"wikipedia\", \"20220301.en\", split='train', streaming=True)\n",
    "print(next(iter(wiki_streamed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93953f-559c-4683-9719-cc81e9641f92",
   "metadata": {
    "id": "9b93953f-559c-4683-9719-cc81e9641f92"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = wiki_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "print(next(iter(tokenized_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41b264-d56a-45ac-b56c-30d6edf013c9",
   "metadata": {
    "id": "2a41b264-d56a-45ac-b56c-30d6edf013c9"
   },
   "outputs": [],
   "source": [
    "shuffled_dataset = wiki_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "print(next(iter(shuffled_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cab24-b8ad-4f0f-85b9-efa100d56f44",
   "metadata": {
    "id": "b83cab24-b8ad-4f0f-85b9-efa100d56f44"
   },
   "outputs": [],
   "source": [
    "dataset_head = wiki_streamed.take(5)\n",
    "print(list(dataset_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549182c-cd03-42aa-8d47-dd5db5367fe1",
   "metadata": {
    "id": "9549182c-cd03-42aa-8d47-dd5db5367fe1"
   },
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98643654-16e5-4241-a43b-68aef1600c11",
   "metadata": {
    "id": "98643654-16e5-4241-a43b-68aef1600c11"
   },
   "outputs": [],
   "source": [
    "simple_wiki_streamed = load_dataset(\"wikipedia\", \"20220301.simple\", split='train', streaming=True)\n",
    "print(next(iter(simple_wiki_streamed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb5e67-c7ed-4c5b-a8a9-1b94af107345",
   "metadata": {
    "id": "a5bb5e67-c7ed-4c5b-a8a9-1b94af107345"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "combined_dataset = interleave_datasets([wiki_streamed, simple_wiki_streamed])\n",
    "print(list(islice(combined_dataset, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa629e-e062-42f3-beab-3a12bdc1bd8e",
   "metadata": {
    "id": "d6fa629e-e062-42f3-beab-3a12bdc1bd8e"
   },
   "source": [
    "## 3-4. Semantic Search with FAISS (Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ca809-e346-4984-86d3-fd4019e960de",
   "metadata": {
    "id": "0c4ca809-e346-4984-86d3-fd4019e960de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99ebb87e-bb08-491d-8f69-d8f4dc8a5f4f",
   "metadata": {
    "id": "99ebb87e-bb08-491d-8f69-d8f4dc8a5f4f"
   },
   "source": [
    "# 4. Subword Tokenization\n",
    "**Subword Tokenization** algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "## 4-1. Tokenizer Training\n",
    "\n",
    "**Training** a tokenizer is a statistical deterministic process, different from model training with stochastic gradient descent which is randomized by nature, that identifies which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm.\n",
    "\n",
    "1. `transformers.PreTrainedTokenizer.train_new_from_iterator(text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)`: A new tokenizer of the same type as the original one, trained on `text_iterator`.\n",
    "\n",
    "[CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1fae0-bb18-4258-9de1-e90a23356bfa",
   "metadata": {
    "id": "cbe1fae0-bb18-4258-9de1-e90a23356bfa",
    "outputId": "13cf09ee-10d9-4f80-9e03-4751753f7c04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "    num_rows: 412178\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac3a90-bd79-42c5-b1fd-fc8dfa60d927",
   "metadata": {
    "id": "29ac3a90-bd79-42c5-b1fd-fc8dfa60d927",
    "outputId": "371c9e7f-0106-46e5-bc04-64431c68d6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def has_elem(elem_ref):\n",
      "    \"\"\"\n",
      "    Has element?\n",
      "    :param elem_ref:\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    if not is_elem_ref(elem_ref):\n",
      "        return False\n",
      "    elif elem_ref[0] == ElemRefObj:\n",
      "        return hasattr(elem_ref[1], elem_ref[2])\n",
      "    elif elem_ref[0] == ElemRefArr:\n",
      "        return elem_ref[2] in elem_ref[1]\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9f96d-c948-4a12-b0ea-756072f0325e",
   "metadata": {
    "id": "e8a9f96d-c948-4a12-b0ea-756072f0325e"
   },
   "outputs": [],
   "source": [
    "# Create a list of lists of texts loading everything in memory\n",
    "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]\n",
    "\n",
    "# Generator expression\n",
    "def get_training_corpus():\n",
    "    return (raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000))\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c37ea-428e-4ebb-9aed-e33a61126616",
   "metadata": {
    "id": "0f1c37ea-428e-4ebb-9aed-e33a61126616"
   },
   "outputs": [],
   "source": [
    "# Define the generator by `yield`\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4247ab-e489-4738-9351-1711bc292c0a",
   "metadata": {
    "id": "6f4247ab-e489-4738-9351-1711bc292c0a",
    "outputId": "73dcb93a-624b-4148-a3b6-0d569f630c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä def', 'Ä __', 'init', '__', '(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'rand', 'n', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä self', '.', 'b', 'ias', 'Ä =', 'Ä torch', '.', 'zer', 'os', '(', 'output', '_', 'size', ')', 'ÄŠÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä def', 'Ä __', 'call', '__', '(', 'self', ',', 'Ä x', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'b', 'ias', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/envs/yungshun-py3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3a651-ef1d-4d15-8b01-a3c97c368e55",
   "metadata": {
    "id": "6eb3a651-ef1d-4d15-8b01-a3c97c368e55",
    "outputId": "f9d4e619-1df0-417a-f524-576649b06678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=52000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecd14c-4114-41c2-afec-a082eb921a4a",
   "metadata": {
    "id": "9aecd14c-4114-41c2-afec-a082eb921a4a",
    "outputId": "b56b6422-5051-4028-d5dd-2d97c23a2fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af1ef0-7238-4333-bca3-dac86fc10a8e",
   "metadata": {
    "id": "b0af1ef0-7238-4333-bca3-dac86fc10a8e",
    "outputId": "93a6eeae-73ba-4265-e373-e4af56786035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d58bb4-889b-45f8-99da-ce598a9a0475",
   "metadata": {
    "id": "a7d58bb4-889b-45f8-99da-ce598a9a0475",
    "outputId": "bb00af1a-a4ce-423e-f418-1378ece04918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b4252-e67f-403c-bf91-825f0e0e1b0c",
   "metadata": {},
   "source": [
    "## 4-2. Normalizer & Pre-Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "106b51f0-6955-4844-a40a-b705f6d7cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44c5cc73-04a4-4716-80b2-0b5b18501d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# Apply lowercasing & remove the accents\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f26e5cb6-d788-408d-aede-a036347d0499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]\n"
     ]
    }
   ],
   "source": [
    "# Pre-tokenization involves splitting on whitespace & punctuation\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d6f3d57-4db7-499d-8ae8-72c745d6dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)), ('?', (19, 20))]\n"
     ]
    }
   ],
   "source": [
    "# Keep the spaces and replace them with a `Ä ` symbol, enabling it to recover the original spaces if we decode the tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34bde363-206e-46df-a6b4-8c2e7d30f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece keeps spaces and replaces them with a specific token `_` but only splits on whitespace, not punctuation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7ae21-b823-4d4a-96f7-db00628e620f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28bb0f71-6a94-4295-acfa-3601f89df49a",
   "metadata": {
    "id": "28bb0f71-6a94-4295-acfa-3601f89df49a"
   },
   "source": [
    "## 4-7. Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b71e1-05de-43aa-8bb7-143563be5ae5",
   "metadata": {
    "id": "496b71e1-05de-43aa-8bb7-143563be5ae5",
    "outputId": "cb010ef1-40c5-4ffa-89b8-c8d8f5e6f4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JupyterLab v4.2.5\n",
      "/home/yungshun317/envs/yungshun-py3/share/jupyter/labextensions\n",
      "        jupyterlab_pygments v0.3.0 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_pygments)\n",
      "        jupyter-matplotlib v0.11.4 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m\n",
      "        @jupyter-widgets/jupyterlab-manager v5.0.13 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_widgets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3ad77-c4c7-4c03-8b52-43f8aac8bb53",
   "metadata": {
    "id": "b6a3ad77-c4c7-4c03-8b52-43f8aac8bb53"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc63f17e-8daa-4709-9fe6-da3852de20d5",
   "metadata": {
    "id": "dc63f17e-8daa-4709-9fe6-da3852de20d5"
   },
   "source": [
    "# 5. Text Generation with GPT-2\n",
    "Fine-tune GPT-2 for language modeling on [WikiText-2](https://paperswithcode.com/dataset/wikitext-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "672k5qgqS-zU",
   "metadata": {
    "id": "672k5qgqS-zU"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/gpt2-finetuned-wikitext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21LvQsNcKnkv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21LvQsNcKnkv",
    "outputId": "57553d3c-1877-449f-8451-2962fb1e8cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "xeOtO7szK-ZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeOtO7szK-ZN",
    "outputId": "3f382854-4360-4449-98ed-5ea2a6135a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a24cd7a-94aa-4885-9e26-5ccd61dc5eb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a24cd7a-94aa-4885-9e26-5ccd61dc5eb1",
    "outputId": "c858efd4-bbfa-434c-cfce-763d975c1ba5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31074d34-9951-4fff-b021-1925cadb222e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31074d34-9951-4fff-b021-1925cadb222e",
    "outputId": "37626c63-0a6c-473a-8f56-591b4eca4c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749afb70-8fd2-4bb6-b58d-c66904248812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "id": "749afb70-8fd2-4bb6-b58d-c66904248812",
    "outputId": "97dceead-e292-4ba2-95bf-1c76fe9e678b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 3D version ranked at No. 11 in the Japanese box office chart with a total of $ 285 @,@ 268 from a total of nine theaters . The 3D version won the Movie award for The Japanese Committee of the International 3D Society . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On Wheeler 's death , H.D. Sankalia of Deccan College , Pune , described him as \" well known among Old World archaeologists in the United States \" , particularly for his book Archaeology from the Earth and his studies of the Indus Valley Civilisation . In its 2013 obituary of the English archaeologist Mick Aston , British Archaeology magazine â€“ the publication of the Council for British Archaeology â€“ described Aston as \" the Mortimer Wheeler of our times \" because despite the strong differences between their personalities , both had done much to bring archaeology to the British public . However , writing in 2011 , Moshenska and Schadla @-@ Hall asserted that Wheeler 's reputation has not undergone significant revision among archaeologists , but that instead he had come to be remembered as \" a cartoonish and slightly eccentric figure \" whom they termed \" Naughty Morty \" . Carr described the Institute of Archaeology as \" one of the [ Wheeler ] couple 's most permanent memorials . \" \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In January 1844 , North Carolina Representative James Iver McKay , the chairman of the Committee on Ways and Means , solicited the views of Director Patterson on the gold dollar . Patterson had more of Gobrecht 's pattern dollar struck to show to committee members , again advising against a coin that if issued would be only about a half inch ( 13 mm ) in diameter . He told Treasury Secretary John C. Spencer that the only gold coins of that size in commerce , the Spanish and Colombian half @-@ escudos , were unpopular and had not been struck for more than twenty years . This seemed to satisfy the committee as nothing more was done for the time , and when a gold dollar was proposed again in 1846 , McKay 's committee recommended against it . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pernick and Wilder present examples which show that the \" clean tech revolution \" is already under way . Very large corporations such as GE , Toyota and Sharp , and investment firms such as Goldman Sachs are making multibillion @-@ dollar investments in clean technology . Emerging clean tech cities are seen to include Copenhagen , where wind power generates 20 percent of Denmark 's electricity , and Chicago , a leader in \" green \" buildings saving energy , heating and cooling costs . Statistics from the U.S. and from abroad , especially from China , India , Brazil , and Europe are presented . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O 'Malley considered a run for Governor of Maryland in the 2002 election , but decided not to run . He was reelected as Mayor of Baltimore in 2003 , and announced his candidacy for Governor in the 2006 election . The Baltimore Sun endorsed O 'Malley , saying : \" When he was first elected mayor in 1999 , the former two @-@ term city councilman inherited a city of rising crime , failing schools , and shrinking economic prospects . He was able to reverse course in all of these areas . \" The Washington Post criticized O 'Malley for \" not solv [ ing ] the problems of rampant crime and rough schools in Baltimore \" , but further said that \" he put a dent in them . \" . O 'Malley defeated incumbent Governor Bob Ehrlich 53 % -46 % in the November 7 , 2006 , general election . O 'Malley defeated Ehrlich in the 2010 election 56 % -42 % , receiving just over one million votes . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The league season concluded with an away match against divisional runners @-@ up Scunthorpe ; having gone two goals down York fought back to draw 2 â€“ 2 with goals scored by Brobbel and Andrew . This result meant York finished the season in seventh @-@ place in League Two , and would thus play fourth @-@ placed Fleetwood in the play @-@ off semi @-@ final on the back of a 17 @-@ match unbeaten run . York lost 1 â€“ 0 to Fleetwood in the first leg at Bootham Crescent ; the goal came from former York player Matty Blair in the 50th @-@ minute , who scored from close range after Antoni Sarcevic 's shot was blocked on the line . A 0 â€“ 0 draw away to Fleetwood in the second leg meant York were eliminated 1 â€“ 0 on aggregate , ending the prospect of a second promotion in three seasons . At an awards night held at York Racecourse , Oyebanjo was voted Clubman of the Year for 2013 â€“ 14 . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>= = Preservation = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\" Crazy in Love \" was the first song on BeyoncÃ© â€™ s set list on The BeyoncÃ© Experience in Los Angeles and the I Am ... Tour at several venues , including the Odyssey Arena in Belfast , the O2 Arena in London , and in Athens and Sydney . On August 5 , 2007 , BeyoncÃ© performed the song at Madison Square Garden in New York City . BeyoncÃ© emerged in a sparkling silver dress with a long train . She walked to the front of the stage , did a couple of snaps of her neck and then started singing \" Crazy in Love \" . She climbed a staircase where her all @-@ female band and three backup singers were positioned . The staircase moved forward in two places ; top part moved while the bottom poked out more . At the top of her staircase , she removed her train and returned to the main stage . Her backup singers followed and danced with BeyoncÃ© . After \" Crazy in Love \" , BeyoncÃ© performed a short rendition of Gnarls Barkley 's \" Crazy \" ( 2006 ) , singing , \" Who do you , who do you think you are ? / Ha , ha , ha , bless your soul . \" \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a58d1e-9b7b-4282-ad0a-bd2f49b60aeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4a58d1e-9b7b-4282-ad0a-bd2f49b60aeb",
    "outputId": "09f110d9-395b-47ad-8bd3-521860e4c231"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      " = Valkyria Chronicles III = \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Set the <EOS> token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "print(tokenized_datasets[\"train\"][1])\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e22c7e-0f65-4a38-bb4c-f6c415665dff",
   "metadata": {
    "id": "74e22c7e-0f65-4a38-bb4c-f6c415665dff"
   },
   "outputs": [],
   "source": [
    "# Not applied\n",
    "# block_size = tokenizer.model_max_length (1024)\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d65785-0f14-4bd5-a3af-0d469439646f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "f0a0fc8b64224c03a9a755d0e778cea7",
      "6f628caf7b4c483db287aaccfa05b692",
      "4d5d8c88f47446de84ac355b0d9ebf67",
      "b6aff30848f04050b374b74d31b5d118",
      "ad6a0b34d7304ebdb4d80e26ed970c52",
      "d4637aeefd584b3680cfb864b2a7a31f",
      "50133e6ee09242c6b73374a88522186b",
      "55cdfd405c9941fa96be66fa33d67fcf",
      "2af61a496ac649b698878e4beea51503",
      "285e14f856f34cc4853e1a2467f6dd8c",
      "5b77cf087a54447a9f86607c69625974",
      "bf6e871c574b41ccbd4489d557079600",
      "925eccf3d6714e70980db4ff44fceb0e",
      "4f9f569a9afc4dfaa0a5472967774e57",
      "5f419d2e829a45429d75c94a24eb30c8",
      "621bae44f2ec40298db67f9202f42ea5",
      "a23eedf9a5544c4892f8406329a20edc",
      "ff13c6cef378412c9d5c5e1d85476d56",
      "37d69fabff83445f910e34cc3e5a2b29",
      "06f24cba0c204846bb105ac9f0c11543",
      "4070430394f34f03ae5e06d802a3456b",
      "73913264c8b447289ee998c1c1b21be9",
      "09e3e6de490343e3a667f32a46c0ac0a",
      "3af65a7f3bc0443dbdbfa835db937d22",
      "6536af3dbcbb4dd09b36e445bc63241c",
      "8c4eebf1753b4b6cbb9a524dc56cf9a8",
      "8560cb4028a746ba9b5bea57229615c9",
      "45233cd1b94e41c9b1a48158a9fc9167",
      "d8baa13f917040d8a9b262f2e823eff6",
      "ab502411710b4b909f962c179446483c",
      "9abadbc42708404daf3050a7541416ad",
      "0f19642782324d41829a7d5797f6ad62",
      "5b9a0fb81dd5446f860124911ba5d705"
     ]
    },
    "id": "25d65785-0f14-4bd5-a3af-0d469439646f",
    "outputId": "2254dba5-615d-46dc-9cff-004821814c02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a0fc8b64224c03a9a755d0e778cea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6e871c574b41ccbd4489d557079600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e3e6de490343e3a667f32a46c0ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Not applied\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4\n",
    ")\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2396034d-337b-490e-8a9a-8ca10b9c8a22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2396034d-337b-490e-8a9a-8ca10b9c8a22",
    "outputId": "34ecb655-306c-42a9-c74f-2f4370e420f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "FF9_nd-mPVgZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF9_nd-mPVgZ",
    "outputId": "5af31521-4564-44ed-dcb5-21ea38fd35d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d9a596-5eef-49f4-b9e0-c7f147515f09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6d9a596-5eef-49f4-b9e0-c7f147515f09",
    "outputId": "ae85e725-d211-49f3-aa17-63b220e29997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=gpt2-finetuned-wikitext2/runs/Oct03_17-18-09_be42ef473a02,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=gpt2-finetuned-wikitext2,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=gpt2-finetuned-wikitext2,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=500,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7097b6a-5ba4-4cdd-b67f-be4f20968769",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "a7097b6a-5ba4-4cdd-b67f-be4f20968769",
    "outputId": "038b8557-e588-4e53-bb9e-f2b33c512a3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13770' max='13770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13770/13770 18:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.261400</td>\n",
       "      <td>1.275541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.163800</td>\n",
       "      <td>1.275133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.157700</td>\n",
       "      <td>1.275393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13770, training_loss=1.1821374919506287, metrics={'train_runtime': 1112.1296, 'train_samples_per_second': 99.048, 'train_steps_per_second': 12.382, 'total_flos': 7195590623232000.0, 'train_loss': 1.1821374919506287, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ef1153-c686-4c39-af6a-292e1af950e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "d9ef1153-c686-4c39-af6a-292e1af950e4",
    "outputId": "7bae5335-6eeb-4f54-c4a8-65b352af0a5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.58\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda2f261-7dd9-459c-9c98-c8c8e7a97435",
   "metadata": {
    "id": "eda2f261-7dd9-459c-9c98-c8c8e7a97435"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('models/wikitext2_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "OFjWdejSd_Iu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFjWdejSd_Iu",
    "outputId": "1163e28a-6efa-4900-d382-f1629b6472ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/models/wikitext2_gpt2/tokenizer_config.json',\n",
       " '/content/models/wikitext2_gpt2/special_tokens_map.json',\n",
       " '/content/models/wikitext2_gpt2/vocab.json',\n",
       " '/content/models/wikitext2_gpt2/merges.txt',\n",
       " '/content/models/wikitext2_gpt2/added_tokens.json',\n",
       " '/content/models/wikitext2_gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('models/wikitext2_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c33015-9175-45a0-8859-b0bf219f5b93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57c33015-9175-45a0-8859-b0bf219f5b93",
    "outputId": "a68a1217-a74c-4ba7-e543-707eec2ef91e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Can you tell me a story about how I was raised in Scientology? I remember the name David Wainwright in Scientology. He was an associate of mine who was a member of my grandfather's anti @-@ Scientology church. He is in\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "finetuned_model = pipeline(\"text-generation\", model='models/wikitext2_gpt2', device=0)\n",
    "finetuned_model('Can you tell me a story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-1CpkyGzLHjg",
   "metadata": {
    "id": "-1CpkyGzLHjg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06f24cba0c204846bb105ac9f0c11543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09e3e6de490343e3a667f32a46c0ac0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3af65a7f3bc0443dbdbfa835db937d22",
       "IPY_MODEL_6536af3dbcbb4dd09b36e445bc63241c",
       "IPY_MODEL_8c4eebf1753b4b6cbb9a524dc56cf9a8"
      ],
      "layout": "IPY_MODEL_8560cb4028a746ba9b5bea57229615c9"
     }
    },
    "0f19642782324d41829a7d5797f6ad62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285e14f856f34cc4853e1a2467f6dd8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2af61a496ac649b698878e4beea51503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37d69fabff83445f910e34cc3e5a2b29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3af65a7f3bc0443dbdbfa835db937d22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45233cd1b94e41c9b1a48158a9fc9167",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d8baa13f917040d8a9b262f2e823eff6",
      "value": "Mapâ€‡(num_proc=4):â€‡100%"
     }
    },
    "4070430394f34f03ae5e06d802a3456b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45233cd1b94e41c9b1a48158a9fc9167": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d5d8c88f47446de84ac355b0d9ebf67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55cdfd405c9941fa96be66fa33d67fcf",
      "max": 4358,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2af61a496ac649b698878e4beea51503",
      "value": 4358
     }
    },
    "4f9f569a9afc4dfaa0a5472967774e57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37d69fabff83445f910e34cc3e5a2b29",
      "max": 36718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06f24cba0c204846bb105ac9f0c11543",
      "value": 36718
     }
    },
    "50133e6ee09242c6b73374a88522186b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55cdfd405c9941fa96be66fa33d67fcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b77cf087a54447a9f86607c69625974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b9a0fb81dd5446f860124911ba5d705": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f419d2e829a45429d75c94a24eb30c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4070430394f34f03ae5e06d802a3456b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_73913264c8b447289ee998c1c1b21be9",
      "value": "â€‡36718/36718â€‡[00:05&lt;00:00,â€‡6451.89â€‡examples/s]"
     }
    },
    "621bae44f2ec40298db67f9202f42ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6536af3dbcbb4dd09b36e445bc63241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab502411710b4b909f962c179446483c",
      "max": 3760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9abadbc42708404daf3050a7541416ad",
      "value": 3760
     }
    },
    "6f628caf7b4c483db287aaccfa05b692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4637aeefd584b3680cfb864b2a7a31f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_50133e6ee09242c6b73374a88522186b",
      "value": "Mapâ€‡(num_proc=4):â€‡100%"
     }
    },
    "73913264c8b447289ee998c1c1b21be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8560cb4028a746ba9b5bea57229615c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c4eebf1753b4b6cbb9a524dc56cf9a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f19642782324d41829a7d5797f6ad62",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5b9a0fb81dd5446f860124911ba5d705",
      "value": "â€‡3760/3760â€‡[00:00&lt;00:00,â€‡1473.97â€‡examples/s]"
     }
    },
    "925eccf3d6714e70980db4ff44fceb0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a23eedf9a5544c4892f8406329a20edc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ff13c6cef378412c9d5c5e1d85476d56",
      "value": "Mapâ€‡(num_proc=4):â€‡100%"
     }
    },
    "9abadbc42708404daf3050a7541416ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a23eedf9a5544c4892f8406329a20edc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab502411710b4b909f962c179446483c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad6a0b34d7304ebdb4d80e26ed970c52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6aff30848f04050b374b74d31b5d118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_285e14f856f34cc4853e1a2467f6dd8c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5b77cf087a54447a9f86607c69625974",
      "value": "â€‡4358/4358â€‡[00:00&lt;00:00,â€‡1382.69â€‡examples/s]"
     }
    },
    "bf6e871c574b41ccbd4489d557079600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_925eccf3d6714e70980db4ff44fceb0e",
       "IPY_MODEL_4f9f569a9afc4dfaa0a5472967774e57",
       "IPY_MODEL_5f419d2e829a45429d75c94a24eb30c8"
      ],
      "layout": "IPY_MODEL_621bae44f2ec40298db67f9202f42ea5"
     }
    },
    "d4637aeefd584b3680cfb864b2a7a31f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8baa13f917040d8a9b262f2e823eff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0a0fc8b64224c03a9a755d0e778cea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f628caf7b4c483db287aaccfa05b692",
       "IPY_MODEL_4d5d8c88f47446de84ac355b0d9ebf67",
       "IPY_MODEL_b6aff30848f04050b374b74d31b5d118"
      ],
      "layout": "IPY_MODEL_ad6a0b34d7304ebdb4d80e26ed970c52"
     }
    },
    "ff13c6cef378412c9d5c5e1d85476d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
