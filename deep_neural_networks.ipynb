{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e4bdbf-eafe-46c0-9d55-35dcad874d0c",
   "metadata": {},
   "source": [
    "# 1. PyTorch\n",
    "## 1-1. Containers\n",
    "Your model should subclass `torch.nn.Module`. `torch.nn` holds basic build blocks for graphs.\n",
    "1. `torch.nn.Module.parameters(recurse=True)`: Returns an iterator over module parameters. This is typically passed to an optimizer.\n",
    "2. `torch.nn.Module.named_parameters(prefix='', recurse=True, remove_duplicate=True)`: Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
    "3. `torch.nn.Module.to(device=None, dtype=None, non_blocking=False)`: Moves and/or casts the parameters and buffers.\n",
    "4. `torch.nn.Module.load_state_dict(state_dict, strict=True, assign=False)`: Copies parameters and buffers from `state_dict` into this module and its descendants.\n",
    "5. `torch.nn.Module.state_dict`: Saves learnable parameters.\n",
    "\n",
    "## 1-2. Loss Functions\n",
    "1. Each PyTorch Loss function creates a criterion that measures the loss between `output` from a model and `target`, returning a `torch.Tensor`.\n",
    "   - [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "   - Metrics supporting backpropagation (`is_differentiable == True`) in **TorchMetrics** can be used. More details in [Metrics & Differentiability](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metrics-and-differentiability).\n",
    "3. `torch.Tensor.backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)`: Computes the gradient of current tensor with reference to graph leaves. The graph is differentiated using the chain rule. This function accumulates gradients in the leaves.\n",
    "\n",
    "## 1-3. Optimizer\n",
    "`torch.optim` implements various [optimization algorithms](https://pytorch.org/docs/stable/optim.html#algorithms).\n",
    "1. `torch.optim.Optimizer.step(closure=None)`: Performs a single optimization step (parameter update).\n",
    "    - For example, [torch.optim.SGD.step(closure=None)](https://github.com/pytorch/pytorch/blob/cd9b27231b51633e76e28b6a34002ab83b0660fc/torch/optim/sgd.py#L63).\n",
    "2. `torch.optim.Optimizer.zero_grad(set_to_none=True)`: Resets the gradients of all optimized `torch.Tensor`s.\n",
    "4. `torch.optim.Optimizer.load_state_dict(state_dict)`: Loads the optimizer state. Uses this function when loading a general checkpoint for inference or resuming training.\n",
    "5. `torch.optim.Optimizer.state_dict`: Contains information about the optimizer's state (parameters to be optimized), as well as the hyperparameters used.\n",
    "6. `torch.optim.Optimizer.add_param_group(param_group)`: Adds a param group to the `Optimizer`'s `param_groups`. Uses this function when fine tuning a pre-trained network as frozen layers can be made trainable and added to the `Optimizer` as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72365525-def0-4046-886a-4cbc7fc15bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6368,  0.6902, -1.1642,  1.7340,  0.3089],\n",
      "        [-0.5741, -2.2496,  0.1549,  0.2255, -0.5959],\n",
      "        [ 0.6467,  0.2575,  1.8013,  0.4424, -0.4409]], requires_grad=True)\n",
      "tensor([[-0.7584,  0.2138, -0.3933,  0.1054,  0.2149],\n",
      "        [ 0.2793,  0.7960, -0.1902,  2.1231, -0.5180],\n",
      "        [ 1.6648, -0.5898,  2.1185,  0.5762,  1.2281]])\n",
      "tensor(1.4591, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.0162,  0.0635, -0.1028,  0.2171,  0.0125],\n",
      "        [-0.1138, -0.4061,  0.0460, -0.2530, -0.0104],\n",
      "        [-0.1358,  0.1130, -0.0423, -0.0178, -0.2225]])\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# Model output\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.randn(3, 5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7430ef-1497-4966-81eb-88b16bec52a4",
   "metadata": {},
   "source": [
    "### 1-3-1. Gradient Accumulation\n",
    "**Gradient accumulation** refers to the situation, where multiple backwards passes are performed before updating the parameters. The goal is to have the same model parameters for multiple inputs (batches) and then update the model's parameters based on all these batches, instead of performing an update after every single batch. This technique is used to overcome GPU memory limitations when training neural networks.\n",
    "1. Manual Implementation\n",
    "2. `pytorch_lightning.Trainer(accumulate_grad_batches)`\n",
    "3. `accelerate.Accelerator(gradient_accumulation_steps)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879fff4-4171-46ad-b02a-e2224446c994",
   "metadata": {},
   "source": [
    "# 2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274d15-d1a5-4259-8b17-5f9daa7f6395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1d25b52-1415-4c42-8586-e05b0c88d7c9",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139365fb-4d8b-44a8-b18f-bbbe505ea6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacdf230-8069-4ae0-92c5-e1df29ffe5af",
   "metadata": {},
   "source": [
    "# 4. Deep Neural Networks (DNNs)\n",
    "# 5. Convolutional Neural Networks (CNNs)\n",
    "# 6. Recurrent Neural Networks (RNNs)\n",
    "## 6-1. RNN\n",
    "## 6-2. GRU\n",
    "## 6-3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81449907-67c7-48db-abea-6012efb43b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
