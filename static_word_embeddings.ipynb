{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f459f152-3b84-4384-af59-46c7559b53fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
       "...                                                 ...       ...\n",
       "2220  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
       "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
       "2222  Be careful how you code\\n\\nA new European dire...      tech\n",
       "2223  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
       "2224  Losing yourself in online gaming\\n\\nOnline rol...      tech\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a pandas DataFrame using `read()`\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/bbc_text_cls.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aca7c7-ee6d-4cc4-9ebd-3c2a834224c1",
   "metadata": {},
   "source": [
    "# 1. Vocabulary\n",
    "Create **Vocabulary** dictionaries:\n",
    "- **word2idx:** All the unique words as keys with a corresponding unique ID as values.\n",
    "- **idx2word:** The reverse of word2idx. It has the unique IDs as keys and their corresponding words as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d39cb4-689e-41a9-a13c-64c934c5d6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad3fcd51-318c-42f8-852c-375418a5e41f",
   "metadata": {},
   "source": [
    "# 1-1. word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a1493-d4bc-41ef-a56b-db9fdb6b3a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9e5d417-2889-42c2-b3eb-3c256a5597b3",
   "metadata": {},
   "source": [
    "# 1-2. idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f1511-7074-4876-aef8-60c18f80e334",
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding\n",
    "The process of one-hot encoding involves **Vocabulary Creation** & **Vector Representation** two steps.\n",
    "\n",
    "1. Manual Implementation.\n",
    "2. `pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)`\n",
    "3. `sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')`\n",
    "4. `tf.keras.utils.to_categorical(x, num_classes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d28d75-bae6-46a8-8f83-96c2b4fcc6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['betts-green,', \"d'amuro\", 'threat', 'glowing', 'work', 'colder.\"', 'court-imposed', 'persuasive', '4%,', 'slams.', 'proven', 'seamen', '\"arrogant', '\"otherwise', 'last-eight', 'beckons', 'pickups,', 'persecuted.\"', 'constantly', 'slightly.', 'excessively', 'striker,', '(australia)', '(2.1', 'together', 'stop', 'razr', '1980s.', \"commission's\", 'ubiquitous', 'sheffield-born', 'costs,\"', 'catholics', 'merchandising', 'koumas', 'nintendo,', 'stops', 'wray,', 'mccabe', 'shove.', 'fraction', 'sexual', 'hearty', 'hegedus,', 'burnley,', 'voldemort', 'visitor', 'deon', 'unpaid', 'departure,\"', 'squirreling', 'striking.', 'course.\"', 'gallery,\"', 'cherie', '(henman).', 'created,', 'rj', 'beating\"', '$111m', 'peek', \"can't\", 'bruvik', '6.67', \"dragon's\", 'harinordoquy', 'make-up', '19.6', 'formal', 'axa', 'birthdays.', 'friday?', 'intent,', 'bellamy,', 'nishioka,', \"regiments'\", 'bertie', 'kaha', 'susanne', 'divert', 'crying', 'non-opec', 'sprinter.', 'arrangements.\"', '(bpd),', 'prize-fighter', \"dane's\", 'limbs,', 'ifa', 'paxman', 'slow,', 'licenses', 'citizens,', 'raided', 'function,', 'dependability', 'cinematographer', 'championship,', 'way\".', 'level.']\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words in the corpus\n",
    "import random\n",
    "\n",
    "unique_words = set()\n",
    "for doc in df[\"text\"]:\n",
    "    for word in doc.split():\n",
    "        unique_words.add(word.lower())\n",
    "\n",
    "# Sample 100 words\n",
    "print(random.sample(tuple(unique_words), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0848808e-f82b-4875-bb53-730e50f60599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hinckley,', 0), ('bucking', 1), ('stores.\"', 2), ('8.6bn', 3), ('12.37', 4), ('osborne', 5), ('egelton,', 6), ('path,', 7), ('pay-out', 8), ('miller.', 9), ('most,', 10), ('struggling', 11), (\"sender's\", 12), ('activities,', 13), ('satirist', 14), ('3,217', 15), ('(team', 16), ('original.', 17), ('mac,\"', 18), ('hughes.', 19), ('a$846m', 20), ('advanced,', 21), ('receivers', 22), ('intervention', 23), ('attack.\"', 24), ('\"audioblogs\"', 25), ('esson,', 26), ('assault,', 27), ('layout', 28), ('campaiging', 29), ('agency.', 30), ('kezman', 31), ('vein', 32), ('filmmaker', 33), (\"employee's\", 34), ('fatboy', 35), ('6,000-strong', 36), ('printers', 37), ('shield', 38), ('ingots', 39), ('leave.', 40), ('aid', 41), ('management.\"', 42), ('1920s.', 43), ('robber', 44), ('difference\"', 45), ('puzzlement', 46), ('\"musicians', 47), ('developed,', 48), ('aids,', 49), ('weh', 50), ('75p', 51), ('frisk,', 52), ('villagers', 53), ('lange', 54), ('serena)', 55), ('masayuki', 56), ('uk-controlled', 57), (\"march's\", 58), ('$2.9bn', 59), ('rubin', 60), ('decidedly', 61), ('untouched', 62), ('akaev,', 63), (\"net's\", 64), ('still,', 65), ('timing,', 66), ('congratulated.', 67), ('legalisation', 68), ('charges.', 69), ('tory', 70), ('underwrite', 71), ('promoter,', 72), ('tacit', 73), ('endorsed', 74), ('proud', 75), ('personally,', 76), ('anti-bribery', 77), ('ambition', 78), ('you,\"', 79), ('mellor', 80), ('martens,', 81), ('prevalence', 82), ('possible\",', 83), ('mcleish', 84), ('chance,\"', 85), ('missile,', 86), ('crystalline,', 87), ('lemar', 88), ('stir', 89), ('24:', 90), ('suitably', 91), ('panel,', 92), (\"westlife's\", 93), ('countryside', 94), (\"fox's\", 95), (\"sport.'\", 96), ('sneak', 97), ('gerrard?', 98), ('\"bebop\",', 99)]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map each unique word to an index\n",
    "import itertools\n",
    "\n",
    "word2idx = {}\n",
    "for idx, word in enumerate(unique_words):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "# Return first 100 items\n",
    "print(list(itertools.islice(word2idx.items(), 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879224a-9ed6-4e74-9da4-0b809c7bbae1",
   "metadata": {},
   "source": [
    "## 2-1. Manual One-Hot Encoding\n",
    "Each word is represented as a vector of `0`s and `1`s. The length of the vector is equal to the size of the vocabulary. Position in the vector corresponds to a specific word in the vocabulary. If the word is present in a particular text sample, its corresponding position in the vector is marked as `1`, and all other positions are `0`. This implies that each word is uniquely represented by a binary vector, with only one element being `1`, indicating its presence, and all others being `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de998795-c3ae-4924-8185-2a2c9d17a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot encoded vectors for each word in the corpus\n",
    "import numpy as np\n",
    "\n",
    "one_hot_vec = []\n",
    "for doc in df['text']:\n",
    "    doc_vec = []\n",
    "    for word in doc.split():\n",
    "        vec = np.zeros(len(unique_words))\n",
    "        vec[word2idx[word.lower()]] = 1\n",
    "        doc_vec.append(vec)\n",
    "    one_hot_vec.append(doc_vec)\n",
    "\n",
    "# One-hot encoded vectors of the first 10 words from the first document\n",
    "for i in range(10):\n",
    "    print(one_hot_vec[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0cedd-e489-4e72-9164-ba0e36e0563f",
   "metadata": {},
   "source": [
    "## 2-2. One-Hot Encoding with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2add37-3abc-473e-be5d-405badfa6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "one_hot_encoded_doc = pd.get_dummies(df['text'][0], )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c0e82-64e8-49ca-8a16-66b391519ece",
   "metadata": {},
   "source": [
    "## 2-3. One-Hot Encoding with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027fa4a3-e68c-4b1e-811a-cb3c62c59988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31692, 45964, 32112, 31249, 53725, 31989, 7696, 4698, 54745, 24126, 16396, 54296, 16577, 56309, 34658, 9114, 45581, 16860, 2541, 8862, 37069, 55973, 9114, 2030, 56064, 58174, 13785, 8862, 24501, 40244, 13911, 25292, 43053, 33388, 8862, 9720, 17127, 21676, 48607, 39588, 56064, 45964, 33388, 9533, 42564, 24866, 27275, 16155, 22174, 39243, 16577, 26770, 43155, 24259, 45964, 27050, 55352, 9114, 45397, 56064, 45760, 22398, 4698, 48483, 20745, 17964, 2700, 4733, 40244, 11483, 17546, 31989, 33049, 54745, 53725, 15436, 27275, 39648, 49671, 2541, 6576, 31249, 53725, 26770, 36693, 54189, 44565, 47792, 25292, 56974, 14032, 33388, 16752, 10759, 51712, 22398, 21806, 42564, 56441, 27634, 25581, 15694, 26344, 51061, 47792, 24931, 33240, 59146, 21676, 8862, 43155, 24259, 4698, 48483, 57681, 31936, 21676, 8862, 57961, 37069, 22764, 30840, 8862, 28483, 26770, 56328, 32225, 31989, 34634, 58913, 51346, 27050, 14032, 36693, 8862, 54625, 33388, 20209, 42564, 60230, 43614, 47792, 55761, 9114, 13394, 59146, 17964, 8717, 8862, 29475, 47791, 31510, 9114, 16577, 42564, 758, 27275, 2076, 5657, 9114, 13746, 13085, 56328, 4706, 758, 2541, 9533, 8771, 16577, 52067, 15694, 9114, 27521, 43537, 27275, 22218, 60141, 37864, 17546, 33692, 17964, 8862, 24126, 24298, 33069, 52005, 37942, 40244, 13911, 18178, 9114, 4863, 31249, 43424, 43155, 24259, 4698, 48483, 19653, 33004, 31936, 52588, 52167, 51712, 22398, 57203, 28952, 27730, 4698, 29762, 38308, 9114, 48959, 58374, 17964, 16985, 46907, 24562, 27275, 46456, 17546, 57005, 42128, 9114, 14150, 13922, 8862, 12361, 27275, 22264, 57203, 21676, 8862, 5433, 33388, 8862, 50284, 22587, 53850, 43446, 2541, 8862, 21183, 16577, 16252, 17546, 31989, 33388, 33256, 13085, 38308, 56064, 22398, 22218, 37468, 54446, 55358, 23788, 44873, 9114, 23555, 58495, 25686, 40855, 8823, 23968, 10794, 27687, 46427, 2414, 33388, 935, 4431, 38703, 27275, 26050, 29477, 935, 13116, 9191, 27275, 44848, 31627, 52644, 20906, 18542, 2541, 47459, 16577, 13911, 17378, 47860, 37198, 34528, 33388, 9868, 8475, 27275, 52067, 34162, 16155, 4900, 27275, 9155, 31989, 43797, 16577, 13911, 9114, 27521, 22398, 9136, 9588, 38579, 33388, 28310, 9114, 16566, 53138, 42589, 24481, 54908, 17964, 24126, 25190, 16204, 47792, 15694, 10743, 39720, 9114, 35518, 58102, 9114, 51207, 48796, 21676, 17546, 45470, 44565, 13911, 7890, 4065, 17964, 8862, 47695, 8862, 28483, 26770, 47792, 8823, 7901, 9114, 9983, 8862, 31698, 47792, 12609, 9114, 10734, 50838, 2541, 36062, 55176, 40244, 47792, 18354, 10734, 54745, 35438, 47792, 9838, 9114, 38034, 8862, 56435, 47792, 9136, 2541, 17546, 45470, 7907, 35621, 53559, 34512, 39163, 12861, 33388, 17546, 56531, 21676, 54908, 17391, 40244, 47792, 25581, 5303, 9588, 60230, 18488, 47792, 2076, 25292, 17379, 8862, 39883, 33388, 22398, 56531, 21676, 54908, 45565, 9588, 17546, 55811, 36693, 8862, 7311, 33388, 44565, 6838]\n"
     ]
    }
   ],
   "source": [
    "# Split documents to tokens\n",
    "\n",
    "tokens_docs = [doc.lower().split() for doc in df['text']]\n",
    "\n",
    "# Convert token lists to token-id lists\n",
    "token_ids = [[word2idx[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "print(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d93675-c683-4696-95bb-e42e6c0f3bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73bdffa6-0000-4386-b133-bb5517a38007",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2225,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert list of token-id lists to one-hot representation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vec \u001b[38;5;241m=\u001b[39m OneHotEncoder(categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(unique_words))\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:975\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    Fit OneHotEncoder to X.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m        Fitted encoder.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_drop_idx()\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:78\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 78\u001b[0m X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m n_features\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:44\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mPerform custom check_array:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m- convert list of strings to object dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     X_temp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(X_temp\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mstr_):\n\u001b[1;32m     46\u001b[0m         X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/utils/validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/utils/_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2225,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert list of token-id lists to one-hot representation\n",
    "vec = OneHotEncoder(categories=list(unique_words))\n",
    "X = vec.fit_transform(token_ids)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4385e3-077b-4843-82f4-895ae6fe3a09",
   "metadata": {},
   "source": [
    "## 2-4. One-Hot Encoding with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c670-d4c5-48a7-9d72-19151436fb66",
   "metadata": {},
   "source": [
    "# 2. Bag-of-Words (BoW)\n",
    "## 2-1. Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4de3c2-4927-4a30-8bfe-aead84a9d764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ed943f-34fa-4246-b475-4ff9e3504058",
   "metadata": {},
   "source": [
    "## 2-2. CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250ef9-8f5f-47f0-bd41-e4e25aba2477",
   "metadata": {},
   "source": [
    "# 3. TF-IDF\n",
    "## 3-1. Manual Implementation\n",
    "### 3-1-1. word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87d7a6-e139-4316-afab-9243e3ea98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9e24e-dbb8-4fa4-81c3-5536f23bc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(df[\"text\"][0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04404d24-600f-4253-a796-8f4702c50217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate word2idx\n",
    "# Convert documents into sequences of ints / ids / indices\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in df[\"text\"]:\n",
    "    words = word_tokenize(doc.lower())\n",
    "    doc_as_int = []\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "        # Save for later\n",
    "        doc_as_int.append(word2idx[word])\n",
    "    tokenized_docs.append(doc_as_int)\n",
    "\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16232bb9-7f41-496a-bc5f-e5869744a8b6",
   "metadata": {},
   "source": [
    "### 3-1-2. idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382cd8a-042d-47bb-965e-80b1361d11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa798d-9c8d-4e2b-a094-3a04305db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(df['text'])\n",
    "\n",
    "# Number of words\n",
    "V = len(word2idx)\n",
    "\n",
    "N, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a923872-2faf-4055-907f-a33fa1d40c66",
   "metadata": {},
   "source": [
    "### 3-1-3. Term Frequency (TF)\n",
    "**Term frequency (TF)** means how often a term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260c138-d650-420b-80a6-334c89c60e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instantiate term-frequency matrix\n",
    "tf = np.zeros((N, V))\n",
    "\n",
    "# Populate term-frequency counts\n",
    "for i, doc_as_int in enumerate(tokenized_docs):\n",
    "    for j in doc_as_int:\n",
    "        tf[i, j] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a3b36-b9f5-4c11-a017-8c33b8b9302e",
   "metadata": {},
   "source": [
    "### 3-1-4. Inverse Document Frequency (IDF)\n",
    "- **Document frequency (DF)** is the number of documents containing a particular term.\n",
    "- **Inverse Document Frequency (IDF)** is a weight indicating how commonly a word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4dc56-5e6b-442c-a848-d79d4d41cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "# `axis=0` is the direction running downward the rows\n",
    "doc_freq = np.sum(tf > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed263b90-46e6-4003-8a14-d682ca20524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f06d7-c1b5-4db2-a26c-2298429ae315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0aceff-5437-495d-bebd-e821fdce8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document, show the top 5 terms (in terms of `tf_idf` score)\n",
    "np.random.seed(36)\n",
    "i = np.random.choice(N)\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row['label'])\n",
    "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tf_idf[i]\n",
    "# Add minus for descending\n",
    "indices = (-scores).argsort()\n",
    "for j in indices[:5]:\n",
    "    print(idx2word[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7e12b-c342-4d2a-8314-e8435b1eb019",
   "metadata": {},
   "source": [
    "## 3-2. CountVectorizer\n",
    "Derived term frequencies from `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938bbb-ad58-456a-bde1-dba001bbc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[\"text\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "labels.hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed262a0-4dec-42cf-8eb3-53d39ccd8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f22eb-171d-4160-a116-29360469dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d31f3-27a5-4d7e-af72-902028f29f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ca2a3-c137-43ab-81e4-1d5b73877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewer words than `nltk.word_tokenize()`\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91acc-4cc8-4890-9bf5-687b12b5fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default `lowercase=True`\n",
    "# np.where(words == \"India\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "# By default `token_pattern=r”(?u)\\b\\w\\w+\\b`\n",
    "# RegExp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)\n",
    "# np.where(words == \"$\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "np.where(words == \"india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2ae6-971d-402b-b635-50fc21226024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf[0][0]` in the above section\n",
    "tf.toarray()[0][13907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a4439-99b2-4bfb-b54d-d0df965546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "doc_freq = np.sum(tf.toarray() > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef44e38-03d7-4be8-9dd6-5f78eed1f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf.toarray() * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afeefb-b636-49c1-ad76-3c37264aec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf7d2d-9355-4a78-82dc-0a82fc1c3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf_idf[0][0]` in the above section\n",
    "tf_idf[0][13907]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62243fee-26e5-4e3c-9c72-6ec2dce2ae8d",
   "metadata": {},
   "source": [
    "## 3-3. TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713c327-82d1-4f66-98bc-0cfc51e58826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tf_idf = transformer.fit_transform(tf)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90fd91-64f9-490e-b628-7dfc211e43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18576d3-a230-49af-8736-6df21fa993e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `sklearn` implementation of TF-IDF is different from our manual implementation \n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46164149-3908-4c64-a94e-43f76019a69a",
   "metadata": {},
   "source": [
    "## 3-4. TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83306771-b56c-46aa-9b54-d11e1ddf4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc7a7a-5e89-41a7-ab70-73e57d2905aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129f297-ff06-4d94-976f-74605fc2a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `CountVectorizer()` followed by `TfidfTransformer()`\n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1e186-481e-41fd-8853-01e5a32deb90",
   "metadata": {},
   "source": [
    "Perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d20106-43a5-4377-ace3-c9d95896ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "inputs_train, inputs_test, y_train, y_test = train_test_split(inputs, labels, random_state=36)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train = vectorizer.fit_transform(inputs_train)\n",
    "x_test = vectorizer.transform(inputs_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train Score:\", model.score(x_train, y_train))\n",
    "print(\"Test Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd88420-26b9-45a6-bd17-10002dfc6530",
   "metadata": {},
   "source": [
    "# 4. word2vec\n",
    "## 4-1. CBOW (Continuous Bag of Words)\n",
    "## 4-2. Skip-Gram\n",
    "# 5. GloVe\n",
    "# 6. FastText\n",
    "# 7. Gaussian Embedding\n",
    "# 8. Pointcare Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bb1dd-560c-43ab-8846-6da417779f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
