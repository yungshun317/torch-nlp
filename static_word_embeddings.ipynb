{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f459f152-3b84-4384-af59-46c7559b53fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
       "...                                                 ...       ...\n",
       "2220  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
       "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
       "2222  Be careful how you code\\n\\nA new European dire...      tech\n",
       "2223  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
       "2224  Losing yourself in online gaming\\n\\nOnline rol...      tech\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a pandas DataFrame using `read()`\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/bbc_text_cls.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fcd51-318c-42f8-852c-375418a5e41f",
   "metadata": {},
   "source": [
    "# 1. Vocabulary\n",
    "Create **Vocabulary** dictionaries:\n",
    "- **word2idx:** All the unique words as keys with a corresponding unique ID as values.\n",
    "- **idx2word:** The reverse of word2idx. It has the unique IDs as keys and their corresponding words as values.\n",
    "## 1-1. word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb40706d-f1ae-4442-beab-903398d98fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(morrison', 'airline.', 'flamini,', 'sized', 'hungry,', 'macintosh', 'sing', 'thoroughness,', 'breakpoint', 'journals', 'unit.\"', 'michels,', 'pr&#233;cis', 'attackers', 'contributing', 'ever', 'art-form', 'semi-final,', 'future,\"', '1888', 'tobacco', 'misfit', 'permits.', 'now,', 'emerges', 'ubisense', 'reuniting', 'telcos,', 'synonymous', 'really,', 'reimbursement', 'book,\"', 'acclaim,', 'carla', 'non-nato', '2.9%', 'leeds,', 'discounted', 'premiership.\"', 'reckons', 'cory', 'mid-march,\"', 'projects,\"', 'hassan,', '$0.99', 'shield\".', \"contractors'\", 'unknowns', 'treasury', 'barred', 'baht', 'communicating,\"', '&#163;1.8m', 'paion,', 'scooted', 'emmanuelle', 'labelling', 'growth', \"isinbayeva's\", 'rods', 'consciously', '7:41.42.', 'exhibitor', 'queried', '40-30', 'random', 'bemoaning', 'vision,', 'marrying', 'periods.', 'centring', 'birkenhead', 'copenhagen', 'invested', 'trial,', 'samsung', 'pleasantly', 'rovers,', '(siac).', 'beckinsale', 'defined,', 'undelivered.', 'fit', 'skipper', 'excuse.', 'mehrtens', 'freer', 'sledge,', 'iconoclastic', 'governments', 'tiresome.', '3.6mbps', 'scale', 'jakobi', 'clerk', 'trainer', 'hybrid,', 'unsigned', 'sellafield,', '£2.5bn']\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words in the corpus\n",
    "import random\n",
    "\n",
    "unique_words = set()\n",
    "for doc in df[\"text\"]:\n",
    "    for word in doc.split():\n",
    "        unique_words.add(word.lower())\n",
    "\n",
    "unique_words.add('<unk>')\n",
    "unique_words.add('<pad>')\n",
    "\n",
    "# Sample 100 words\n",
    "print(random.sample(tuple(unique_words), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62b83f32-4e12-46af-b479-9443f1bfde0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hinckley,', 0), ('bucking', 1), ('stores.\"', 2), ('8.6bn', 3), ('12.37', 4), ('osborne', 5), ('egelton,', 6), ('path,', 7), ('pay-out', 8), ('miller.', 9), ('most,', 10), ('struggling', 11), (\"sender's\", 12), ('activities,', 13), ('satirist', 14), ('3,217', 15), ('(team', 16), ('original.', 17), ('mac,\"', 18), ('hughes.', 19), ('a$846m', 20), ('advanced,', 21), ('receivers', 22), ('intervention', 23), ('attack.\"', 24), ('\"audioblogs\"', 25), ('esson,', 26), ('assault,', 27), ('layout', 28), ('campaiging', 29), ('agency.', 30), ('kezman', 31), ('vein', 32), ('filmmaker', 33), (\"employee's\", 34), ('fatboy', 35), ('6,000-strong', 36), ('printers', 37), ('shield', 38), ('ingots', 39), ('leave.', 40), ('aid', 41), ('management.\"', 42), ('1920s.', 43), ('robber', 44), ('difference\"', 45), ('puzzlement', 46), ('\"musicians', 47), ('developed,', 48), ('aids,', 49), ('weh', 50), ('75p', 51), ('frisk,', 52), ('villagers', 53), ('lange', 54), ('serena)', 55), ('masayuki', 56), ('uk-controlled', 57), (\"march's\", 58), ('$2.9bn', 59), ('rubin', 60), ('decidedly', 61), ('untouched', 62), ('akaev,', 63), (\"net's\", 64), ('still,', 65), ('timing,', 66), ('congratulated.', 67), ('legalisation', 68), ('charges.', 69), ('tory', 70), ('underwrite', 71), ('promoter,', 72), ('tacit', 73), ('endorsed', 74), ('proud', 75), ('personally,', 76), ('anti-bribery', 77), ('ambition', 78), ('you,\"', 79), ('mellor', 80), ('martens,', 81), ('prevalence', 82), ('possible\",', 83), ('mcleish', 84), ('chance,\"', 85), ('missile,', 86), ('crystalline,', 87), ('lemar', 88), ('stir', 89), ('24:', 90), ('suitably', 91), ('panel,', 92), (\"westlife's\", 93), ('countryside', 94), (\"fox's\", 95), (\"sport.'\", 96), ('sneak', 97), ('gerrard?', 98), ('\"bebop\",', 99)]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map each unique word to an index\n",
    "import itertools\n",
    "\n",
    "word2idx = {}\n",
    "for idx, word in enumerate(unique_words):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "# Return first 100 items\n",
    "print(list(itertools.islice(word2idx.items(), 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6709818-9b7b-49ef-ad38-e0d919c6dbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60618"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f1511-7074-4876-aef8-60c18f80e334",
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding\n",
    "The process of one-hot encoding involves **Vocabulary Creation** & **Vector Representation** two steps.\n",
    "\n",
    "1. Manual Implementation.\n",
    "2. `pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)`\n",
    "3. `sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')`\n",
    "4. `tf.keras.utils.to_categorical(x, num_classes)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879224a-9ed6-4e74-9da4-0b809c7bbae1",
   "metadata": {},
   "source": [
    "## 2-1. Manual One-Hot Encoding\n",
    "Each word is represented as a vector of `0`s and `1`s. The length of the vector is equal to the size of the vocabulary. Position in the vector corresponds to a specific word in the vocabulary. If the word is present in a particular text sample, its corresponding position in the vector is marked as `1`, and all other positions are `0`. This implies that each word is uniquely represented by a binary vector, with only one element being `1`, indicating its presence, and all others being `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de998795-c3ae-4924-8185-2a2c9d17a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot encoded vectors for each word in the corpus\n",
    "import numpy as np\n",
    "\n",
    "one_hot_vec = []\n",
    "for doc in df['text']:\n",
    "    doc_vec = []\n",
    "    for word in doc.split():\n",
    "        vec = np.zeros(len(unique_words))\n",
    "        vec[word2idx[word.lower()]] = 1\n",
    "        doc_vec.append(vec)\n",
    "    one_hot_vec.append(doc_vec)\n",
    "\n",
    "# One-hot encoded vectors of the first 10 words from the first document\n",
    "for i in range(10):\n",
    "    print(one_hot_vec[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91709f2c-a762-4af0-a916-3abc7000b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n"
     ]
    }
   ],
   "source": [
    "print(len(one_hot_vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58bc3ab1-8363-428d-b287-ba6e428ea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60618\n"
     ]
    }
   ],
   "source": [
    "print(len(one_hot_vec[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0cedd-e489-4e72-9164-ba0e36e0563f",
   "metadata": {},
   "source": [
    "## 2-2. One-Hot Encoding with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8dfa3b7a-d7ee-4478-b8f0-0f2fd8ecae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog of War</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat and Dog Man</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text    labels\n",
       "0       Dog of War  business\n",
       "1  Cat and Dog Man      tech"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'text': [\"Dog of War\", \"Cat and Dog Man\"], 'labels': [\"business\", \"tech\"]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "16216943-b3f1-4a0f-b782-64a0d512bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      cat\n",
       "1       of\n",
       "2      man\n",
       "3    <unk>\n",
       "4      dog\n",
       "5      war\n",
       "6    <pad>\n",
       "7      and\n",
       "dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unique_word_series = pd.Series(list(unique_words))\n",
    "unique_word_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "351a9d98-7d40-4d81-9b9e-4bba0b3c72cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;pad&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>man</th>\n",
       "      <th>of</th>\n",
       "      <th>war</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   <pad>  <unk>    and    cat    dog    man     of    war\n",
       "0  False  False  False   True  False  False  False  False\n",
       "1  False  False  False  False  False  False   True  False\n",
       "2  False  False  False  False  False   True  False  False\n",
       "3  False   True  False  False  False  False  False  False\n",
       "4  False  False  False  False   True  False  False  False\n",
       "5  False  False  False  False  False  False  False   True\n",
       "6   True  False  False  False  False  False  False  False\n",
       "7  False  False   True  False  False  False  False  False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_dummies = pd.get_dummies(unique_word_series)\n",
    "unique_word_dummies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e78d1c38-9d1d-4dfd-b574-fb08b80adf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([False, False, False, False,  True, False, False, False]),\n",
       "  array([False,  True, False, False, False, False, False, False]),\n",
       "  array([False, False, False, False, False,  True, False, False])],\n",
       " [array([ True, False, False, False, False, False, False, False]),\n",
       "  array([False, False, False, False, False, False, False,  True]),\n",
       "  array([False, False, False, False,  True, False, False, False]),\n",
       "  array([False, False,  True, False, False, False, False, False])]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vec = []\n",
    "for doc in df['text']:\n",
    "    doc_vec = []\n",
    "    for word in doc.split():\n",
    "        doc_vec.append(unique_word_dummies[word.lower()].values)\n",
    "    one_hot_vec.append(doc_vec)\n",
    "\n",
    "one_hot_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c0e82-64e8-49ca-8a16-66b391519ece",
   "metadata": {},
   "source": [
    "## 2-3. One-Hot Encoding with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "027fa4a3-e68c-4b1e-811a-cb3c62c59988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31693, 45965, 32113, 31250, 53727, 31990, 7696, 4698, 54747, 24127, 16397, 54298, 16578, 56311, 34659, 9115, 45582, 16861, 2541, 8863, 37070, 55975, 9115, 2030, 56066, 58176, 13786, 8863, 24502, 40245, 13912, 25293, 43054, 33389, 8863, 9721, 17128, 21677, 48608, 39589, 56066, 45965, 33389, 9534, 42565, 24867, 27276, 16156, 22175, 39244, 16578, 26771, 43156, 24260, 45965, 27051, 55354, 9115, 45398, 56066, 45761, 22399, 4698, 48484, 20746, 17965, 2700, 4733, 40245, 11484, 17547, 31990, 33050, 54747, 53727, 15437, 27276, 39649, 49673, 2541, 6576, 31250, 53727, 26771, 36694, 54191, 44566, 47793, 25293, 56976, 14033, 33389, 16753, 10760, 51714, 22399, 21807, 42565, 56443, 27635, 25582, 15695, 26345, 51063, 47793, 24932, 33241, 59148, 21677, 8863, 43156, 24260, 4698, 48484, 57683, 31937, 21677, 8863, 57963, 37070, 22765, 30841, 8863, 28484, 26771, 56330, 32226, 31990, 34635, 58915, 51348, 27051, 14033, 36694, 8863, 54627, 33389, 20210, 42565, 60232, 43615, 47793, 55763, 9115, 13395, 59148, 17965, 8718, 8863, 29476, 47792, 31511, 9115, 16578, 42565, 758, 27276, 2076, 5657, 9115, 13747, 13086, 56330, 4706, 758, 2541, 9534, 8772, 16578, 52069, 15695, 9115, 27522, 43538, 27276, 22219, 60143, 37865, 17547, 33693, 17965, 8863, 24127, 24299, 33070, 52007, 37943, 40245, 13912, 18179, 9115, 4863, 31250, 43425, 43156, 24260, 4698, 48484, 19654, 33005, 31937, 52590, 52169, 51714, 22399, 57205, 28953, 27731, 4698, 29763, 38309, 9115, 48961, 58376, 17965, 16986, 46908, 24563, 27276, 46457, 17547, 57007, 42129, 9115, 14151, 13923, 8863, 12362, 27276, 22265, 57205, 21677, 8863, 5433, 33389, 8863, 50286, 22588, 53852, 43447, 2541, 8863, 21184, 16578, 16253, 17547, 31990, 33389, 33257, 13086, 38309, 56066, 22399, 22219, 37469, 54448, 55360, 23789, 44874, 9115, 23556, 58497, 25687, 40856, 8824, 23969, 10795, 27688, 46428, 2414, 33389, 935, 4431, 38704, 27276, 26051, 29478, 935, 13117, 9192, 27276, 44849, 31628, 52646, 20907, 18543, 2541, 47460, 16578, 13912, 17379, 47861, 37199, 34529, 33389, 9869, 8475, 27276, 52069, 34163, 16156, 4900, 27276, 9156, 31990, 43798, 16578, 13912, 9115, 27522, 22399, 9137, 9589, 38580, 33389, 28311, 9115, 16567, 53140, 42590, 24482, 54910, 17965, 24127, 25191, 16205, 47793, 15695, 10744, 39721, 9115, 35519, 58104, 9115, 51209, 48797, 21677, 17547, 45471, 44566, 13912, 7890, 4065, 17965, 8863, 47696, 8863, 28484, 26771, 47793, 8824, 7901, 9115, 9984, 8863, 31699, 47793, 12610, 9115, 10735, 50840, 2541, 36063, 55178, 40245, 47793, 18355, 10735, 54747, 35439, 47793, 9839, 9115, 38035, 8863, 56437, 47793, 9137, 2541, 17547, 45471, 7907, 35622, 53561, 34513, 39164, 12862, 33389, 17547, 56533, 21677, 54910, 17392, 40245, 47793, 25582, 5303, 9589, 60232, 18489, 47793, 2076, 25293, 17380, 8863, 39884, 33389, 22399, 56533, 21677, 54910, 45566, 9589, 17547, 55813, 36694, 8863, 7311, 33389, 44566, 6838]\n"
     ]
    }
   ],
   "source": [
    "# Split documents to tokens\n",
    "tokens_docs = [doc.lower().split() for doc in df['text']]\n",
    "\n",
    "# Convert token lists to token-id lists\n",
    "token_ids = [[word2idx[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "print(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a57c6d30-ae10-41e7-a73f-1cd16dcffb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'sales',\n",
       " 'boost',\n",
       " 'time',\n",
       " 'warner',\n",
       " 'profit',\n",
       " 'quarterly',\n",
       " 'profits',\n",
       " 'at',\n",
       " 'us',\n",
       " 'media',\n",
       " 'giant',\n",
       " 'timewarner',\n",
       " 'jumped',\n",
       " '76%',\n",
       " 'to',\n",
       " '$1.13bn',\n",
       " '(£600m)',\n",
       " 'for',\n",
       " 'the',\n",
       " 'three',\n",
       " 'months',\n",
       " 'to',\n",
       " 'december,',\n",
       " 'from',\n",
       " '$639m',\n",
       " 'year-earlier.',\n",
       " 'the',\n",
       " 'firm,',\n",
       " 'which',\n",
       " 'is',\n",
       " 'now',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'investors',\n",
       " 'in',\n",
       " 'google,',\n",
       " 'benefited',\n",
       " 'from',\n",
       " 'sales',\n",
       " 'of',\n",
       " 'high-speed',\n",
       " 'internet',\n",
       " 'connections',\n",
       " 'and',\n",
       " 'higher',\n",
       " 'advert',\n",
       " 'sales.',\n",
       " 'timewarner',\n",
       " 'said',\n",
       " 'fourth',\n",
       " 'quarter',\n",
       " 'sales',\n",
       " 'rose',\n",
       " '2%',\n",
       " 'to',\n",
       " '$11.1bn',\n",
       " 'from',\n",
       " '$10.9bn.',\n",
       " 'its',\n",
       " 'profits',\n",
       " 'were',\n",
       " 'buoyed',\n",
       " 'by',\n",
       " 'one-off',\n",
       " 'gains',\n",
       " 'which',\n",
       " 'offset',\n",
       " 'a',\n",
       " 'profit',\n",
       " 'dip',\n",
       " 'at',\n",
       " 'warner',\n",
       " 'bros,',\n",
       " 'and',\n",
       " 'less',\n",
       " 'users',\n",
       " 'for',\n",
       " 'aol.',\n",
       " 'time',\n",
       " 'warner',\n",
       " 'said',\n",
       " 'on',\n",
       " 'friday',\n",
       " 'that',\n",
       " 'it',\n",
       " 'now',\n",
       " 'owns',\n",
       " '8%',\n",
       " 'of',\n",
       " 'search-engine',\n",
       " 'google.',\n",
       " 'but',\n",
       " 'its',\n",
       " 'own',\n",
       " 'internet',\n",
       " 'business,',\n",
       " 'aol,',\n",
       " 'had',\n",
       " 'has',\n",
       " 'mixed',\n",
       " 'fortunes.',\n",
       " 'it',\n",
       " 'lost',\n",
       " '464,000',\n",
       " 'subscribers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fourth',\n",
       " 'quarter',\n",
       " 'profits',\n",
       " 'were',\n",
       " 'lower',\n",
       " 'than',\n",
       " 'in',\n",
       " 'the',\n",
       " 'preceding',\n",
       " 'three',\n",
       " 'quarters.',\n",
       " 'however,',\n",
       " 'the',\n",
       " 'company',\n",
       " 'said',\n",
       " \"aol's\",\n",
       " 'underlying',\n",
       " 'profit',\n",
       " 'before',\n",
       " 'exceptional',\n",
       " 'items',\n",
       " 'rose',\n",
       " '8%',\n",
       " 'on',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'stronger',\n",
       " 'internet',\n",
       " 'advertising',\n",
       " 'revenues.',\n",
       " 'it',\n",
       " 'hopes',\n",
       " 'to',\n",
       " 'increase',\n",
       " 'subscribers',\n",
       " 'by',\n",
       " 'offering',\n",
       " 'the',\n",
       " 'online',\n",
       " 'service',\n",
       " 'free',\n",
       " 'to',\n",
       " 'timewarner',\n",
       " 'internet',\n",
       " 'customers',\n",
       " 'and',\n",
       " 'will',\n",
       " 'try',\n",
       " 'to',\n",
       " 'sign',\n",
       " 'up',\n",
       " \"aol's\",\n",
       " 'existing',\n",
       " 'customers',\n",
       " 'for',\n",
       " 'high-speed',\n",
       " 'broadband.',\n",
       " 'timewarner',\n",
       " 'also',\n",
       " 'has',\n",
       " 'to',\n",
       " 'restate',\n",
       " '2000',\n",
       " 'and',\n",
       " '2003',\n",
       " 'results',\n",
       " 'following',\n",
       " 'a',\n",
       " 'probe',\n",
       " 'by',\n",
       " 'the',\n",
       " 'us',\n",
       " 'securities',\n",
       " 'exchange',\n",
       " 'commission',\n",
       " '(sec),',\n",
       " 'which',\n",
       " 'is',\n",
       " 'close',\n",
       " 'to',\n",
       " 'concluding.',\n",
       " 'time',\n",
       " \"warner's\",\n",
       " 'fourth',\n",
       " 'quarter',\n",
       " 'profits',\n",
       " 'were',\n",
       " 'slightly',\n",
       " 'better',\n",
       " 'than',\n",
       " \"analysts'\",\n",
       " 'expectations.',\n",
       " 'but',\n",
       " 'its',\n",
       " 'film',\n",
       " 'division',\n",
       " 'saw',\n",
       " 'profits',\n",
       " 'slump',\n",
       " '27%',\n",
       " 'to',\n",
       " '$284m,',\n",
       " 'helped',\n",
       " 'by',\n",
       " 'box-office',\n",
       " 'flops',\n",
       " 'alexander',\n",
       " 'and',\n",
       " 'catwoman,',\n",
       " 'a',\n",
       " 'sharp',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'year-earlier,',\n",
       " 'when',\n",
       " 'the',\n",
       " 'third',\n",
       " 'and',\n",
       " 'final',\n",
       " 'film',\n",
       " 'in',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rings',\n",
       " 'trilogy',\n",
       " 'boosted',\n",
       " 'results.',\n",
       " 'for',\n",
       " 'the',\n",
       " 'full-year,',\n",
       " 'timewarner',\n",
       " 'posted',\n",
       " 'a',\n",
       " 'profit',\n",
       " 'of',\n",
       " '$3.36bn,',\n",
       " 'up',\n",
       " '27%',\n",
       " 'from',\n",
       " 'its',\n",
       " '2003',\n",
       " 'performance,',\n",
       " 'while',\n",
       " 'revenues',\n",
       " 'grew',\n",
       " '6.4%',\n",
       " 'to',\n",
       " '$42.09bn.',\n",
       " '\"our',\n",
       " 'financial',\n",
       " 'performance',\n",
       " 'was',\n",
       " 'strong,',\n",
       " 'meeting',\n",
       " 'or',\n",
       " 'exceeding',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'full-year',\n",
       " 'objectives',\n",
       " 'and',\n",
       " 'greatly',\n",
       " 'enhancing',\n",
       " 'our',\n",
       " 'flexibility,\"',\n",
       " 'chairman',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'richard',\n",
       " 'parsons',\n",
       " 'said.',\n",
       " 'for',\n",
       " '2005,',\n",
       " 'timewarner',\n",
       " 'is',\n",
       " 'projecting',\n",
       " 'operating',\n",
       " 'earnings',\n",
       " 'growth',\n",
       " 'of',\n",
       " 'around',\n",
       " '5%,',\n",
       " 'and',\n",
       " 'also',\n",
       " 'expects',\n",
       " 'higher',\n",
       " 'revenue',\n",
       " 'and',\n",
       " 'wider',\n",
       " 'profit',\n",
       " 'margins.',\n",
       " 'timewarner',\n",
       " 'is',\n",
       " 'to',\n",
       " 'restate',\n",
       " 'its',\n",
       " 'accounts',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'resolve',\n",
       " 'an',\n",
       " 'inquiry',\n",
       " 'into',\n",
       " 'aol',\n",
       " 'by',\n",
       " 'us',\n",
       " 'market',\n",
       " 'regulators.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'already',\n",
       " 'offered',\n",
       " 'to',\n",
       " 'pay',\n",
       " '$300m',\n",
       " 'to',\n",
       " 'settle',\n",
       " 'charges,',\n",
       " 'in',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'that',\n",
       " 'is',\n",
       " 'under',\n",
       " 'review',\n",
       " 'by',\n",
       " 'the',\n",
       " 'sec.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'estimate',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'it',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'set',\n",
       " 'aside',\n",
       " 'for',\n",
       " 'legal',\n",
       " 'reserves,',\n",
       " 'which',\n",
       " 'it',\n",
       " 'previously',\n",
       " 'set',\n",
       " 'at',\n",
       " '$500m.',\n",
       " 'it',\n",
       " 'intends',\n",
       " 'to',\n",
       " 'adjust',\n",
       " 'the',\n",
       " 'way',\n",
       " 'it',\n",
       " 'accounts',\n",
       " 'for',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'german',\n",
       " 'music',\n",
       " 'publisher',\n",
       " \"bertelsmann's\",\n",
       " 'purchase',\n",
       " 'of',\n",
       " 'a',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'aol',\n",
       " 'europe,',\n",
       " 'which',\n",
       " 'it',\n",
       " 'had',\n",
       " 'reported',\n",
       " 'as',\n",
       " 'advertising',\n",
       " 'revenue.',\n",
       " 'it',\n",
       " 'will',\n",
       " 'now',\n",
       " 'book',\n",
       " 'the',\n",
       " 'sale',\n",
       " 'of',\n",
       " 'its',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'aol',\n",
       " 'europe',\n",
       " 'as',\n",
       " 'a',\n",
       " 'loss',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'that',\n",
       " 'stake.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddb3125f-daed-4ba2-a6da-a0d40efe6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31693"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['ad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60ab46e0-03cc-4430-8a69-aa5ee8acff63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6838"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['stake.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "062d4ec8-d875-469f-9b01-68bbe86e4c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cde252e9-e8cf-4ccd-972e-11632d7e2ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0d93675-c683-4696-95bb-e42e6c0f3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lists have the same length.\n"
     ]
    }
   ],
   "source": [
    "# Pad each document manually\n",
    "doc_max_length = max(len(t) for t in token_ids)\n",
    "for t in token_ids:\n",
    "    t += [word2idx['<pad>']] * (doc_max_length - len(t))\n",
    "\n",
    "# Check paddings\n",
    "it = iter(token_ids)\n",
    "the_len = len(next(it))\n",
    "if not all(len(l) == the_len for l in it):\n",
    "    raise ValueError('not all lists have same length!')\n",
    "else:\n",
    "    print(\"All lists have the same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a0ea6f9-b989-4844-a246-9d02cbf2db57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4432"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e300179-b9ae-404e-ad15-15802ac28907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31693,\n",
       " 45965,\n",
       " 32113,\n",
       " 31250,\n",
       " 53727,\n",
       " 31990,\n",
       " 7696,\n",
       " 4698,\n",
       " 54747,\n",
       " 24127,\n",
       " 16397,\n",
       " 54298,\n",
       " 16578,\n",
       " 56311,\n",
       " 34659,\n",
       " 9115,\n",
       " 45582,\n",
       " 16861,\n",
       " 2541,\n",
       " 8863,\n",
       " 37070,\n",
       " 55975,\n",
       " 9115,\n",
       " 2030,\n",
       " 56066,\n",
       " 58176,\n",
       " 13786,\n",
       " 8863,\n",
       " 24502,\n",
       " 40245,\n",
       " 13912,\n",
       " 25293,\n",
       " 43054,\n",
       " 33389,\n",
       " 8863,\n",
       " 9721,\n",
       " 17128,\n",
       " 21677,\n",
       " 48608,\n",
       " 39589,\n",
       " 56066,\n",
       " 45965,\n",
       " 33389,\n",
       " 9534,\n",
       " 42565,\n",
       " 24867,\n",
       " 27276,\n",
       " 16156,\n",
       " 22175,\n",
       " 39244,\n",
       " 16578,\n",
       " 26771,\n",
       " 43156,\n",
       " 24260,\n",
       " 45965,\n",
       " 27051,\n",
       " 55354,\n",
       " 9115,\n",
       " 45398,\n",
       " 56066,\n",
       " 45761,\n",
       " 22399,\n",
       " 4698,\n",
       " 48484,\n",
       " 20746,\n",
       " 17965,\n",
       " 2700,\n",
       " 4733,\n",
       " 40245,\n",
       " 11484,\n",
       " 17547,\n",
       " 31990,\n",
       " 33050,\n",
       " 54747,\n",
       " 53727,\n",
       " 15437,\n",
       " 27276,\n",
       " 39649,\n",
       " 49673,\n",
       " 2541,\n",
       " 6576,\n",
       " 31250,\n",
       " 53727,\n",
       " 26771,\n",
       " 36694,\n",
       " 54191,\n",
       " 44566,\n",
       " 47793,\n",
       " 25293,\n",
       " 56976,\n",
       " 14033,\n",
       " 33389,\n",
       " 16753,\n",
       " 10760,\n",
       " 51714,\n",
       " 22399,\n",
       " 21807,\n",
       " 42565,\n",
       " 56443,\n",
       " 27635,\n",
       " 25582,\n",
       " 15695,\n",
       " 26345,\n",
       " 51063,\n",
       " 47793,\n",
       " 24932,\n",
       " 33241,\n",
       " 59148,\n",
       " 21677,\n",
       " 8863,\n",
       " 43156,\n",
       " 24260,\n",
       " 4698,\n",
       " 48484,\n",
       " 57683,\n",
       " 31937,\n",
       " 21677,\n",
       " 8863,\n",
       " 57963,\n",
       " 37070,\n",
       " 22765,\n",
       " 30841,\n",
       " 8863,\n",
       " 28484,\n",
       " 26771,\n",
       " 56330,\n",
       " 32226,\n",
       " 31990,\n",
       " 34635,\n",
       " 58915,\n",
       " 51348,\n",
       " 27051,\n",
       " 14033,\n",
       " 36694,\n",
       " 8863,\n",
       " 54627,\n",
       " 33389,\n",
       " 20210,\n",
       " 42565,\n",
       " 60232,\n",
       " 43615,\n",
       " 47793,\n",
       " 55763,\n",
       " 9115,\n",
       " 13395,\n",
       " 59148,\n",
       " 17965,\n",
       " 8718,\n",
       " 8863,\n",
       " 29476,\n",
       " 47792,\n",
       " 31511,\n",
       " 9115,\n",
       " 16578,\n",
       " 42565,\n",
       " 758,\n",
       " 27276,\n",
       " 2076,\n",
       " 5657,\n",
       " 9115,\n",
       " 13747,\n",
       " 13086,\n",
       " 56330,\n",
       " 4706,\n",
       " 758,\n",
       " 2541,\n",
       " 9534,\n",
       " 8772,\n",
       " 16578,\n",
       " 52069,\n",
       " 15695,\n",
       " 9115,\n",
       " 27522,\n",
       " 43538,\n",
       " 27276,\n",
       " 22219,\n",
       " 60143,\n",
       " 37865,\n",
       " 17547,\n",
       " 33693,\n",
       " 17965,\n",
       " 8863,\n",
       " 24127,\n",
       " 24299,\n",
       " 33070,\n",
       " 52007,\n",
       " 37943,\n",
       " 40245,\n",
       " 13912,\n",
       " 18179,\n",
       " 9115,\n",
       " 4863,\n",
       " 31250,\n",
       " 43425,\n",
       " 43156,\n",
       " 24260,\n",
       " 4698,\n",
       " 48484,\n",
       " 19654,\n",
       " 33005,\n",
       " 31937,\n",
       " 52590,\n",
       " 52169,\n",
       " 51714,\n",
       " 22399,\n",
       " 57205,\n",
       " 28953,\n",
       " 27731,\n",
       " 4698,\n",
       " 29763,\n",
       " 38309,\n",
       " 9115,\n",
       " 48961,\n",
       " 58376,\n",
       " 17965,\n",
       " 16986,\n",
       " 46908,\n",
       " 24563,\n",
       " 27276,\n",
       " 46457,\n",
       " 17547,\n",
       " 57007,\n",
       " 42129,\n",
       " 9115,\n",
       " 14151,\n",
       " 13923,\n",
       " 8863,\n",
       " 12362,\n",
       " 27276,\n",
       " 22265,\n",
       " 57205,\n",
       " 21677,\n",
       " 8863,\n",
       " 5433,\n",
       " 33389,\n",
       " 8863,\n",
       " 50286,\n",
       " 22588,\n",
       " 53852,\n",
       " 43447,\n",
       " 2541,\n",
       " 8863,\n",
       " 21184,\n",
       " 16578,\n",
       " 16253,\n",
       " 17547,\n",
       " 31990,\n",
       " 33389,\n",
       " 33257,\n",
       " 13086,\n",
       " 38309,\n",
       " 56066,\n",
       " 22399,\n",
       " 22219,\n",
       " 37469,\n",
       " 54448,\n",
       " 55360,\n",
       " 23789,\n",
       " 44874,\n",
       " 9115,\n",
       " 23556,\n",
       " 58497,\n",
       " 25687,\n",
       " 40856,\n",
       " 8824,\n",
       " 23969,\n",
       " 10795,\n",
       " 27688,\n",
       " 46428,\n",
       " 2414,\n",
       " 33389,\n",
       " 935,\n",
       " 4431,\n",
       " 38704,\n",
       " 27276,\n",
       " 26051,\n",
       " 29478,\n",
       " 935,\n",
       " 13117,\n",
       " 9192,\n",
       " 27276,\n",
       " 44849,\n",
       " 31628,\n",
       " 52646,\n",
       " 20907,\n",
       " 18543,\n",
       " 2541,\n",
       " 47460,\n",
       " 16578,\n",
       " 13912,\n",
       " 17379,\n",
       " 47861,\n",
       " 37199,\n",
       " 34529,\n",
       " 33389,\n",
       " 9869,\n",
       " 8475,\n",
       " 27276,\n",
       " 52069,\n",
       " 34163,\n",
       " 16156,\n",
       " 4900,\n",
       " 27276,\n",
       " 9156,\n",
       " 31990,\n",
       " 43798,\n",
       " 16578,\n",
       " 13912,\n",
       " 9115,\n",
       " 27522,\n",
       " 22399,\n",
       " 9137,\n",
       " 9589,\n",
       " 38580,\n",
       " 33389,\n",
       " 28311,\n",
       " 9115,\n",
       " 16567,\n",
       " 53140,\n",
       " 42590,\n",
       " 24482,\n",
       " 54910,\n",
       " 17965,\n",
       " 24127,\n",
       " 25191,\n",
       " 16205,\n",
       " 47793,\n",
       " 15695,\n",
       " 10744,\n",
       " 39721,\n",
       " 9115,\n",
       " 35519,\n",
       " 58104,\n",
       " 9115,\n",
       " 51209,\n",
       " 48797,\n",
       " 21677,\n",
       " 17547,\n",
       " 45471,\n",
       " 44566,\n",
       " 13912,\n",
       " 7890,\n",
       " 4065,\n",
       " 17965,\n",
       " 8863,\n",
       " 47696,\n",
       " 8863,\n",
       " 28484,\n",
       " 26771,\n",
       " 47793,\n",
       " 8824,\n",
       " 7901,\n",
       " 9115,\n",
       " 9984,\n",
       " 8863,\n",
       " 31699,\n",
       " 47793,\n",
       " 12610,\n",
       " 9115,\n",
       " 10735,\n",
       " 50840,\n",
       " 2541,\n",
       " 36063,\n",
       " 55178,\n",
       " 40245,\n",
       " 47793,\n",
       " 18355,\n",
       " 10735,\n",
       " 54747,\n",
       " 35439,\n",
       " 47793,\n",
       " 9839,\n",
       " 9115,\n",
       " 38035,\n",
       " 8863,\n",
       " 56437,\n",
       " 47793,\n",
       " 9137,\n",
       " 2541,\n",
       " 17547,\n",
       " 45471,\n",
       " 7907,\n",
       " 35622,\n",
       " 53561,\n",
       " 34513,\n",
       " 39164,\n",
       " 12862,\n",
       " 33389,\n",
       " 17547,\n",
       " 56533,\n",
       " 21677,\n",
       " 54910,\n",
       " 17392,\n",
       " 40245,\n",
       " 47793,\n",
       " 25582,\n",
       " 5303,\n",
       " 9589,\n",
       " 60232,\n",
       " 18489,\n",
       " 47793,\n",
       " 2076,\n",
       " 25293,\n",
       " 17380,\n",
       " 8863,\n",
       " 39884,\n",
       " 33389,\n",
       " 22399,\n",
       " 56533,\n",
       " 21677,\n",
       " 54910,\n",
       " 45566,\n",
       " 9589,\n",
       " 17547,\n",
       " 55813,\n",
       " 36694,\n",
       " 8863,\n",
       " 7311,\n",
       " 33389,\n",
       " 44566,\n",
       " 6838,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " 8617,\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7db30a2d-74ad-46e8-ae73-73526a04cae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60618"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([*range(len(list(unique_words)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "45ffd0fa-072a-4873-91e3-14a83f6c359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat', 'of', 'man', '<unk>', 'dog', 'war', '<pad>', 'and'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "unique_words = set()\n",
    "for doc in df[\"text\"]:\n",
    "    for word in doc.split():\n",
    "        unique_words.add(word.lower())\n",
    "\n",
    "unique_words.add('<unk>')\n",
    "unique_words.add('<pad>')\n",
    "\n",
    "# Sample 100 words\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e739b16e-8bd6-4df2-9874-248ea5232aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'of': 1, 'man': 2, '<unk>': 3, 'dog': 4, 'war': 5, '<pad>': 6, 'and': 7}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map each unique word to an index\n",
    "import itertools\n",
    "\n",
    "word2idx = {}\n",
    "for idx, word in enumerate(unique_words):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "# Return first 100 items\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b91205b0-b167-49a2-ba35-48a6f389efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 5], [0, 7, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Split documents to tokens\n",
    "tokens_docs = [doc.lower().split() for doc in df['text']]\n",
    "\n",
    "# Convert token lists to token-id lists\n",
    "token_ids = [[word2idx[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4bab5af4-003e-4a52-adf0-d1477c8b1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lists have the same length.\n"
     ]
    }
   ],
   "source": [
    "# Pad each document manually\n",
    "doc_max_length = max(len(t) for t in token_ids)\n",
    "for t in token_ids:\n",
    "    t += [word2idx['<pad>']] * (doc_max_length - len(t))\n",
    "\n",
    "# Check paddings\n",
    "it = iter(token_ids)\n",
    "the_len = len(next(it))\n",
    "if not all(len(l) == the_len for l in it):\n",
    "    raise ValueError('not all lists have same length!')\n",
    "else:\n",
    "    print(\"All lists have the same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "91640467-007d-4a15-bedb-1356889f7e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 5, 6], [0, 7, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bcd6a3af-5bcf-42ea-b478-85c9300176a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      cat\n",
       "1       of\n",
       "2      man\n",
       "3    <unk>\n",
       "4      dog\n",
       "5      war\n",
       "6    <pad>\n",
       "7      and\n",
       "dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06e582a4-5e71-4408-a9c1-c359d0acfc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    cat\n",
       "1     of\n",
       "2    man\n",
       "3  <unk>\n",
       "4    dog\n",
       "5    war\n",
       "6  <pad>\n",
       "7    and"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_df = unique_word_series.to_frame()\n",
    "unique_word_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26da4979-34a6-44ba-9f04-819de345d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(sparse_output=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(sparse_output=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "# unique_word_encoded = ohe.fit_transform(unique_word_series.values.reshape(-1, 1))\n",
    "unique_word_encoded = ohe.fit(unique_word_df)\n",
    "unique_word_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "231f7b6b-afe7-428f-9008-d02d0ba17fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<pad>', '<unk>', 'and', 'cat', 'dog', 'man', 'of', 'war'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_encoded.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "af56712a-cc00-48b7-bbd4-120cab4086b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
       "  array([[0., 0., 0., 0., 0., 0., 0., 1.]])],\n",
       " [array([[0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       "  array([[0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., 0., 0., 1., 0., 0.]])]]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vec = []\n",
    "for doc in df['text']:\n",
    "    doc_vec = []\n",
    "    for word in doc.lower().split():\n",
    "        doc_vec.append(ohe.transform(np.array(word).reshape(1, -1)))\n",
    "    one_hot_vec.append(doc_vec)\n",
    "\n",
    "one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a5d8959c-d7c5-4180-9b12-eff032ad22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#label_encoded = le.fit_transform(unique_words)\n",
    "\n",
    "# Convert list of token-id lists to one-hot representation\n",
    "vec = OneHotEncoder(sparse_output=False)\n",
    "X = vec.fit_transform(token_ids)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73bdffa6-0000-4386-b133-bb5517a38007",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: if categories is an array, it has to be of shape (n_features,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#le = LabelEncoder()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#label_encoded = le.fit_transform(unique_words)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert list of token-id lists to one-hot representation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vec \u001b[38;5;241m=\u001b[39m OneHotEncoder(categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m([\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(unique_words)))]), sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:975\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    Fit OneHotEncoder to X.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m        Fitted encoder.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_drop_idx()\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[0;32m~/envs/yungshun-py3/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:85\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories) \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: if categories is an array,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m it has to be of shape (n_features,).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m category_counts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch: if categories is an array, it has to be of shape (n_features,)."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#label_encoded = le.fit_transform(unique_words)\n",
    "\n",
    "# Convert list of token-id lists to one-hot representation\n",
    "vec = OneHotEncoder(categories=list([*range(len(list(unique_words)))]), sparse_output=False)\n",
    "X = vec.fit_transform(token_ids)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad133257-f9a1-4fd1-ba63-486c4f594048",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m vec\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "X = vec.fit_transform(token_ids.reshape(-1, 1))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0611a66d-6ce8-4214-be19-882975eff0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab43403b-d489-40b3-970e-03276762209a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479138"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4385e3-077b-4843-82f4-895ae6fe3a09",
   "metadata": {},
   "source": [
    "## 2-4. One-Hot Encoding with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c670-d4c5-48a7-9d72-19151436fb66",
   "metadata": {},
   "source": [
    "# 2. Bag-of-Words (BoW)\n",
    "## 2-1. Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4de3c2-4927-4a30-8bfe-aead84a9d764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ed943f-34fa-4246-b475-4ff9e3504058",
   "metadata": {},
   "source": [
    "## 2-2. CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250ef9-8f5f-47f0-bd41-e4e25aba2477",
   "metadata": {},
   "source": [
    "# 3. TF-IDF\n",
    "## 3-1. Manual Implementation\n",
    "### 3-1-1. word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87d7a6-e139-4316-afab-9243e3ea98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9e24e-dbb8-4fa4-81c3-5536f23bc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(df[\"text\"][0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04404d24-600f-4253-a796-8f4702c50217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate word2idx\n",
    "# Convert documents into sequences of ints / ids / indices\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in df[\"text\"]:\n",
    "    words = word_tokenize(doc.lower())\n",
    "    doc_as_int = []\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "        # Save for later\n",
    "        doc_as_int.append(word2idx[word])\n",
    "    tokenized_docs.append(doc_as_int)\n",
    "\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16232bb9-7f41-496a-bc5f-e5869744a8b6",
   "metadata": {},
   "source": [
    "### 3-1-2. idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382cd8a-042d-47bb-965e-80b1361d11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa798d-9c8d-4e2b-a094-3a04305db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(df['text'])\n",
    "\n",
    "# Number of words\n",
    "V = len(word2idx)\n",
    "\n",
    "N, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a923872-2faf-4055-907f-a33fa1d40c66",
   "metadata": {},
   "source": [
    "### 3-1-3. Term Frequency (TF)\n",
    "**Term frequency (TF)** means how often a term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260c138-d650-420b-80a6-334c89c60e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instantiate term-frequency matrix\n",
    "tf = np.zeros((N, V))\n",
    "\n",
    "# Populate term-frequency counts\n",
    "for i, doc_as_int in enumerate(tokenized_docs):\n",
    "    for j in doc_as_int:\n",
    "        tf[i, j] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a3b36-b9f5-4c11-a017-8c33b8b9302e",
   "metadata": {},
   "source": [
    "### 3-1-4. Inverse Document Frequency (IDF)\n",
    "- **Document frequency (DF)** is the number of documents containing a particular term.\n",
    "- **Inverse Document Frequency (IDF)** is a weight indicating how commonly a word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4dc56-5e6b-442c-a848-d79d4d41cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "# `axis=0` is the direction running downward the rows\n",
    "doc_freq = np.sum(tf > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed263b90-46e6-4003-8a14-d682ca20524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f06d7-c1b5-4db2-a26c-2298429ae315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0aceff-5437-495d-bebd-e821fdce8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document, show the top 5 terms (in terms of `tf_idf` score)\n",
    "np.random.seed(36)\n",
    "i = np.random.choice(N)\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row['label'])\n",
    "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tf_idf[i]\n",
    "# Add minus for descending\n",
    "indices = (-scores).argsort()\n",
    "for j in indices[:5]:\n",
    "    print(idx2word[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7e12b-c342-4d2a-8314-e8435b1eb019",
   "metadata": {},
   "source": [
    "## 3-2. CountVectorizer\n",
    "Derived term frequencies from `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938bbb-ad58-456a-bde1-dba001bbc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[\"text\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "labels.hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed262a0-4dec-42cf-8eb3-53d39ccd8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f22eb-171d-4160-a116-29360469dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d31f3-27a5-4d7e-af72-902028f29f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ca2a3-c137-43ab-81e4-1d5b73877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewer words than `nltk.word_tokenize()`\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91acc-4cc8-4890-9bf5-687b12b5fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default `lowercase=True`\n",
    "# np.where(words == \"India\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "# By default `token_pattern=r”(?u)\\b\\w\\w+\\b`\n",
    "# RegExp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)\n",
    "# np.where(words == \"$\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "np.where(words == \"india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2ae6-971d-402b-b635-50fc21226024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf[0][0]` in the above section\n",
    "tf.toarray()[0][13907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a4439-99b2-4bfb-b54d-d0df965546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "doc_freq = np.sum(tf.toarray() > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef44e38-03d7-4be8-9dd6-5f78eed1f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf.toarray() * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afeefb-b636-49c1-ad76-3c37264aec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf7d2d-9355-4a78-82dc-0a82fc1c3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf_idf[0][0]` in the above section\n",
    "tf_idf[0][13907]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62243fee-26e5-4e3c-9c72-6ec2dce2ae8d",
   "metadata": {},
   "source": [
    "## 3-3. TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713c327-82d1-4f66-98bc-0cfc51e58826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tf_idf = transformer.fit_transform(tf)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90fd91-64f9-490e-b628-7dfc211e43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18576d3-a230-49af-8736-6df21fa993e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `sklearn` implementation of TF-IDF is different from our manual implementation \n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46164149-3908-4c64-a94e-43f76019a69a",
   "metadata": {},
   "source": [
    "## 3-4. TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83306771-b56c-46aa-9b54-d11e1ddf4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc7a7a-5e89-41a7-ab70-73e57d2905aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129f297-ff06-4d94-976f-74605fc2a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `CountVectorizer()` followed by `TfidfTransformer()`\n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1e186-481e-41fd-8853-01e5a32deb90",
   "metadata": {},
   "source": [
    "Perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d20106-43a5-4377-ace3-c9d95896ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "inputs_train, inputs_test, y_train, y_test = train_test_split(inputs, labels, random_state=36)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train = vectorizer.fit_transform(inputs_train)\n",
    "x_test = vectorizer.transform(inputs_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train Score:\", model.score(x_train, y_train))\n",
    "print(\"Test Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd88420-26b9-45a6-bd17-10002dfc6530",
   "metadata": {},
   "source": [
    "# 4. word2vec\n",
    "## 4-1. CBOW (Continuous Bag of Words)\n",
    "## 4-2. Skip-Gram\n",
    "# 5. GloVe\n",
    "# 6. FastText\n",
    "# 7. Gaussian Embedding\n",
    "# 8. Pointcare Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bb1dd-560c-43ab-8846-6da417779f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
