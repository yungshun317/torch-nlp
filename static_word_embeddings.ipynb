{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9f1511-7074-4876-aef8-60c18f80e334",
   "metadata": {},
   "source": [
    "# 1. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c670-d4c5-48a7-9d72-19151436fb66",
   "metadata": {},
   "source": [
    "# 2. Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250ef9-8f5f-47f0-bd41-e4e25aba2477",
   "metadata": {},
   "source": [
    "# 3. TF-IDF\n",
    "## 3-1. word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87d7a6-e139-4316-afab-9243e3ea98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9e24e-dbb8-4fa4-81c3-5536f23bc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(df[\"text\"][0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04404d24-600f-4253-a796-8f4702c50217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate word2idx\n",
    "# Convert documents into sequences of ints / ids / indices\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in df[\"text\"]:\n",
    "    words = word_tokenize(doc.lower())\n",
    "    doc_as_int = []\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "        # Save for later\n",
    "        doc_as_int.append(word2idx[word])\n",
    "    tokenized_docs.append(doc_as_int)\n",
    "\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16232bb9-7f41-496a-bc5f-e5869744a8b6",
   "metadata": {},
   "source": [
    "### idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382cd8a-042d-47bb-965e-80b1361d11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa798d-9c8d-4e2b-a094-3a04305db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(df['text'])\n",
    "\n",
    "# Number of words\n",
    "V = len(word2idx)\n",
    "\n",
    "N, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a923872-2faf-4055-907f-a33fa1d40c66",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "**Term frequency (TF)** means how often a term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260c138-d650-420b-80a6-334c89c60e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instantiate term-frequency matrix\n",
    "tf = np.zeros((N, V))\n",
    "\n",
    "# Populate term-frequency counts\n",
    "for i, doc_as_int in enumerate(tokenized_docs):\n",
    "    for j in doc_as_int:\n",
    "        tf[i, j] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a3b36-b9f5-4c11-a017-8c33b8b9302e",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "- **Document frequency (DF)** is the number of documents containing a particular term.\n",
    "- **Inverse Document Frequency (IDF)** is a weight indicating how commonly a word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4dc56-5e6b-442c-a848-d79d4d41cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "# `axis=0` is the direction running downward the rows\n",
    "doc_freq = np.sum(tf > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed263b90-46e6-4003-8a14-d682ca20524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f06d7-c1b5-4db2-a26c-2298429ae315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0aceff-5437-495d-bebd-e821fdce8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document, show the top 5 terms (in terms of `tf_idf` score)\n",
    "np.random.seed(36)\n",
    "i = np.random.choice(N)\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row['label'])\n",
    "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tf_idf[i]\n",
    "# Add minus for descending\n",
    "indices = (-scores).argsort()\n",
    "for j in indices[:5]:\n",
    "    print(idx2word[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7e12b-c342-4d2a-8314-e8435b1eb019",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938bbb-ad58-456a-bde1-dba001bbc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[\"text\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "labels.hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed262a0-4dec-42cf-8eb3-53d39ccd8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f22eb-171d-4160-a116-29360469dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d31f3-27a5-4d7e-af72-902028f29f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ca2a3-c137-43ab-81e4-1d5b73877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewer words than `nltk.word_tokenize()`\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91acc-4cc8-4890-9bf5-687b12b5fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default `lowercase=True`\n",
    "# np.where(words == \"India\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "# By default `token_pattern=râ€(?u)\\b\\w\\w+\\b`\n",
    "# RegExp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)\n",
    "# np.where(words == \"$\")\n",
    "# (array([], dtype=int64),)\n",
    "\n",
    "np.where(words == \"india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2ae6-971d-402b-b635-50fc21226024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf[0][0]` in the above section\n",
    "tf.toarray()[0][13907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a4439-99b2-4bfb-b54d-d0df965546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "doc_freq = np.sum(tf.toarray() > 0, axis=0)\n",
    "idf = np.log(N / doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef44e38-03d7-4be8-9dd6-5f78eed1f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "tf_idf = tf.toarray() * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afeefb-b636-49c1-ad76-3c37264aec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf7d2d-9355-4a78-82dc-0a82fc1c3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `tf_idf[0][0]` in the above section\n",
    "tf_idf[0][13907]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62243fee-26e5-4e3c-9c72-6ec2dce2ae8d",
   "metadata": {},
   "source": [
    "## TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713c327-82d1-4f66-98bc-0cfc51e58826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tf_idf = transformer.fit_transform(tf)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90fd91-64f9-490e-b628-7dfc211e43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18576d3-a230-49af-8736-6df21fa993e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `sklearn` implementation of TF-IDF is different from our manual implementation \n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46164149-3908-4c64-a94e-43f76019a69a",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83306771-b56c-46aa-9b54-d11e1ddf4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(inputs)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc7a7a-5e89-41a7-ab70-73e57d2905aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129f297-ff06-4d94-976f-74605fc2a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as `CountVectorizer()` followed by `TfidfTransformer()`\n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1e186-481e-41fd-8853-01e5a32deb90",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d20106-43a5-4377-ace3-c9d95896ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "inputs_train, inputs_test, y_train, y_test = train_test_split(inputs, labels, random_state=36)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train = vectorizer.fit_transform(inputs_train)\n",
    "x_test = vectorizer.transform(inputs_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train Score:\", model.score(x_train, y_train))\n",
    "print(\"Test Score:\", model.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
